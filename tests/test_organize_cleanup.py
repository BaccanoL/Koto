"""æµ‹è¯• OrganizeCleanup æ¸…ç†é‡å¤æ–‡ä»¶å¤¹åŠŸèƒ½ã€‚"""
import sys
import os
import tempfile
import shutil
import json

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from web.organize_cleanup import OrganizeCleanup


def test_cleanup_merges_duplicates():
    """æ¨¡æ‹ŸçœŸå®åœºæ™¯ï¼šç›¸åŒè®ºæ–‡å› ä¿®è®¢åç¼€äº§ç”Ÿäº†å¤šä¸ªæ–‡ä»¶å¤¹ã€‚"""
    test_root = tempfile.mkdtemp(prefix="koto_cleanup_test_")
    organize_dir = os.path.join(test_root, "_organize")

    try:
        # æ¨¡æ‹Ÿç°æœ‰æ··ä¹±ç»“æ„
        base_name = "ç”µå½±æ—¶é—´çš„è®¡ç®—è§£æ"
        variants = [
            base_name,
            base_name + "_revised",
            base_name + "_revised(1)",
            base_name + "_revised(2)",
            base_name + "_revised_20260215",
        ]

        for i, name in enumerate(variants):
            folder = os.path.join(organize_dir, "other", name)
            os.makedirs(folder, exist_ok=True)
            # å†™å…¥ç›¸åŒå†…å®¹ï¼ˆæ¨¡æ‹ŸåŒä¸€ç¯‡è®ºæ–‡çš„å¤šä¸ªç‰ˆæœ¬ï¼‰
            with open(os.path.join(folder, f"paper.docx"), "wb") as f:
                f.write(b"SAME CONTENT FOR ALL VERSIONS " + b"x" * 1000)
            # æœ€æ–°ç‰ˆå¤šä¸€ä¸ªé™„ä»¶
            if i == len(variants) - 1:
                with open(os.path.join(folder, "appendix.pdf"), "wb") as f:
                    f.write(b"APPENDIX CONTENT " + b"y" * 500)

        # å¦ä¸€ä¸ªé‡å¤ç»„
        group2 = ["å…¥ä¼™åè®®-ä»ŠèŒ‚", "å…¥ä¼™åè®®-æœ±æ€»", "å…¥ä¼™åè®®"]
        for name in group2:
            folder = os.path.join(organize_dir, "legal", name)
            os.makedirs(folder, exist_ok=True)
            with open(os.path.join(folder, "contract.pdf"), "wb") as f:
                f.write(b"CONTRACT CONTENT " + b"z" * 800)

        # åˆ›å»º index.jsonï¼ˆå ä½ï¼‰
        idx_path = os.path.join(organize_dir, "index.json")
        with open(idx_path, "w", encoding="utf-8") as f:
            json.dump({"version": "1.0", "total_files": 0, "files": [],
                        "created_at": "", "last_updated": ""}, f)

        # éªŒè¯åˆå§‹çŠ¶æ€
        cleanup = OrganizeCleanup(organize_root=organize_dir)
        folder_info = cleanup._scan_folders()
        initial_count = len(folder_info)
        initial_with_files = len([k for k, v in folder_info.items() if v["files"]])
        print(f"åˆå§‹æ–‡ä»¶å¤¹æ•°: {initial_count} (å«æ–‡ä»¶: {initial_with_files})")
        assert initial_with_files == len(variants) + len(group2), f"Expected {len(variants) + len(group2)} with files, got {initial_with_files}"

        # dry run å…ˆçœ‹çœ‹
        dry_report = cleanup.run(dry_run=True)
        print(f"ç›¸ä¼¼ç»„æ•°: {dry_report['similarity_groups']}")
        assert dry_report["similarity_groups"] >= 2, f"Expected >= 2 groups, got {dry_report['similarity_groups']}"
        print("âœ… Dry run è¯†åˆ«åˆ°ç›¸ä¼¼ç»„")

        # å®é™…æ‰§è¡Œ
        cleanup2 = OrganizeCleanup(organize_root=organize_dir)
        report = cleanup2.run(dry_run=False)
        print(f"åˆå¹¶æ–‡ä»¶: {report['merged_files']}")
        print(f"å»é‡æ–‡ä»¶: {report['deduped_files']}")
        print(f"åˆ é™¤æ–‡ä»¶å¤¹: {report['removed_folders']}")

        # éªŒè¯ç»“æœ
        remaining = cleanup2._scan_folders()
        remaining_with_files = {k: v for k, v in remaining.items() if v["files"]}
        print(f"å‰©ä½™æ–‡ä»¶å¤¹: {list(remaining_with_files.keys())}")

        # åº”è¯¥åªå‰© 2 ä¸ªæ–‡ä»¶å¤¹ï¼ˆæ¯ç»„1ä¸ªï¼‰
        assert len(remaining_with_files) <= 3, f"Expected <= 3 remaining, got {len(remaining_with_files)}: {list(remaining_with_files.keys())}"
        print("âœ… é‡å¤æ–‡ä»¶å¤¹å·²åˆå¹¶")

        # éªŒè¯ç´¢å¼•å·²é‡å»º
        with open(idx_path, "r", encoding="utf-8") as f:
            new_idx = json.load(f)
        assert new_idx["total_files"] >= 2, f"Expected >= 2 files in index, got {new_idx['total_files']}"
        print(f"âœ… ç´¢å¼•å·²é‡å»º: {new_idx['total_files']} æ¡è®°å½•")

        print("\nğŸ‰ æ¸…ç†æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")

    finally:
        shutil.rmtree(test_root)


if __name__ == "__main__":
    test_cleanup_merges_duplicates()
