"""
æ™ºèƒ½æ–‡ä»¶å½’çº³å™¨ - è‡ªåŠ¨åˆ›å»ºæ–‡ä»¶å¤¹å’Œç»„ç»‡æ–‡ä»¶

åŒ…å«æ™ºèƒ½å»é‡ã€ç›¸ä¼¼æ–‡ä»¶å¤¹åˆå¹¶ã€å†…å®¹hashæ¯”å¯¹ç­‰æœºåˆ¶ã€‚
"""
import os
import json
import shutil
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import hashlib
from difflib import SequenceMatcher


class FileOrganizer:
    """æ–‡ä»¶å½’çº³ç®¡ç†å™¨"""
    
    def __init__(self, organize_root: str = "workspace/_organize"):
        """
        åˆå§‹åŒ–å½’çº³ç³»ç»Ÿ
        
        Args:
            organize_root: å½’çº³æ–‡ä»¶å¤¹çš„æ ¹ç›®å½•
        """
        self.organize_root = Path(organize_root)
        self.organize_root.mkdir(parents=True, exist_ok=True)
        
        self.index_file = self.organize_root / "index.json"
        self.metadata_template = {
            "created_at": datetime.now().isoformat(),
            "files_count": 0,
            "last_updated": datetime.now().isoformat()
        }
        
        self._ensure_index_exists()
    
    def _ensure_index_exists(self):
        """ç¡®ä¿ç´¢å¼•æ–‡ä»¶å­˜åœ¨"""
        if not self.index_file.exists():
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "version": "1.0",
                    "created_at": datetime.now().isoformat(),
                    "total_files": 0,
                    "last_updated": datetime.now().isoformat(),
                    "files": []
                }, f, ensure_ascii=False, indent=2)
    
    def organize_file(self, source_file: str, suggested_folder: str, auto_confirm: bool = False, metadata: Optional[Dict] = None) -> Dict:
        """
        ç»„ç»‡å•ä¸ªæ–‡ä»¶åˆ°å½’çº³æ–‡ä»¶å¤¹
        
        åŒ…å«æ™ºèƒ½å»é‡:
        1. æ£€æŸ¥ç°æœ‰æ–‡ä»¶å¤¹æ˜¯å¦å·²æœ‰ç›¸ä¼¼åç§° â†’ åˆå¹¶åˆ°ç°æœ‰æ–‡ä»¶å¤¹
        2. æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹å†…æ˜¯å¦å·²æœ‰ç›¸åŒå†…å®¹æ–‡ä»¶ â†’ è·³è¿‡å¤åˆ¶
        
        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„
            suggested_folder: å»ºè®®çš„ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ "finance/åèŠ¯é•¿æ˜‡"ï¼‰
            auto_confirm: æ˜¯å¦è‡ªåŠ¨åº”ç”¨å»ºè®®
            metadata: å…ƒæ•°æ®ï¼ˆå‘é€è€…ã€å®ä½“ç­‰ï¼‰
        """
        source_path = Path(source_file)
        
        if not source_path.exists():
            return {
                "success": False,
                "error": f"æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}"
            }
        
        # æ¸…ç†å»ºè®®è·¯å¾„ï¼ˆç§»é™¤ä¸å®‰å…¨å­—ç¬¦ï¼‰
        safe_folder = self._sanitize_path(suggested_folder)
        
        # â˜… æ™ºèƒ½æ–‡ä»¶å¤¹åŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼æ–‡ä»¶å¤¹ï¼Œé¿å…é‡å¤åˆ›å»º
        matched_folder = self._find_similar_existing_folder(safe_folder)
        if matched_folder:
            safe_folder = matched_folder
        
        # åˆ›å»ºå®Œæ•´ç›®æ ‡è·¯å¾„
        dest_dir = self.organize_root / safe_folder
        dest_path = dest_dir / source_path.name
        
        # â˜… æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹é‡Œæ˜¯å¦å·²æœ‰å†…å®¹ç›¸åŒçš„æ–‡ä»¶
        source_hash = self._compute_file_hash(source_path)
        existing_dup = self._find_content_duplicate(dest_dir, source_hash)
        if existing_dup:
            return {
                "success": True,
                "source_file": source_file,
                "dest_file": str(existing_dup),
                "relative_path": str(existing_dup.relative_to(self.organize_root)),
                "folder_created": False,
                "message": f"æ–‡ä»¶å·²å­˜åœ¨ï¼ˆå†…å®¹ç›¸åŒï¼‰: {existing_dup.name}",
                "skipped_duplicate": True
            }
        
        # å¤„ç†é‡å¤æ–‡ä»¶å
        dest_path = self._get_unique_path(dest_path, source_path)
        
        try:
            # åˆ›å»ºç›®æ ‡ç›®å½•
            dest_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼‰
            shutil.copy2(source_path, dest_path)
            
            # æ›´æ–°ç´¢å¼•
            self._update_index(source_file, str(dest_path), safe_folder, metadata)
            
            # åˆ›å»ºæ–‡ä»¶å¤¹å…ƒæ•°æ®
            self._update_folder_metadata(dest_dir)
            
            return {
                "success": True,
                "source_file": source_file,
                "dest_file": str(dest_path),
                "relative_path": str(dest_path.relative_to(self.organize_root)),
                "folder_created": True,
                "message": f"æ–‡ä»¶å·²æˆåŠŸç»„ç»‡åˆ°: {safe_folder}"
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": f"æ–‡ä»¶ç»„ç»‡å¤±è´¥: {str(e)}"
            }
    
    def _sanitize_path(self, path: str) -> str:
        """æ¸…ç†è·¯å¾„ä¸­çš„ä¸å®‰å…¨å­—ç¬¦"""
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä½†ä¿ç•™ / ä½œä¸ºè·¯å¾„åˆ†éš”ç¬¦
        replacements = {
            '\\': '/',
            ':': '_',
            '*': '_',
            '?': '_',
            '"': '_',
            '<': '_',
            '>': '_',
            '|': '_'
        }
        
        for char, replacement in replacements.items():
            path = path.replace(char, replacement)
        
        # ç§»é™¤è¿ç»­çš„ç©ºæ ¼
        while '  ' in path:
            path = path.replace('  ', ' ')
        
        return path.strip()
    
    def _get_unique_path(self, path: Path, source_path: Path = None) -> Path:
        """è·å–å”¯ä¸€çš„æ–‡ä»¶è·¯å¾„ï¼ˆé¿å…è¦†ç›–ï¼‰"""
        if not path.exists():
            return path
        
        # å¦‚æœæºæ–‡ä»¶è·Ÿç›®æ ‡æ–‡ä»¶å†…å®¹ç›¸åŒï¼Œç›´æ¥è¿”å›åŸè·¯å¾„ï¼ˆè·³è¿‡ï¼‰
        if source_path and self._is_same_file(source_path, path):
            return path
        
        stem = path.stem
        suffix = path.suffix
        parent = path.parent
        
        counter = 1
        while True:
            new_name = f"{stem}_{counter}{suffix}"
            new_path = parent / new_name
            if not new_path.exists():
                return new_path
            # æ£€æŸ¥æ–°è·¯å¾„çš„æ–‡ä»¶å†…å®¹æ˜¯å¦ä¸æºæ–‡ä»¶ç›¸åŒ
            if source_path and self._is_same_file(source_path, new_path):
                return new_path
            counter += 1

    def _normalize_entity_name(self, name: str) -> str:
        """Normalize entity name for matching."""
        if not name:
            return ""
        normalized = re.sub(r"\s+", " ", name.strip())
        return normalized.lower()
    
    @staticmethod
    def _compute_file_hash(file_path: Path, chunk_size: int = 8192) -> str:
        """è®¡ç®—æ–‡ä»¶çš„ SHA-256 hash"""
        h = hashlib.sha256()
        try:
            with open(file_path, 'rb') as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    h.update(chunk)
        except Exception:
            return ""
        return h.hexdigest()
    
    def _is_same_file(self, file_a: Path, file_b: Path) -> bool:
        """æ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶å†…å®¹æ˜¯å¦ç›¸åŒï¼ˆå…ˆæ¯”å¤§å°ï¼Œå†æ¯”hashï¼‰"""
        try:
            if file_a.stat().st_size != file_b.stat().st_size:
                return False
            return self._compute_file_hash(file_a) == self._compute_file_hash(file_b)
        except Exception:
            return False

    def _find_content_duplicate(self, dest_dir: Path, source_hash: str) -> Optional[Path]:
        """æ£€æŸ¥ç›®æ ‡æ–‡ä»¶å¤¹å†…æ˜¯å¦å·²æœ‰å†…å®¹ç›¸åŒçš„æ–‡ä»¶"""
        if not dest_dir.exists() or not source_hash:
            return None
        for existing_file in dest_dir.iterdir():
            if existing_file.is_file() and not existing_file.name.startswith('_'):
                if self._compute_file_hash(existing_file) == source_hash:
                    return existing_file
        return None

    # ä¿®è®¢åç¼€æ¨¡å¼ï¼ˆç”¨äºæ–‡ä»¶å¤¹åæ¸…ç†ï¼Œä¸ FileAnalyzer ä¿æŒä¸€è‡´ï¼‰
    _REVISION_PATTERNS = [
        re.compile(r'_revised\(\d+\)$', re.IGNORECASE),
        re.compile(r'_revised_\d{8,14}$', re.IGNORECASE),
        re.compile(r'_revised$', re.IGNORECASE),
        re.compile(r'\(\d+\)$'),
        re.compile(r'_\d{8,14}$'),
        re.compile(r'_copy\d*$', re.IGNORECASE),
        re.compile(r'_å‰¯æœ¬\d*$'),
    ]

    def _clean_name_for_matching(self, name: str) -> str:
        """æ¸…ç†åç§°ç”¨äºåŒ¹é…ï¼šå»æ‰ä¿®è®¢åç¼€ã€ç©ºæ ¼ç­‰ã€‚"""
        for pat in self._REVISION_PATTERNS:
            name = pat.sub('', name)
        return name.strip().lower()

    def _find_similar_existing_folder(self, suggested_folder: str) -> Optional[str]:
        """æ£€æŸ¥ _organize æ ¹ç›®å½•ä¸‹æ˜¯å¦å·²æœ‰ç›¸ä¼¼åç§°çš„æ–‡ä»¶å¤¹ã€‚
        
        ä¾‹å¦‚ suggested_folder = "other/ç”µå½±æ—¶é—´çš„è®¡ç®—è§£æ"ï¼Œ
        è€Œå·²å­˜åœ¨ "other/ç”µå½±æ—¶é—´çš„è®¡ç®—è§£æï¼šåŸºäºå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„ç”µå½±è¿ç»­æ€§ç ”ç©¶"ï¼Œ
        åº”è¯¥å½’å…¥åè€…ã€‚
        
        åŒ¹é…ç­–ç•¥: 
        1. ç²¾ç¡®åŒ¹é…ï¼ˆå®Œå…¨ç›¸åŒï¼‰
        2. æ¸…ç†ä¿®è®¢åç¼€åå†åŒ¹é…
        3. å‰ç¼€åŒ¹é…ï¼ˆA æ˜¯ B çš„å‰ç¼€ï¼Œæˆ–åä¹‹ï¼‰
        4. æ¨¡ç³ŠåŒ¹é…ï¼ˆç›¸ä¼¼åº¦ > 0.6ï¼‰
        """
        parts = suggested_folder.replace('\\', '/').split('/')
        
        if len(parts) >= 2:
            parent_name = parts[0]  # industry
            entity_name = '/'.join(parts[1:])  # entity part
            parent_dir = self.organize_root / parent_name
        else:
            parent_dir = self.organize_root
            entity_name = parts[0]
        
        # æœé›†æ‰€æœ‰ç°æœ‰æ–‡ä»¶å¤¹
        existing_folders = []
        search_dirs = set()
        if parent_dir.exists():
            search_dirs.add(parent_dir)
        search_dirs.add(self.organize_root)
        
        for search_dir in search_dirs:
            if not search_dir.exists():
                continue
            for item in search_dir.iterdir():
                if item.is_dir() and not item.name.startswith('_'):
                    try:
                        rel = str(item.relative_to(self.organize_root)).replace('\\', '/')
                        existing_folders.append(rel)
                    except ValueError:
                        pass
                    # ä¹Ÿæ£€æŸ¥äºŒçº§ç›®å½•
                    for sub_item in item.iterdir():
                        if sub_item.is_dir() and not sub_item.name.startswith('_'):
                            try:
                                rel = str(sub_item.relative_to(self.organize_root)).replace('\\', '/')
                                existing_folders.append(rel)
                            except ValueError:
                                pass
        
        if not existing_folders:
            return None
        
        # æ¸…ç†å»ºè®®åç§°çš„ä¿®è®¢åç¼€
        entity_clean = self._clean_name_for_matching(entity_name)
        
        best_match = None
        best_score = 0.0
        
        for existing in existing_folders:
            existing_lower = existing.lower().replace('\\', '/')
            existing_leaf = existing_lower.split('/')[-1] if '/' in existing_lower else existing_lower
            existing_clean = self._clean_name_for_matching(existing_leaf)
            
            # 1. ç²¾ç¡®åŒ¹é…
            if existing_lower == suggested_folder.lower().replace('\\', '/'):
                return existing  # å®Œå…¨ç›¸åŒ
            
            # 2. æ¸…ç†åç²¾ç¡®åŒ¹é…ï¼ˆå¦‚ test_doc_revised â†’ test_docï¼‰
            if entity_clean == existing_clean:
                return existing
            
            # 3. å‰ç¼€åŒ¹é…ï¼ˆæŸä¸ªæ˜¯å¦ä¸€ä¸ªçš„å‰ç¼€ï¼‰
            if existing_clean.startswith(entity_clean) or entity_clean.startswith(existing_clean):
                shorter = min(len(entity_clean), len(existing_clean))
                longer = max(len(entity_clean), len(existing_clean), 1)
                score = shorter / longer
                if score > best_score:
                    best_score = score
                    best_match = existing
            
            # 4. æ¨¡ç³ŠåŒ¹é…
            similarity = SequenceMatcher(None, entity_clean, existing_clean).ratio()
            if similarity > 0.6 and similarity > best_score:
                best_score = similarity
                best_match = existing
        
        if best_match and best_score >= 0.4:
            return best_match
        
        return None
    
    def _update_index(self, source_file: str, dest_file: str, folder: str, metadata: Optional[Dict] = None):
        """æ›´æ–°å…¨å±€ç´¢å¼•ï¼ˆå«å»é‡ï¼‰"""
        with open(self.index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
        
        file_size = Path(source_file).stat().st_size if Path(source_file).exists() else 0
        
        # â˜… å»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒæºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶çš„è®°å½•
        for existing in index.get("files", []):
            if existing.get("organized_path") == dest_file:
                # å·²å­˜åœ¨ç›¸åŒè®°å½•ï¼Œæ›´æ–°æ—¶é—´æˆ³å³å¯
                existing["organized_at"] = datetime.now().isoformat()
                index["last_updated"] = datetime.now().isoformat()
                with open(self.index_file, 'w', encoding='utf-8') as f:
                    json.dump(index, f, ensure_ascii=False, indent=2)
                return
        
        # æ·»åŠ æ–°æ¡ç›®
        entry = {
            "source_path": source_file,
            "organized_path": dest_file,
            "folder": folder,
            "file_name": Path(source_file).name,
            "file_size": file_size,
            "organized_at": datetime.now().isoformat()
        }

        if metadata:
            entry["metadata"] = metadata
            entity = metadata.get("entity")
            entity_type = metadata.get("entity_type")
            if entity:
                entry["entity"] = entity
            if entity_type:
                entry["entity_type"] = entity_type

        index["files"].append(entry)
        
        index["total_files"] = len(index["files"])
        index["last_updated"] = datetime.now().isoformat()
        
        with open(self.index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)

    def find_entity_folder(self, entity_name: str) -> Optional[str]:
        """Find an existing folder for the given entity name.
        Only returns the top-level entity folder (no deep subpaths).
        """
        if not entity_name:
            return None
        target = self._normalize_entity_name(entity_name)
        index = self.get_index()
        for entry in index.get("files", []):
            existing = entry.get("entity")
            if existing and self._normalize_entity_name(existing) == target:
                old_folder = entry.get("folder", "")
                # åªå–ç¬¬ä¸€çº§ç›®å½•ï¼ˆå®ä½“åï¼‰ï¼Œä¸å¤ç”¨æ—§çš„æ·±å±‚è·¯å¾„
                top_level = old_folder.split("/")[0].split("\\")[0]
                if top_level:
                    return top_level
        return None
    
    def _update_folder_metadata(self, folder_path: Path):
        """æ›´æ–°æ–‡ä»¶å¤¹çš„å…ƒæ•°æ®"""
        metadata_file = folder_path / "_metadata.json"
        
        # ç»Ÿè®¡è¯¥æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶
        file_count = len(list(folder_path.glob("*"))) - 1  # å‡å»metadataæ–‡ä»¶æœ¬èº«
        
        metadata = {
            "folder": str(folder_path.relative_to(self.organize_root)),
            "file_count": max(0, file_count),
            "last_updated": datetime.now().isoformat(),
            "files": [
                f.name for f in folder_path.glob("*") 
                if f.is_file() and not f.name.startswith("_")
            ]
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def get_index(self) -> Dict:
        """è·å–å®Œæ•´ç´¢å¼•"""
        with open(self.index_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def search_files(self, keyword: str) -> List[Dict]:
        """æœç´¢å·²ç»„ç»‡çš„æ–‡ä»¶"""
        index = self.get_index()
        results = []
        
        keyword_lower = keyword.lower()
        for entry in index.get("files", []):
            if (keyword_lower in entry.get("file_name", "").lower() or 
                keyword_lower in entry.get("folder", "").lower()):
                results.append(entry)
        
        return results
    
    def get_categories_stats(self) -> Dict:
        """è·å–åˆ†ç±»ç»Ÿè®¡ä¿¡æ¯"""
        index = self.get_index()
        stats = {}
        
        for entry in index.get("files", []):
            folder = entry.get("folder", "other")
            industry = folder.split("/")[0]
            
            if industry not in stats:
                stats[industry] = {
                    "count": 0,
                    "size": 0,
                    "files": []
                }
            
            stats[industry]["count"] += 1
            stats[industry]["size"] += entry.get("file_size", 0)
            stats[industry]["files"].append(entry.get("file_name"))
        
        return stats
    
    def list_organized_folders(self) -> Dict:
        """åˆ—å‡ºæ‰€æœ‰å·²åˆ›å»ºçš„æ–‡ä»¶å¤¹"""
        folders = {}
        
        for root, dirs, files in os.walk(self.organize_root):
            root_path = Path(root)
            
            # è·³è¿‡æ ¹ç›®å½•å’Œå…ƒæ•°æ®æ–‡ä»¶
            if root_path == self.organize_root:
                continue
            
            relative = root_path.relative_to(self.organize_root)
            non_metadata_files = [f for f in files if not f.startswith("_")]
            
            if non_metadata_files:  # åªæ˜¾ç¤ºæœ‰æ–‡ä»¶çš„æ–‡ä»¶å¤¹
                folders[str(relative)] = {
                    "file_count": len(non_metadata_files),
                    "files": non_metadata_files,
                    "full_path": str(root_path)
                }
        
        return folders
    
    def organize_batch(self, files_with_suggestions: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡ç»„ç»‡æ–‡ä»¶
        
        Args:
            files_with_suggestions: [
                {"file": "path", "folder": "suggested_folder"},
                ...
            ]
        
        Returns:
            List of organization results
        """
        results = []
        for item in files_with_suggestions:
            result = self.organize_file(
                item["file"],
                item["folder"],
                item.get("auto_confirm", False)
            )
            results.append(result)
        
        return results


# å¿«é€Ÿæµ‹è¯•
if __name__ == "__main__":
    organizer = FileOrganizer()
    
    # æµ‹è¯•ç»Ÿè®¡
    print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    stats = organizer.get_categories_stats()
    for industry, info in stats.items():
        print(f"  {industry}: {info['count']} æ–‡ä»¶")
    
    print("\nğŸ“ å·²åˆ›å»ºçš„æ–‡ä»¶å¤¹:")
    folders = organizer.list_organized_folders()
    for folder, info in folders.items():
        print(f"  {folder}: {info['file_count']} æ–‡ä»¶")
