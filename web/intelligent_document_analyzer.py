#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½æ–‡æ¡£åˆ†æå¼•æ“ - Intelligent Document Analyzer
èƒ½å¤Ÿç†è§£ç”¨æˆ·æ„å›¾ã€åˆ†ææ–‡æ¡£ç»“æ„ã€åˆ†è§£ä»»åŠ¡ã€ç”Ÿæˆé«˜è´¨é‡å›å¤
"""

import os
import re
import json
import difflib
from typing import Dict, List, Any, Optional, Tuple, AsyncGenerator
from docx import Document
from docx.shared import RGBColor
from pathlib import Path


class IntelligentDocumentAnalyzer:
    """æ™ºèƒ½æ–‡æ¡£åˆ†æå¼•æ“"""
    
    # æ–‡æ¡£ç±»å‹æ£€æµ‹
    ACADEMIC_KEYWORDS = ['æ‘˜è¦', 'abstract', 'å¼•è¨€', 'ç»“è®º', 'å‚è€ƒæ–‡çŒ®', 'å…³é”®è¯', 'è®ºæ–‡', 'ç ”ç©¶']
    REPORT_KEYWORDS = ['æŠ¥å‘Š', 'æ€»ç»“', 'åˆ†æ', 'ç»“æœ', 'å»ºè®®', 'ç»“è®º']
    ARTICLE_KEYWORDS = ['æ ‡é¢˜', 'ç›®å½•', 'ç« èŠ‚', 'æ®µè½']
    
    # ä»»åŠ¡ç±»å‹åˆ†è§£
    TASK_PATTERNS = {
        'write_abstract': {
            'keywords': ['å†™.*æ‘˜è¦', 'ç”Ÿæˆ.*æ‘˜è¦', 'æ‘˜è¦.*å†™', 'æ‘˜è¦.*ç”Ÿæˆ', 'abstract'],
            'requirements': ['ç ”ç©¶èƒŒæ™¯', 'ç ”ç©¶æ–¹æ³•', 'ç ”ç©¶ç»“æœ', 'ç ”ç©¶ç»“è®º', '300.*400å­—', '300-400'],
            'output_type': 'generate'  # ç”Ÿæˆæ–°å†…å®¹ï¼Œä¸ä¿®æ”¹åŸæ–‡
        },
        'revise_intro': {
            'keywords': ['æ”¹.*å¼•è¨€', 'æ”¹è¿›.*å¼•è¨€', 'é‡å†™.*å¼•è¨€', 'ä¼˜åŒ–.*å¼•è¨€', 'å¼•è¨€.*æ”¹', 'å¼•è¨€.*ä¸ç¬¦åˆ'],
            'requirements': ['æ¶æ„', 'ä¸»ä½“', 'æ–‡ç« ', 'å¯¹åº”', 'ç¬¦åˆ'],
            'output_type': 'generate'  # ç”Ÿæˆæ–°å†…å®¹
        },
        'revise_conclusion': {
            'keywords': ['æ”¹.*ç»“è®º', 'æ”¹è¿›.*ç»“è®º', 'é‡å†™.*ç»“è®º', 'ä¼˜åŒ–.*ç»“è®º', 'ç»“è®º.*ä¸æ»¡æ„', 'overcap'],
            'requirements': ['æ•´ç¯‡', 'å…¨æ–‡', 'æ€»ç»“', 'æ¦‚æ‹¬'],
            'output_type': 'generate'  # ç”Ÿæˆæ–°å†…å®¹
        },
        'general_revision': {
            'keywords': ['æ”¹', 'æ”¹è¿›', 'ä¼˜åŒ–', 'ä¿®æ”¹', 'æ¶¦è‰²', 'æå‡'],
            'requirements': [],
            'output_type': 'generate'  # é»˜è®¤ä¸ºç”Ÿæˆæ–°å†…å®¹
        },
        'analysis': {
            'keywords': ['åˆ†æ', 'æ€»ç»“', 'æ¢³ç†', 'æ¦‚è¿°', 'è¦ç‚¹'],
            'requirements': [],
            'output_type': 'analysis'  # è¿”å›åˆ†æç»“æœ
        }
    }
    
    def __init__(self, llm_client):
        """
        åˆå§‹åŒ–æ–‡æ¡£åˆ†æå¼•æ“
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆKotoBrainæˆ–Gemini clientï¼‰
        """
        self.llm_client = llm_client
    
    def analyze_request(self, user_input: str, document_structure: Dict) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œç†è§£æ„å›¾å¹¶åˆ†è§£ä»»åŠ¡
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            document_structure: æ–‡æ¡£ç»“æ„ï¼ˆæ¥è‡ªDocumentReaderï¼‰
            
        Returns:
            {
                'tasks': [{'type': str, 'description': str, 'target_sections': [str]}],
                'document_type': 'academic' | 'report' | 'article' | 'general',
                'is_multi_task': bool,
                'confidence': float
            }
        """
        result = {
            'tasks': [],
            'document_type': 'general',
            'is_multi_task': False,
            'confidence': 0.5
        }
        
        # æ£€æµ‹æ–‡æ¡£ç±»å‹
        doc_content = ' '.join([p.get('text', '') for p in document_structure.get('paragraphs', [])])
        result['document_type'] = self._detect_document_type(doc_content)
        
        # åˆ†è§£ä»»åŠ¡
        user_lower = user_input.lower()
        detected_tasks = []
        
        for task_name, pattern in self.TASK_PATTERNS.items():
            # æ£€æŸ¥å…³é”®è¯åŒ¹é…
            keyword_match = any(re.search(kw, user_lower) for kw in pattern['keywords'])
            if keyword_match:
                # æ£€æŸ¥éœ€æ±‚åŒ¹é…
                req_match_count = sum(
                    1 for req in pattern['requirements']
                    if re.search(req, user_lower)
                )
                confidence = 0.7 + (0.1 * min(req_match_count, 3))
                
                detected_tasks.append({
                    'type': task_name,
                    'description': self._get_task_description(task_name, user_input),
                    'confidence': confidence,
                    'target_sections': self._identify_target_sections(task_name, document_structure)
                })
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ä»»åŠ¡ï¼Œé»˜è®¤ä¸ºgeneral_revision
        if not detected_tasks:
            detected_tasks.append({
                'type': 'analysis',
                'description': 'åˆ†æå¹¶æ”¹è¿›æ–‡æ¡£',
                'confidence': 0.5,
                'target_sections': []
            })
        
        result['tasks'] = detected_tasks
        result['is_multi_task'] = len(detected_tasks) > 1
        result['confidence'] = max(t['confidence'] for t in detected_tasks)
        
        return result
    
    def _detect_document_type(self, content: str) -> str:
        """æ£€æµ‹æ–‡æ¡£ç±»å‹"""
        content_lower = content.lower()
        
        academic_score = sum(1 for kw in self.ACADEMIC_KEYWORDS if kw in content_lower)
        report_score = sum(1 for kw in self.REPORT_KEYWORDS if kw in content_lower)
        
        if academic_score >= 3:
            return 'academic'
        elif report_score >= 2:
            return 'report'
        else:
            return 'article'
    
    def _get_task_description(self, task_type: str, user_input: str) -> str:
        """è·å–ä»»åŠ¡æè¿°"""
        descriptions = {
            'write_abstract': f'æ ¹æ®è¦æ±‚ç”Ÿæˆè®ºæ–‡æ‘˜è¦ï¼š{user_input}',
            'revise_intro': f'æ”¹è¿›å¼•è¨€ï¼š{user_input}',
            'revise_conclusion': f'æ”¹è¿›ç»“è®ºï¼š{user_input}',
            'general_revision': f'æ–‡æ¡£ä¼˜åŒ–ï¼š{user_input}',
            'analysis': f'æ–‡æ¡£åˆ†æï¼š{user_input}'
        }
        return descriptions.get(task_type, user_input)
    
    def _identify_target_sections(self, task_type: str, doc_structure: Dict) -> List[str]:
        """è¯†åˆ«ç›®æ ‡æ®µè½/ç« èŠ‚"""
        paragraphs = doc_structure.get('paragraphs', [])
        target_sections = []
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹å®šä½ç›®æ ‡æ®µè½
        if task_type == 'write_abstract':
            # æŸ¥æ‰¾æ‘˜è¦ä½ç½®ï¼ˆé€šå¸¸åœ¨æ–‡æ¡£å¼€å¤´ï¼‰
            for idx, para in enumerate(paragraphs[:10]):
                text = para.get('text', '').lower()
                if 'abstract' in text or 'æ‘˜è¦' in text:
                    target_sections.append(f'paragraph_{idx}')
                    break
            if not target_sections:
                target_sections.append('paragraph_0')  # é»˜è®¤ç¬¬ä¸€æ®µ
        
        elif task_type == 'revise_intro':
            # æŸ¥æ‰¾å¼•è¨€
            for idx, para in enumerate(paragraphs[:20]):
                text = para.get('text', '').lower()
                if 'å¼•è¨€' in text or 'introduction' in text or para.get('type') == 'heading' and para.get('level') == 1:
                    # æ‰¾åˆ°å¼•è¨€æ ‡é¢˜åï¼Œæ”¶é›†åç»­æ®µè½ç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜
                    target_sections.append(f'paragraph_{idx}')
                    for offset in range(1, 10):
                        if idx + offset < len(paragraphs):
                            next_para = paragraphs[idx + offset]
                            if next_para.get('type') == 'heading':
                                break
                            target_sections.append(f'paragraph_{idx + offset}')
                    break
        
        elif task_type == 'revise_conclusion':
            # æŸ¥æ‰¾ç»“è®ºï¼ˆé€šå¸¸åœ¨æ–‡æ¡£æœ«å°¾ï¼‰
            for idx in range(len(paragraphs) - 1, max(0, len(paragraphs) - 30), -1):
                text = paragraphs[idx].get('text', '').lower()
                if 'ç»“è®º' in text or 'ç»“è¯­' in text or 'conclusion' in text:
                    target_sections.append(f'paragraph_{idx}')
                    # æ”¶é›†ç»“è®ºæ®µè½
                    for offset in range(1, 5):
                        if idx + offset < len(paragraphs):
                            target_sections.append(f'paragraph_{idx + offset}')
                    break
        
        return target_sections
    
    def generate_specialized_prompt(self, task: Dict, doc_structure: Dict, user_input: str) -> str:
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆä¸“é—¨çš„æç¤ºè¯
        
        Args:
            task: ä»»åŠ¡ä¿¡æ¯
            doc_structure: æ–‡æ¡£ç»“æ„
            user_input: ç”¨æˆ·åŸå§‹è¾“å…¥
            
        Returns:
            ä¸“é—¨çš„æç¤ºè¯
        """
        task_type = task['type']
        
        # æå–æ–‡æ¡£å†…å®¹
        paragraphs = doc_structure.get('paragraphs', [])
        doc_text = '\n\n'.join([p.get('text', '') for p in paragraphs])
        
        # æå–æ–‡æ¡£ç»“æ„æ¦‚è§ˆ
        structure_overview = self._get_structure_overview(doc_structure)
        
        if task_type == 'write_abstract':
            return self._generate_abstract_prompt(doc_text, structure_overview, user_input)
        elif task_type == 'revise_intro':
            return self._generate_intro_prompt(doc_text, structure_overview, user_input)
        elif task_type == 'revise_conclusion':
            return self._generate_conclusion_prompt(doc_text, structure_overview, user_input)
        elif task_type == 'general_revision':
            return self._generate_revision_prompt(doc_text, structure_overview, user_input)
        else:  # analysis
            return self._generate_analysis_prompt(doc_text, structure_overview, user_input)
    
    def _get_structure_overview(self, doc_structure: Dict) -> str:
        """è·å–æ–‡æ¡£ç»“æ„æ¦‚è§ˆ"""
        paragraphs = doc_structure.get('paragraphs', [])
        headings = [
            (idx, p['text'], p.get('level', 1))
            for idx, p in enumerate(paragraphs)
            if p.get('type') == 'heading'
        ]
        
        structure_lines = ['æ–‡æ¡£ç»“æ„æ¦‚è§ˆ:']
        for idx, text, level in headings:
            indent = '  ' * (level - 1)
            structure_lines.append(f'{indent}- {text} (æ®µè½{idx})')
        
        return '\n'.join(structure_lines)
    
    def _generate_abstract_prompt(self, doc_text: str, structure: str, user_req: str) -> str:
        """ç”Ÿæˆæ‘˜è¦ä»»åŠ¡çš„ä¸“ç”¨æç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡æ‘˜è¦æ’°å†™ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹å®Œæ•´è®ºæ–‡å†…å®¹ï¼Œæ’°å†™ä¸€ç¯‡é«˜è´¨é‡çš„ä¸­æ–‡å­¦æœ¯æ‘˜è¦ã€‚

ç”¨æˆ·è¦æ±‚ï¼š
{user_req}

æ–‡æ¡£ç»“æ„ï¼š
{structure}

å®Œæ•´è®ºæ–‡å†…å®¹ï¼š
{doc_text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†å­¦æœ¯æ‘˜è¦æ¨¡æ¿æ’°å†™ï¼ˆæ§åˆ¶åœ¨300-400å­—ï¼‰ï¼š

1. ç ”ç©¶èƒŒæ™¯ä¸ç›®çš„ï¼ˆ2-3å¥ï¼‰ï¼š
   - ç®€è¦ä»‹ç»ç ”ç©¶é¢†åŸŸçš„èƒŒæ™¯
   - é˜è¿°ç ”ç©¶çš„ç›®çš„å’Œæ„ä¹‰

2. ç ”ç©¶æ–¹æ³•ï¼ˆ2-3å¥ï¼‰ï¼š
   - æè¿°ç ”ç©¶é‡‡ç”¨çš„æ–¹æ³•è®º
   - è¯´æ˜åˆ†ææ–¹æ³•å’ŒæŠ€æœ¯æ‰‹æ®µ

3. ç ”ç©¶ç»“æœï¼ˆ3-4å¥ï¼‰ï¼š
   - æ¦‚æ‹¬ç ”ç©¶çš„ä¸»è¦å‘ç°
   - çªå‡ºåˆ›æ–°ç‚¹å’Œæ ¸å¿ƒè´¡çŒ®

4. ç ”ç©¶ç»“è®ºï¼ˆ2-3å¥ï¼‰ï¼š
   - æ€»ç»“ç ”ç©¶çš„ä¸»è¦è´¡çŒ®
   - æŒ‡å‡ºç ”ç©¶çš„å±€é™æ€§å’Œæœªæ¥ç ”ç©¶æ–¹å‘

æ³¨æ„äº‹é¡¹ï¼š
- æ‘˜è¦åº”è¯¥è‡ªæˆä¸€ä½“ï¼Œä¸ä¾èµ–æ­£æ–‡å³å¯ç†è§£
- ä½¿ç”¨ç¬¬ä¸‰äººç§°å’Œå®¢è§‚è¯­æ°”
- é¿å…ä½¿ç”¨"æœ¬æ–‡è®¤ä¸º"ç­‰ä¸»è§‚è¡¨è¾¾
- çªå‡ºç†è®ºåˆ›æ–°å’Œå®è·µä»·å€¼
- ä¸¥æ ¼æ§åˆ¶å­—æ•°åœ¨300-400å­—ä¹‹é—´

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œæ— éœ€å…¶ä»–è¯´æ˜ã€‚"""

    def _generate_intro_prompt(self, doc_text: str, structure: str, user_req: str) -> str:
        """ç”Ÿæˆå¼•è¨€æ”¹è¿›ä»»åŠ¡çš„ä¸“ç”¨æç¤ºè¯"""
        # æå–å½“å‰å¼•è¨€
        lines = doc_text.split('\n')
        intro_start = -1
        intro_end = -1
        
        for idx, line in enumerate(lines):
            if 'å¼•è¨€' in line or 'Introduction' in line.lower():
                intro_start = idx
            elif intro_start != -1 and ('äºŒ ' in line or 'ç¬¬äºŒ' in line or '2.' in line or '2 ' in line[:3]):
                intro_end = idx
                break
        
        current_intro = '\n'.join(lines[intro_start:intro_end]) if intro_start != -1 and intro_end != -1 else "æœªæ‰¾åˆ°æ˜ç¡®çš„å¼•è¨€éƒ¨åˆ†"
        
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¼–è¾‘ã€‚è¯·æ ¹æ®è®ºæ–‡çš„æ•´ä½“æ¶æ„å’Œå†…å®¹ï¼Œæ”¹è¿›å¼•è¨€éƒ¨åˆ†ï¼Œä½¿å…¶ä¸æ–‡ç« ä¸»ä½“ç»“æ„é«˜åº¦å¯¹åº”ã€‚

ç”¨æˆ·è¦æ±‚ï¼š
{user_req}

æ–‡æ¡£æ•´ä½“ç»“æ„ï¼š
{structure}

å½“å‰å¼•è¨€å†…å®¹ï¼š
{current_intro}

å®Œæ•´è®ºæ–‡å†…å®¹ï¼ˆä¾›å‚è€ƒï¼‰ï¼š
{doc_text[:5000]}...  [æ–‡æ¡£è¾ƒé•¿ï¼Œå·²æˆªå–å‰5000å­—]

æ”¹è¿›è¦æ±‚ï¼š
1. **ç»“æ„å¯¹åº”**ï¼šå¼•è¨€å¿…é¡»æ¸…æ™°åœ°å¯¹åº”è®ºæ–‡å„ç« èŠ‚
   - ä¸ºæ¯ä¸€ç« ï¼ˆç¬¬äºŒã€ä¸‰ã€å››ã€äº”ç« ç­‰ï¼‰æä¾›æ˜ç¡®çš„å¯¼å¼•
   - è¯´æ˜å„ç« èŠ‚ä¹‹é—´çš„é€»è¾‘é€’è¿›å…³ç³»

2. **å±‚æ¬¡é€’è¿›**ï¼šé‡‡ç”¨"é—®é¢˜æå‡º â†’ å±‚æ¬¡å±•å¼€ â†’ æ€»ä½“å½’çº³"çš„ç»“æ„
   - ç¬¬ä¸€æ®µï¼šæå‡ºæ ¸å¿ƒé—®é¢˜
   - ä¸­é—´æ®µï¼šåˆ†å±‚æ¬¡ä»‹ç»å„ç« å†…å®¹ï¼ˆç¬¬ä¸€å±‚æ¬¡/ç¬¬äºŒå±‚æ¬¡/ç¬¬ä¸‰å±‚æ¬¡...ï¼‰
   - æœ€åæ®µï¼šæ€»ç»“å…¨æ–‡è®ºè¯è·¯å¾„

3. **é€»è¾‘æ¸…æ™°**ï¼šæ¯ä¸€å±‚æ¬¡çš„è¯´æ˜åº”åŒ…å«ï¼š
   - æœ¬ç« è¦è§£å†³ä»€ä¹ˆé—®é¢˜
   - é‡‡ç”¨ä»€ä¹ˆæ–¹æ³•/ç†è®º
   - å¾—å‡ºä»€ä¹ˆç»“è®º

4. **æ‰¿ä¸Šå¯ä¸‹**ï¼šè¯´æ˜å„ç« èŠ‚ä¹‹é—´çš„å› æœå…³è”

è¯·ç›´æ¥è¾“å‡ºæ”¹è¿›åçš„å¼•è¨€å†…å®¹ï¼Œæ— éœ€å…¶ä»–è¯´æ˜ã€‚"""

    def _generate_conclusion_prompt(self, doc_text: str, structure: str, user_req: str) -> str:
        """ç”Ÿæˆç»“è®ºæ”¹è¿›ä»»åŠ¡çš„ä¸“ç”¨æç¤ºè¯"""
        # æå–å½“å‰ç»“è®º
        lines = doc_text.split('\n')
        conclusion_start = -1
        
        for idx in range(len(lines) - 1, max(0, len(lines) - 100), -1):
            if 'ç»“è®º' in lines[idx] or 'ç»“è¯­' in lines[idx] or 'Conclusion' in lines[idx].lower():
                conclusion_start = idx
                break
        
        current_conclusion = '\n'.join(lines[conclusion_start:]) if conclusion_start != -1 else "æœªæ‰¾åˆ°æ˜ç¡®çš„ç»“è®ºéƒ¨åˆ†"
        
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡ç¼–è¾‘ã€‚è¯·æ ¹æ®æ•´ç¯‡è®ºæ–‡çš„å†…å®¹ï¼Œæ”¹è¿›ç»“è®ºéƒ¨åˆ†ï¼Œä½¿å…¶èƒ½å¤Ÿ"overcap"ï¼ˆå…¨é¢æ€»ç»“ï¼‰æ•´ç¯‡æ–‡ç« ã€‚

ç”¨æˆ·è¦æ±‚ï¼š
{user_req}

æ–‡æ¡£æ•´ä½“ç»“æ„ï¼š
{structure}

å½“å‰ç»“è®ºå†…å®¹ï¼š
{current_conclusion}

å®Œæ•´è®ºæ–‡å†…å®¹ï¼š
{doc_text}

æ”¹è¿›è¦æ±‚ï¼š
1. **é€ç« æ¦‚æ‹¬**ï¼ˆæ ¸å¿ƒè¦æ±‚ï¼‰ï¼š
   - å¿…é¡»ç³»ç»Ÿåœ°å›é¡¾æ¯ä¸€ç« çš„ä¸»è¦å†…å®¹å’Œè´¡çŒ®
   - ç¬¬äºŒç« ï¼šè¯´æ˜äº†ä»€ä¹ˆé—®é¢˜/é‡‡ç”¨äº†ä»€ä¹ˆæ–¹æ³•
   - ç¬¬ä¸‰ç« ï¼šæ­ç¤ºäº†ä»€ä¹ˆæ ¹æº/æä¾›äº†ä»€ä¹ˆç†è®º
   - ç¬¬å››ç« ï¼šè®ºè¯äº†ä»€ä¹ˆè§‚ç‚¹/å¼•å…¥äº†ä»€ä¹ˆæ¦‚å¿µ
   - ç¬¬äº”ç« ï¼šæå‡ºäº†ä»€ä¹ˆè§£å†³æ–¹æ¡ˆ
   
2. **ç†è®ºç»¼åˆ**ï¼š
   - é˜æ˜å„ç« èŠ‚ä¹‹é—´çš„é€»è¾‘é€’è¿›å…³ç³»
   - è¯´æ˜ç†è®ºæ¡†æ¶æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥å»ºç«‹çš„
   - æŒ‡å‡ºå…³é”®è½¬æŠ˜ç‚¹å’Œæ¢çº½æ€§è®ºè¯

3. **è´¡çŒ®æ˜ç¡®**ï¼š
   - æ¦‚æ‹¬ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®
   - æŒ‡å‡ºç†è®ºåˆ›æ–°ç‚¹
   - è¯´æ˜å®è·µæ„ä¹‰

4. **å±€é™ä¸å±•æœ›**ï¼š
   - å¦è¯šæŒ‡å‡ºç ”ç©¶çš„å±€é™æ€§
   - æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘
   - ä¿æŒå­¦æœ¯è°¦é€Š

è¯·ç›´æ¥è¾“å‡ºæ”¹è¿›åçš„ç»“è®ºå†…å®¹ï¼Œè¦æ±‚èƒ½å¤Ÿè®©è¯»è€…ä»…é€šè¿‡ç»“è®ºå°±èƒ½ç†è§£å…¨æ–‡çš„æ ¸å¿ƒè®ºè¯è·¯å¾„ï¼Œæ— éœ€å…¶ä»–è¯´æ˜ã€‚"""

    def _generate_revision_prompt(self, doc_text: str, structure: str, user_req: str) -> str:
        """ç”Ÿæˆä¸€èˆ¬æ€§ä¿®æ”¹ä»»åŠ¡çš„ä¸“ç”¨æç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£ç¼–è¾‘ä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å¯¹æ–‡æ¡£è¿›è¡Œæ”¹è¿›ã€‚

ç”¨æˆ·è¦æ±‚ï¼š
{user_req}

æ–‡æ¡£ç»“æ„ï¼š
{structure}

æ–‡æ¡£å†…å®¹ï¼š
{doc_text}

æ”¹è¿›åŸåˆ™ï¼š
1. å¿ å®ç”¨æˆ·æ„å›¾ï¼Œç²¾å‡†ç†è§£è¦æ±‚
2. ä¿æŒæ–‡æ¡£åŸæœ‰é£æ ¼å’Œå­¦æœ¯æ°´å‡†
3. æ”¹è¿›åº”è¯¥æœ‰é’ˆå¯¹æ€§ï¼Œä¸è¦æ³›æ³›è€Œè°ˆ
4. å¦‚æœæ˜¯å­¦æœ¯æ–‡æ¡£ï¼Œä¿æŒå­¦æœ¯è§„èŒƒ
5. å¦‚æœæ¶‰åŠç»“æ„è°ƒæ•´ï¼Œè¯´æ˜è°ƒæ•´ç†ç”±

è¯·ç›´æ¥è¾“å‡ºæ”¹è¿›å»ºè®®æˆ–æ”¹è¿›åçš„å†…å®¹ï¼ˆæ ¹æ®ç”¨æˆ·è¦æ±‚åˆ¤æ–­ï¼‰ï¼Œæ— éœ€å…¶ä»–è¯´æ˜ã€‚"""

    def _generate_analysis_prompt(self, doc_text: str, structure: str, user_req: str) -> str:
        """ç”Ÿæˆåˆ†æä»»åŠ¡çš„ä¸“ç”¨æç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚å¯¹æ–‡æ¡£è¿›è¡Œæ·±å…¥åˆ†æã€‚

ç”¨æˆ·è¦æ±‚ï¼š
{user_req}

æ–‡æ¡£ç»“æ„ï¼š
{structure}

æ–‡æ¡£å†…å®¹ï¼š
{doc_text}

åˆ†æè¦æ±‚ï¼š
1. ç»“æ„åˆ†æï¼šåˆ†ææ–‡æ¡£çš„æ•´ä½“ç»“æ„å’Œé€»è¾‘
2. å†…å®¹åˆ†æï¼šæç‚¼æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®è®ºè¯
3. è´¨é‡è¯„ä¼°ï¼šè¯„ä¼°æ–‡æ¡£çš„å®Œæ•´æ€§å’Œé€»è¾‘ä¸¥å¯†æ€§
4. æ”¹è¿›å»ºè®®ï¼šæå‡ºå…·ä½“çš„æ”¹è¿›æ–¹å‘

è¯·æä¾›ç³»ç»Ÿæ€§çš„åˆ†æç»“æœã€‚"""

    async def process_document_intelligent_streaming(
        self,
        doc_path: str,
        user_input: str,
        session_name: str = None
    ) -> AsyncGenerator[Dict, None]:
        """
        æ™ºèƒ½æµå¼å¤„ç†æ–‡æ¡£åˆ†æè¯·æ±‚
        æ ¹æ®ç”¨æˆ·æ„å›¾ï¼Œå¯ä»¥ï¼š
        1. ç”Ÿæˆæ–°æ–‡æœ¬ï¼ˆæ‘˜è¦/å¼•è¨€/ç»“è®ºï¼‰å¹¶ç›´æ¥è¿”å›
        2. ä¿®æ”¹æ–‡æ¡£å¹¶æ ‡çº¢
        3. è¿”å›åˆ†æç»“æœ
        
        Args:
            doc_path: æ–‡æ¡£è·¯å¾„
            user_input: ç”¨æˆ·è¾“å…¥
            session_name: ä¼šè¯åç§°
            
        Yields:
            è¿›åº¦äº‹ä»¶å­—å…¸
        """
        from web.document_reader import DocumentReader
        
        yield {
            'stage': 'reading',
            'progress': 10,
            'message': 'ğŸ“– æ­£åœ¨è¯»å–æ–‡æ¡£ç»“æ„...'
        }
        
        # è¯»å–æ–‡æ¡£ç»“æ„
        doc_structure = DocumentReader.read_word(doc_path)
        if not doc_structure.get('success'):
            yield {
                'stage': 'error',
                'message': f"è¯»å–æ–‡æ¡£å¤±è´¥: {doc_structure.get('error')}"
            }
            return
        
        yield {
            'stage': 'analyzing',
            'progress': 20,
            'message': 'ğŸ” åˆ†æç”¨æˆ·éœ€æ±‚å’Œæ„å›¾...'
        }
        
        # åˆ†æç”¨æˆ·è¯·æ±‚
        request_analysis = self.analyze_request(user_input, doc_structure)
        tasks = request_analysis['tasks']
        output_type = self._determine_output_type(tasks)
        
        yield {
            'stage': 'planning',
            'progress': 30,
            'message': f'ğŸ“‹ è¯†åˆ«åˆ° {len(tasks)} ä¸ªä»»åŠ¡ï¼Œè¾“å‡ºæ¨¡å¼: {output_type}',
            'detail': json.dumps(tasks, ensure_ascii=False)
        }
        
        # å¤„ç†æ¯ä¸ªä»»åŠ¡
        all_results = {}
        generated_contents = []
        
        for task_idx, task in enumerate(tasks):
            progress_base = 30 + (task_idx * 40 // len(tasks))
            
            yield {
                'stage': 'generating',
                'progress': progress_base,
                'message': f'âœï¸ æ­£åœ¨å¤„ç†: {task["description"]}'
            }
            
            # ç”Ÿæˆä¸“ç”¨æç¤ºè¯
            specialized_prompt = self.generate_specialized_prompt(task, doc_structure, user_input)
            
            # è°ƒç”¨LLMç”Ÿæˆå†…å®¹
            response = await self._call_llm(specialized_prompt)
            
            all_results[task['type']] = {
                'task': task,
                'generated_content': response
            }
            generated_contents.append({
                'task_type': task['type'],
                'task_description': task['description'],
                'content': response
            })
            
            yield {
                'stage': 'task_complete',
                'progress': progress_base + 35 // len(tasks),
                'message': f'âœ… å®Œæˆ: {task["type"]}',
                'detail': response[:200] + '...' if len(response) > 200 else response
            }
        
        yield {
            'stage': 'processing',
            'progress': 80,
            'message': f'ğŸ“ å¤„ç†è¾“å‡º (æ¨¡å¼: {output_type})...'
        }
        
        # æ ¹æ®è¾“å‡ºç±»å‹å¤„ç†ç»“æœ
        if output_type == 'generate':
            # ç”Ÿæˆæ¨¡å¼ï¼šç›´æ¥è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
            yield {
                'stage': 'complete',
                'progress': 100,
                'message': 'âœ… æ–‡æœ¬ç”Ÿæˆå®Œæˆ',
                'result': {
                    'output_type': 'generated_texts',
                    'tasks_completed': len(tasks),
                    'generated_contents': generated_contents
                }
            }
        
        elif output_type == 'modify':
            # ä¿®æ”¹æ¨¡å¼ï¼šåº”ç”¨åˆ°æ–‡æ¡£å¹¶æ ‡çº¢
            output_path = await self._apply_revisions_with_red_marking(
                doc_path,
                all_results,
                doc_structure,
                os.path.join(os.path.dirname(__file__), "..", "workspace", "documents")
            )
            
            yield {
                'stage': 'complete',
                'progress': 100,
                'message': 'âœ… æ–‡æ¡£ä¿®è®¢å®Œæˆ',
                'result': {
                    'output_type': 'modified_document',
                    'output_file': output_path,
                    'tasks_completed': len(tasks),
                    'revisions': list(all_results.keys())
                }
            }
        
        else:  # analysis
            # åˆ†ææ¨¡å¼ï¼šè¿”å›åˆ†æç»“æœ
            yield {
                'stage': 'complete',
                'progress': 100,
                'message': 'âœ… åˆ†æå®Œæˆ',
                'result': {
                    'output_type': 'analysis_results',
                    'tasks_completed': len(tasks),
                    'analysis': generated_contents
                }
            }
    
    def _determine_output_type(self, tasks: List[Dict]) -> str:
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šè¾“å‡ºæ–¹å¼
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨
            
        Returns:
            'generate' - ç”Ÿæˆæ–°æ–‡æœ¬å¹¶ç›´æ¥è¿”å›
            'modify' - ä¿®æ”¹æ–‡æ¡£å¹¶æ ‡çº¢
            'analysis' - è¿”å›åˆ†æç»“æœ
        """
        if not tasks:
            return 'analysis'
        
        task_types = [t['type'] for t in tasks]
        
        # åŒ…å«"å†™"æˆ–"æ”¹"çš„ä»»åŠ¡ -> ç”Ÿæˆæ¨¡å¼
        generate_types = {'write_abstract', 'revise_intro', 'revise_conclusion', 'general_revision'}
        if any(t in generate_types for t in task_types):
            return 'generate'
        
        # åˆ†æç±»ä»»åŠ¡ -> åˆ†ææ¨¡å¼
        if 'analysis' in task_types:
            return 'analysis'
        
        # é»˜è®¤ç”Ÿæˆ
        return 'generate'
    
    def _replace_paragraph_with_diff(self, paragraph, new_text: str):
        """ç”¨diffå¯¹æ¯”æ–¹å¼æ›¿æ¢æ®µè½å¹¶æ ‡çº¢ä¿®æ”¹éƒ¨åˆ†"""
        old_text = paragraph.text
        
        # æŒ‰å¥å­åˆ†å‰²
        old_sentences = self._split_sentences(old_text)
        new_sentences = self._split_sentences(new_text)
        
        # ä½¿ç”¨difflibå¯¹æ¯”
        matcher = difflib.SequenceMatcher(None, old_sentences, new_sentences)
        
        # æ¸…ç©ºæ®µè½
        paragraph.clear()
        
        # æ ¹æ®diffç»“æœé‡å»ºæ®µè½
        for tag, i1, i2, j1, j2 in matcher.get_opcodes():
            if tag == 'equal':
                # ä¿ç•™çš„å¥å­ - é»‘è‰²
                for sent in new_sentences[j1:j2]:
                    run = paragraph.add_run(sent)
                    run.font.color.rgb = RGBColor(0, 0, 0)
            elif tag in ('replace', 'insert'):
                # æ–°å¢/ä¿®æ”¹çš„å¥å­ - çº¢è‰²
                for sent in new_sentences[j1:j2]:
                    run = paragraph.add_run(sent)
                    run.font.color.rgb = RGBColor(255, 0, 0)
    
    def _split_sentences(self, text: str) -> List[str]:
        """æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬"""
        # æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å‰²
        parts = re.split(r'((?:[\u3002\uff01\uff1f\uff1b]|\.(?:\s|$)))', text)
        sentences = []
        i = 0
        while i < len(parts):
            s = parts[i]
            if i + 1 < len(parts):
                s += parts[i + 1]
                i += 2
            else:
                i += 1
            s = s.strip()
            if s:
                sentences.append(s)
        return sentences


def create_intelligent_analyzer(llm_client) -> IntelligentDocumentAnalyzer:
    """å·¥å‚å‡½æ•°ï¼šåˆ›å»ºæ™ºèƒ½æ–‡æ¡£åˆ†æå™¨å®ä¾‹"""
    return IntelligentDocumentAnalyzer(llm_client)
