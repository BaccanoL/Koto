#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¦‚å¿µæå–æ¨¡å— - Kotoæ™ºèƒ½æ–‡ä»¶å¤§è„‘çš„æ ¸å¿ƒ
ä½¿ç”¨TF-IDFç®—æ³•ä»æ–‡ä»¶å†…å®¹ä¸­æå–å…³é”®æ¦‚å¿µå’Œä¸»é¢˜
"""

import re
import math
import sqlite3
from typing import List, Dict, Tuple, Set
from collections import Counter, defaultdict
from pathlib import Path
import json
from datetime import datetime

# ç®€åŒ–çš„ä¸­æ–‡åœç”¨è¯è¡¨
CHINESE_STOPWORDS = {
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
    'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
    'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'ä¸º', 'èƒ½', 'è¿™ä¸ª', 'ä¸', 'åŠ', 'è€Œ', 'æˆ–', 'ç­‰',
    'å¯ä»¥', 'ä½†', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è¿™æ ·', 'é‚£æ ·', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ',
    'how', 'the', 'is', 'at', 'which', 'on', 'a', 'an', 'and', 'or', 'but', 'in',
    'with', 'to', 'for', 'of', 'as', 'by', 'from', 'that', 'this', 'it', 'be', 'are'
}

class ConceptExtractor:
    """æ¦‚å¿µæå–å™¨ - ä½¿ç”¨TF-IDFç®—æ³•æå–æ–‡ä»¶å…³é”®æ¦‚å¿µ"""
    
    def __init__(self, db_path: str = "config/concepts.db"):
        """
        åˆå§‹åŒ–æ¦‚å¿µæå–å™¨
        
        Args:
            db_path: SQLiteæ•°æ®åº“è·¯å¾„ï¼Œç”¨äºå­˜å‚¨æ¦‚å¿µå’Œæ–‡ä»¶å…³è”
        """
        self.db_path = db_path
        self._ensure_db()
        
    def _ensure_db(self):
        """ç¡®ä¿æ•°æ®åº“å’Œè¡¨ç»“æ„å­˜åœ¨"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ–‡ä»¶æ¦‚å¿µè¡¨ - å­˜å‚¨æ¯ä¸ªæ–‡ä»¶æå–çš„æ¦‚å¿µ
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS file_concepts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_path TEXT NOT NULL,
                concept TEXT NOT NULL,
                tf_idf_score REAL NOT NULL,
                extraction_time TEXT NOT NULL,
                UNIQUE(file_path, concept)
            )
        """)
        
        # æ¦‚å¿µç»Ÿè®¡è¡¨ - å­˜å‚¨å…¨å±€æ¦‚å¿µç»Ÿè®¡
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS concept_stats (
                concept TEXT PRIMARY KEY,
                document_frequency INTEGER DEFAULT 1,
                total_occurrences INTEGER DEFAULT 0,
                last_updated TEXT
            )
        """)
        
        # æ–‡ä»¶å…ƒæ•°æ®è¡¨ - å­˜å‚¨æ–‡ä»¶å¤„ç†ä¿¡æ¯
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS file_metadata (
                file_path TEXT PRIMARY KEY,
                total_words INTEGER,
                unique_concepts INTEGER,
                last_analyzed TEXT,
                content_hash TEXT
            )
        """)
        
        # åˆ›å»ºç´¢å¼•åŠ é€ŸæŸ¥è¯¢
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_file_concepts_path ON file_concepts(file_path)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_file_concepts_concept ON file_concepts(concept)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_file_concepts_score ON file_concepts(tf_idf_score DESC)")
        
        conn.commit()
        conn.close()
    
    def tokenize(self, text: str) -> List[str]:
        """
        åˆ†è¯ - æ”¯æŒä¸­è‹±æ–‡æ··åˆ
        
        Args:
            text: è¦åˆ†è¯çš„æ–‡æœ¬
            
        Returns:
            è¯è¯­åˆ—è¡¨
        """
        # å°è¯•å¯¼å…¥jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        try:
            import jieba
            # ä½¿ç”¨jiebaåˆ†è¯
            words = list(jieba.cut(text))
        except ImportError:
            # å¦‚æœæ²¡æœ‰jiebaï¼Œä½¿ç”¨ç®€å•çš„æ­£åˆ™åˆ†è¯
            # æå–ä¸­æ–‡å­—ç¬¦ï¼ˆ2-3ä¸ªå­—çš„è¯ï¼‰å’Œè‹±æ–‡å•è¯
            chinese_pattern = r'[\u4e00-\u9fff]{2,3}'
            english_pattern = r'\b[a-zA-Z]{3,}\b'
            
            chinese_words = re.findall(chinese_pattern, text)
            english_words = re.findall(english_pattern, text.lower())
            
            words = chinese_words + english_words
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [
            w.strip().lower() for w in words 
            if len(w.strip()) >= 2 and w.strip().lower() not in CHINESE_STOPWORDS
        ]
        
        return filtered_words
    
    def calculate_tf(self, words: List[str]) -> Dict[str, float]:
        """
        è®¡ç®—è¯é¢‘(TF - Term Frequency)
        
        Args:
            words: è¯è¯­åˆ—è¡¨
            
        Returns:
            {è¯è¯­: TFå€¼} å­—å…¸
        """
        if not words:
            return {}
        
        word_count = Counter(words)
        total_words = len(words)
        
        # TF = è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°æ¬¡æ•° / æ–‡æ¡£æ€»è¯æ•°
        tf_dict = {word: count / total_words for word, count in word_count.items()}
        
        return tf_dict
    
    def get_idf(self, concept: str) -> float:
        """
        è·å–é€†æ–‡æ¡£é¢‘ç‡(IDF - Inverse Document Frequency)
        
        Args:
            concept: æ¦‚å¿µ/è¯è¯­
            
        Returns:
            IDFå€¼
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–åŒ…å«è¯¥æ¦‚å¿µçš„æ–‡æ¡£æ•°
        cursor.execute("SELECT document_frequency FROM concept_stats WHERE concept = ?", (concept,))
        result = cursor.fetchone()
        
        # è·å–æ€»æ–‡æ¡£æ•°
        cursor.execute("SELECT COUNT(DISTINCT file_path) FROM file_metadata")
        total_docs = cursor.fetchone()[0] or 1
        
        conn.close()
        
        if result:
            doc_freq = result[0]
        else:
            doc_freq = 1  # æ–°æ¦‚å¿µï¼Œå‡è®¾å‡ºç°åœ¨1ä¸ªæ–‡æ¡£ä¸­
        
        # IDF = log(æ€»æ–‡æ¡£æ•° / (åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•° + 1))
        idf = math.log((total_docs + 1) / (doc_freq + 1)) + 1
        
        return idf
    
    def extract_concepts(self, text: str, top_n: int = 10) -> List[Tuple[str, float]]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®æ¦‚å¿µ
        
        Args:
            text: è¦åˆ†æçš„æ–‡æœ¬
            top_n: è¿”å›å‰Nä¸ªå…³é”®æ¦‚å¿µ
            
        Returns:
            [(æ¦‚å¿µ, TF-IDFåˆ†æ•°), ...] æŒ‰åˆ†æ•°é™åºæ’åˆ—
        """
        # åˆ†è¯
        words = self.tokenize(text)
        
        if not words:
            return []
        
        # è®¡ç®—TF
        tf_dict = self.calculate_tf(words)
        
        # è®¡ç®—TF-IDF
        tfidf_scores = {}
        for word, tf in tf_dict.items():
            idf = self.get_idf(word)
            tfidf_scores[word] = tf * idf
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œè¿”å›topN
        sorted_concepts = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_concepts[:top_n]
    
    def analyze_file(self, file_path: str, content: str = None) -> Dict:
        """
        åˆ†ææ–‡ä»¶å¹¶æå–æ¦‚å¿µ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            content: æ–‡ä»¶å†…å®¹ï¼ˆå¦‚æœå·²è¯»å–ï¼‰
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        # å¦‚æœæ²¡æœ‰æä¾›å†…å®¹ï¼Œå°è¯•è¯»å–æ–‡ä»¶
        if content is None:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
            except:
                try:
                    with open(file_path, 'r', encoding='gbk') as f:
                        content = f.read()
                except Exception as e:
                    return {"error": f"æ— æ³•è¯»å–æ–‡ä»¶: {str(e)}"}
        
        # è®¡ç®—å†…å®¹hash
        import hashlib
        content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²åˆ†æè¿‡ä¸”å†…å®¹æœªå˜
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute(
            "SELECT content_hash, last_analyzed FROM file_metadata WHERE file_path = ?",
            (file_path,)
        )
        result = cursor.fetchone()
        
        if result and result[0] == content_hash:
            # å†…å®¹æœªå˜ï¼Œè¿”å›ç¼“å­˜çš„æ¦‚å¿µ
            cursor.execute(
                "SELECT concept, tf_idf_score FROM file_concepts WHERE file_path = ? ORDER BY tf_idf_score DESC",
                (file_path,)
            )
            concepts = cursor.fetchall()
            conn.close()
            
            return {
                "file_path": file_path,
                "concepts": [{"concept": c[0], "score": c[1]} for c in concepts],
                "cached": True,
                "analyzed_at": result[1]
            }
        
        conn.close()
        
        # æå–æ–°æ¦‚å¿µ
        concepts = self.extract_concepts(content, top_n=20)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self._save_concepts(file_path, concepts, content_hash, len(self.tokenize(content)))
        
        return {
            "file_path": file_path,
            "concepts": [{"concept": c[0], "score": c[1]} for c in concepts],
            "cached": False,
            "analyzed_at": datetime.now().isoformat()
        }
    
    def _save_concepts(self, file_path: str, concepts: List[Tuple[str, float]], 
                       content_hash: str, total_words: int):
        """ä¿å­˜æå–çš„æ¦‚å¿µåˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        current_time = datetime.now().isoformat()
        
        # åˆ é™¤æ—§æ¦‚å¿µ
        cursor.execute("DELETE FROM file_concepts WHERE file_path = ?", (file_path,))
        
        # æ’å…¥æ–°æ¦‚å¿µ
        for concept, score in concepts:
            cursor.execute("""
                INSERT OR REPLACE INTO file_concepts (file_path, concept, tf_idf_score, extraction_time)
                VALUES (?, ?, ?, ?)
            """, (file_path, concept, score, current_time))
            
            # æ›´æ–°å…¨å±€æ¦‚å¿µç»Ÿè®¡
            cursor.execute("""
                INSERT INTO concept_stats (concept, document_frequency, total_occurrences, last_updated)
                VALUES (?, 1, 1, ?)
                ON CONFLICT(concept) DO UPDATE SET
                    document_frequency = document_frequency + 1,
                    total_occurrences = total_occurrences + 1,
                    last_updated = ?
            """, (concept, current_time, current_time))
        
        # æ›´æ–°æ–‡ä»¶å…ƒæ•°æ®
        cursor.execute("""
            INSERT OR REPLACE INTO file_metadata 
            (file_path, total_words, unique_concepts, last_analyzed, content_hash)
            VALUES (?, ?, ?, ?, ?)
        """, (file_path, total_words, len(concepts), current_time, content_hash))
        
        conn.commit()
        conn.close()
    
    def get_file_concepts(self, file_path: str, limit: int = 10) -> List[Dict]:
        """
        è·å–æ–‡ä»¶çš„æ¦‚å¿µ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            æ¦‚å¿µåˆ—è¡¨
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT concept, tf_idf_score, extraction_time
            FROM file_concepts
            WHERE file_path = ?
            ORDER BY tf_idf_score DESC
            LIMIT ?
        """, (file_path, limit))
        
        concepts = []
        for row in cursor.fetchall():
            concepts.append({
                "concept": row[0],
                "score": row[1],
                "extracted_at": row[2]
            })
        
        conn.close()
        return concepts
    
    def find_related_files(self, file_path: str, limit: int = 5) -> List[Dict]:
        """
        æŸ¥æ‰¾ä¸æŒ‡å®šæ–‡ä»¶ç›¸å…³çš„å…¶ä»–æ–‡ä»¶ï¼ˆåŸºäºå…±äº«æ¦‚å¿µï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            ç›¸å…³æ–‡ä»¶åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åº
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–è¯¥æ–‡ä»¶çš„æ¦‚å¿µ
        cursor.execute("""
            SELECT concept, tf_idf_score FROM file_concepts WHERE file_path = ?
        """, (file_path,))
        
        file_concepts = {row[0]: row[1] for row in cursor.fetchall()}
        
        if not file_concepts:
            conn.close()
            return []
        
        # æŸ¥æ‰¾å…±äº«æ¦‚å¿µçš„å…¶ä»–æ–‡ä»¶
        concepts_str = ','.join(['?' for _ in file_concepts.keys()])
        cursor.execute(f"""
            SELECT file_path, concept, tf_idf_score
            FROM file_concepts
            WHERE concept IN ({concepts_str}) AND file_path != ?
        """, (*file_concepts.keys(), file_path))
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        file_vectors = defaultdict(dict)
        for row in cursor.fetchall():
            other_file, concept, score = row
            file_vectors[other_file][concept] = score
        
        # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
        similarities = []
        for other_file, other_concepts in file_vectors.items():
            # è®¡ç®—å…±äº«æ¦‚å¿µçš„åŠ æƒå¾—åˆ†
            shared_score = sum(
                file_concepts.get(c, 0) * score 
                for c, score in other_concepts.items()
            )
            
            # å½’ä¸€åŒ–
            norm1 = math.sqrt(sum(v**2 for v in file_concepts.values()))
            norm2 = math.sqrt(sum(v**2 for v in other_concepts.values()))
            
            if norm1 > 0 and norm2 > 0:
                similarity = shared_score / (norm1 * norm2)
                
                # è·å–å…±äº«æ¦‚å¿µ
                shared_concepts = set(file_concepts.keys()) & set(other_concepts.keys())
                
                similarities.append({
                    "file_path": other_file,
                    "similarity": similarity,
                    "shared_concepts": list(shared_concepts)[:5]  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                })
        
        conn.close()
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:limit]
    
    def get_top_concepts(self, limit: int = 20) -> List[Dict]:
        """
        è·å–å…¨å±€æœ€çƒ­é—¨çš„æ¦‚å¿µ
        
        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶
            
        Returns:
            æ¦‚å¿µåˆ—è¡¨
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT concept, document_frequency, total_occurrences, last_updated
            FROM concept_stats
            ORDER BY document_frequency DESC, total_occurrences DESC
            LIMIT ?
        """, (limit,))
        
        concepts = []
        for row in cursor.fetchall():
            concepts.append({
                "concept": row[0],
                "document_count": row[1],
                "total_occurrences": row[2],
                "last_updated": row[3]
            })
        
        conn.close()
        return concepts
    
    def get_statistics(self) -> Dict:
        """è·å–æ¦‚å¿µæå–å™¨çš„ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM file_metadata")
        total_files = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM concept_stats")
        total_concepts = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM file_concepts")
        total_relations = cursor.fetchone()[0]
        
        cursor.execute("SELECT AVG(unique_concepts) FROM file_metadata")
        avg_concepts_per_file = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            "total_files_analyzed": total_files,
            "total_unique_concepts": total_concepts,
            "total_concept_relations": total_relations,
            "avg_concepts_per_file": round(avg_concepts_per_file, 2)
        }


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    extractor = ConceptExtractor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚
    ç¥ç»ç½‘ç»œæ¨¡å‹å¯ä»¥å¤„ç†å¤æ‚çš„æ•°æ®ã€‚è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€ã€‚
    è®¡ç®—æœºè§†è§‰æŠ€æœ¯èƒ½å¤Ÿè¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“ã€‚å¼ºåŒ–å­¦ä¹ ç”¨äºè®­ç»ƒæ™ºèƒ½ä»£ç†ã€‚
    """
    
    print("ğŸ§  æ¦‚å¿µæå–æµ‹è¯•")
    print("=" * 50)
    
    concepts = extractor.extract_concepts(test_text, top_n=10)
    print("\næå–çš„å…³é”®æ¦‚å¿µï¼š")
    for concept, score in concepts:
        print(f"  â€¢ {concept}: {score:.4f}")
    
    print("\n" + "=" * 50)
    print("âœ… æ¦‚å¿µæå–æ¨¡å—å·²å°±ç»ª")
