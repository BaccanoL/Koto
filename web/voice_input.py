#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è½»é‡çº§è¯­éŸ³è¾“å…¥æ¨¡å— - é€‚é… Koto æ‰“åŒ…ç¯å¢ƒ
æ”¯æŒå¤šç§é™çº§æ–¹æ¡ˆï¼Œç¡®ä¿åœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½æä¾›åé¦ˆ
"""
import os
import sys
import json
import time
import tempfile
import traceback
from typing import Dict, Optional, List
from dataclasses import dataclass
from enum import Enum


class EngineType(Enum):
    """å¼•æ“ç±»å‹"""
    VOSK_LOCAL = "vosk"         # Vosk æœ¬åœ°ç¦»çº¿è¯†åˆ«ï¼ˆæœ€å¿«ï¼‰
    WINDOWS_SPEECH = "windows"  # Windows ç³»ç»Ÿè¯­éŸ³è¯†åˆ«
    GOOGLE_WEB = "google"       # Google Web Speech API (æµè§ˆå™¨ç«¯)
    GEMINI_API = "gemini"       # Gemini API è¯†åˆ«éŸ³é¢‘
    OFFLINE = "offline"         # å®Œå…¨ç¦»çº¿ï¼ˆä»…å½•éŸ³ï¼‰


@dataclass
class RecognitionResult:
    """è¯†åˆ«ç»“æœ"""
    success: bool
    text: str = ""
    engine: str = ""
    message: str = ""
    audio_file: Optional[str] = None
    confidence: float = 0.0
    
    def to_dict(self) -> Dict:
        return {
            "success": self.success,
            "text": self.text,
            "engine": self.engine,
            "message": self.message,
            "audio_file": self.audio_file,
            "confidence": self.confidence
        }


class VoiceInputEngine:
    """è½»é‡çº§è¯­éŸ³è¾“å…¥å¼•æ“ - ä¸“ä¸ºæ‰“åŒ…ç¯å¢ƒè®¾è®¡"""
    
    def __init__(self):
        self.available_engines: List[EngineType] = []
        self.primary_engine: Optional[EngineType] = None
        self.vosk_model = None  # Vosk æ¨¡å‹ç¼“å­˜
        self.vosk_model_path = None
        self._detect_engines()
        
    def _detect_engines(self):
        """æ£€æµ‹å¯ç”¨çš„è¯­éŸ³å¼•æ“"""
        print("\n[è¯­éŸ³è¾“å…¥] æ­£åœ¨æ£€æµ‹å¯ç”¨å¼•æ“...")
        
        # 0. æ£€æµ‹ Vosk æœ¬åœ°è¯†åˆ«ï¼ˆæœ€å¿«ï¼Œå®Œå…¨ç¦»çº¿ï¼‰
        if self._check_vosk():
            self.available_engines.append(EngineType.VOSK_LOCAL)
            print("  âœ“ Vosk æœ¬åœ°è¯†åˆ«å¯ç”¨ï¼ˆæ¨èï¼Œæœ€å¿«ï¼‰")
        
        # 1. æ£€æµ‹æœ¬åœ° speech_recognition + pyaudio
        if self._check_speech_recognition():
            self.available_engines.append(EngineType.GOOGLE_WEB)
            print("  âœ“ Google Speech API å¯ç”¨ (éœ€ç½‘ç»œ)")
        
        # 2. æ£€æµ‹ Windows è¯­éŸ³è¯†åˆ«
        if self._check_windows_speech():
            self.available_engines.append(EngineType.WINDOWS_SPEECH)
            print("  âœ“ Windows è¯­éŸ³è¯†åˆ«å¯ç”¨")
        
        # 3. æ£€æµ‹ Gemini API
        if self._check_gemini_api():
            self.available_engines.append(EngineType.GEMINI_API)
            print("  âœ“ Gemini API å¯ç”¨")
        
        # 4. ç¦»çº¿æ¨¡å¼ï¼ˆå…œåº•ï¼‰
        self.available_engines.append(EngineType.OFFLINE)
        print("  âœ“ ç¦»çº¿å½•éŸ³æ¨¡å¼å¯ç”¨")
        
        # è®¾ç½®ä¸»å¼•æ“ - ä¼˜å…ˆ Vosk æœ¬åœ°è¯†åˆ«ï¼ˆæœ€å¿«ï¼‰
        if EngineType.VOSK_LOCAL in self.available_engines:
            self.primary_engine = EngineType.VOSK_LOCAL  # Vosk æœ€å¿«
        elif EngineType.GOOGLE_WEB in self.available_engines:
            self.primary_engine = EngineType.GOOGLE_WEB
        elif EngineType.WINDOWS_SPEECH in self.available_engines:
            self.primary_engine = EngineType.WINDOWS_SPEECH
        elif EngineType.GEMINI_API in self.available_engines:
            self.primary_engine = EngineType.GEMINI_API
        else:
            self.primary_engine = EngineType.OFFLINE
        
        print(f"\n[è¯­éŸ³è¾“å…¥] ä¸»å¼•æ“: {self.primary_engine.value}")
        print(f"[è¯­éŸ³è¾“å…¥] å¯ç”¨å¼•æ“: {[e.value for e in self.available_engines]}")
    
    def _check_vosk(self) -> bool:
        """æ£€æŸ¥ Vosk æœ¬åœ°è¯†åˆ«æ˜¯å¦å¯ç”¨"""
        # åœ¨æ‰“åŒ…ç¯å¢ƒä¸­ç¦ç”¨ Voskï¼ˆvoskåœ¨PyInstallerä¸­æœ‰ä¾èµ–é—®é¢˜ï¼‰
        if getattr(sys, 'frozen', False):
            print("  âš  æ‰“åŒ…ç¯å¢ƒä¸­ç¦ç”¨ Voskï¼ˆä¾èµ–é—®é¢˜ï¼‰")
            return False
            
        try:
            from vosk import Model, KaldiRecognizer
            import pyaudio
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„ï¼ˆåŒ…å«ç‰ˆæœ¬å·çš„è·¯å¾„ï¼‰
            base_dir = os.path.dirname(__file__)
            model_paths = [
                os.path.join(base_dir, "..", "models", "vosk-model-small-cn-0.22"),
                os.path.join(base_dir, "..", "models", "vosk-model-small-cn"),
                os.path.join(base_dir, "..", "models", "vosk-model-cn-0.22"),
                os.path.join(base_dir, "..", "models", "vosk-model-cn"),
                os.path.expanduser("~/.cache/vosk/vosk-model-small-cn-0.22"),
                os.path.expanduser("~/.cache/vosk/vosk-model-cn-0.22"),
            ]
            
            for path in model_paths:
                abs_path = os.path.abspath(path)
                if os.path.exists(abs_path) and os.path.isdir(abs_path):
                    self.vosk_model_path = abs_path
                    print(f"  âœ“ æ‰¾åˆ° Vosk æ¨¡å‹: {abs_path}")
                    return True
            
            # æ¨¡å‹ä¸å­˜åœ¨ï¼Œä½† Vosk åº“å¯ç”¨ï¼Œå¯ä»¥ä¸‹è½½
            print("  âš  Vosk åº“å·²å®‰è£…ï¼Œä½†éœ€è¦ä¸‹è½½ä¸­æ–‡æ¨¡å‹")
            return True  # å…è®¸ä½¿ç”¨ï¼Œç¨åä¸‹è½½æ¨¡å‹
            
        except ImportError:
            return False
        except Exception as e:
            print(f"  âš  Vosk æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _check_speech_recognition(self) -> bool:
        """æ£€æŸ¥æœ¬åœ° speech_recognition + pyaudio æ˜¯å¦å¯ç”¨"""
        try:
            import speech_recognition as sr
            import pyaudio
            # æµ‹è¯•éº¦å…‹é£
            try:
                r = sr.Recognizer()
                with sr.Microphone() as source:
                    pass  # åªæµ‹è¯•èƒ½å¦æ‰“å¼€éº¦å…‹é£
                return True
            except Exception as e:
                print(f"  âš  éº¦å…‹é£æµ‹è¯•å¤±è´¥: {e}")
                return False
        except ImportError as e:
            print(f"  âš  ç¼ºå°‘ä¾èµ–: {e}")
            return False
    
    def _check_windows_speech(self) -> bool:
        """æ£€æŸ¥ Windows è¯­éŸ³è¯†åˆ«æ˜¯å¦å¯ç”¨"""
        if sys.platform != "win32":
            return False
        
        try:
            # æ£€æŸ¥ Windows è¯­éŸ³è¯†åˆ« COM ç»„ä»¶
            import win32com.client
            try:
                # å°è¯•åˆ›å»ºè¯­éŸ³è¯†åˆ«å¯¹è±¡
                speech = win32com.client.Dispatch("SAPI.SpVoice")
                return True
            except:
                return False
        except ImportError:
            return False
    
    def _check_gemini_api(self) -> bool:
        """æ£€æŸ¥ Gemini API æ˜¯å¦é…ç½®"""
        try:
            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            config_path = os.path.join(os.path.dirname(__file__), "..", "config", "gemini_config.env")
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # ç®€å•æ£€æŸ¥æ˜¯å¦æœ‰ API key
                    if "GEMINI_API_KEY" in content and len(content) > 50:
                        return True
            return False
        except:
            return False
    
    def get_available_engines(self) -> Dict:
        """è·å–å¯ç”¨å¼•æ“åˆ—è¡¨"""
        return {
            "success": True,
            "engines": [
                {
                    "type": e.value,
                    "name": self._get_engine_name(e),
                    "is_primary": e == self.primary_engine,
                    "description": self._get_engine_description(e)
                }
                for e in self.available_engines
            ],
            "primary": self.primary_engine.value if self.primary_engine else None
        }
    
    def _get_engine_name(self, engine: EngineType) -> str:
        """è·å–å¼•æ“åç§°"""
        names = {
            EngineType.VOSK_LOCAL: "Vosk æœ¬åœ°è¯†åˆ«",
            EngineType.WINDOWS_SPEECH: "Windows è¯­éŸ³è¯†åˆ«",
            EngineType.GOOGLE_WEB: "Google Web Speech",
            EngineType.GEMINI_API: "Gemini API",
            EngineType.OFFLINE: "ç¦»çº¿å½•éŸ³"
        }
        return names.get(engine, engine.value)
    
    def _get_engine_description(self, engine: EngineType) -> str:
        """è·å–å¼•æ“æè¿°"""
        descriptions = {
            EngineType.VOSK_LOCAL: "å®Œå…¨ç¦»çº¿ï¼Œå“åº”æœ€å¿«ï¼ˆæ¨èï¼‰",
            EngineType.WINDOWS_SPEECH: "ä½¿ç”¨ Windows ç³»ç»Ÿå†…ç½®è¯­éŸ³è¯†åˆ«",
            EngineType.GOOGLE_WEB: "ä½¿ç”¨ Google API (éœ€ç½‘ç»œ)",
            EngineType.GEMINI_API: "ä½¿ç”¨ Gemini API è¯†åˆ«éŸ³é¢‘",
            EngineType.OFFLINE: "ä»…å½•éŸ³ï¼Œéœ€æ‰‹åŠ¨å¤„ç†"
        }
        return descriptions.get(engine, "")
    
    def recognize_microphone(self, timeout: int = 5, language: str = 'zh-CN') -> RecognitionResult:
        """ä»éº¦å…‹é£å®æ—¶è¯†åˆ« - ä¼˜å…ˆä½¿ç”¨æœ¬åœ°å¼•æ“"""
        # ä¼˜å…ˆä½¿ç”¨ Vosk æœ¬åœ°è¯†åˆ«ï¼ˆæœ€å¿«ï¼‰
        if self.primary_engine == EngineType.VOSK_LOCAL:
            result = self._recognize_with_vosk(timeout, language)
            if result.success:
                return result
            print("[è¯­éŸ³è¾“å…¥] Vosk å¤±è´¥ï¼Œå°è¯• Google...")
        
        # å…¶æ¬¡ Windows SAPI
        if self.primary_engine == EngineType.WINDOWS_SPEECH:
            result = self._recognize_with_windows_sapi(timeout, language)
            if result.success:
                return result
            print("[è¯­éŸ³è¾“å…¥] Windows SAPI å¤±è´¥ï¼Œå°è¯• Google...")
        
        # ä½¿ç”¨ speech_recognition + Google API
        return self._recognize_with_google(timeout, language)
    
    def _clean_chinese_text(self, text: str) -> str:
        """æ¸…ç†ä¸­æ–‡æ–‡æœ¬ï¼Œå»é™¤ä¸å¿…è¦çš„ç©ºæ ¼"""
        if not text:
            return text
        # å»é™¤ä¸­æ–‡å­—ç¬¦ä¹‹é—´çš„ç©ºæ ¼
        import re
        # åŒ¹é…ä¸­æ–‡å­—ç¬¦
        chinese_pattern = r'[\u4e00-\u9fff]'
        # å»é™¤ä¸­æ–‡å­—ç¬¦ä¹‹é—´çš„ç©ºæ ¼
        result = re.sub(f'({chinese_pattern})\\s+({chinese_pattern})', r'\1\2', text)
        # å¤šæ¬¡å¤„ç†ç¡®ä¿æ‰€æœ‰ç©ºæ ¼éƒ½è¢«å»é™¤
        while re.search(f'({chinese_pattern})\\s+({chinese_pattern})', result):
            result = re.sub(f'({chinese_pattern})\\s+({chinese_pattern})', r'\1\2', result)
        return result.strip()
    
    def _recognize_with_vosk(self, timeout: int = 5, language: str = 'zh-CN') -> RecognitionResult:
        """ä½¿ç”¨ Vosk æœ¬åœ°è¯†åˆ« - å®Œå…¨ç¦»çº¿ï¼Œè¶…å¿«å“åº”ï¼"""
        try:
            from vosk import Model, KaldiRecognizer, SetLogLevel
            import pyaudio
            import wave
            
            # ç¦ç”¨ Vosk æ—¥å¿—
            SetLogLevel(-1)
            
            # åŠ è½½æˆ–ä¸‹è½½æ¨¡å‹
            if not self.vosk_model:
                model_path = self._get_or_download_vosk_model()
                if not model_path:
                    return RecognitionResult(
                        success=False,
                        message="Vosk æ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¯·ç¨å€™è‡ªåŠ¨ä¸‹è½½...",
                        engine="vosk"
                    )
                print(f"[è¯­éŸ³è¾“å…¥] åŠ è½½ Vosk æ¨¡å‹: {model_path}")
                self.vosk_model = Model(model_path)
            
            # éŸ³é¢‘å‚æ•° - ä¼˜åŒ–çš„å‚æ•°ä»¥æé«˜è¯†åˆ«ç²¾å‡†åº¦
            RATE = 16000  # Vosk æ¨èé‡‡æ ·ç‡
            CHUNK = 1600  # 0.1ç§’ä¸€å—ï¼Œæ›´å¿«æ£€æµ‹é™éŸ³
            
            # åˆ›å»ºè¯†åˆ«å™¨ - é…ç½®æ›´é«˜çš„ç²¾å‡†åº¦è®¾ç½®
            rec = KaldiRecognizer(self.vosk_model, RATE)
            rec.SetWords(True)
            # å¯ç”¨æ›´è¯¦ç»†çš„ç»“æœï¼ˆå¦‚æœæ”¯æŒï¼‰
            try:
                rec.SetMaxAlternatives(3)  # è·å–å¤šä¸ªå€™é€‰ç»“æœ
                rec.SetPartialWords(True)  # å¯ç”¨éƒ¨åˆ†è¯æ±‡ç»“æœ
            except:
                pass  # æŸäº› Vosk ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒ
            
            # æ‰“å¼€éº¦å…‹é£
            p = pyaudio.PyAudio()
            stream = p.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK
            )
            
            print("[è¯­éŸ³è¾“å…¥] ğŸ¤ Vosk æœ¬åœ°è¯†åˆ«ä¸­...")
            
            # å®æ—¶è¯†åˆ« - ä½¿ç”¨æ›´æ™ºèƒ½çš„é™éŸ³æ£€æµ‹
            silence_count = 0
            max_silence = 15  # 15 * 0.1ç§’ = 1.5ç§’é™éŸ³å³åœæ­¢ï¼ˆå¢åŠ ç­‰å¾…æ—¶é—´ï¼‰
            has_speech = False
            start_time = time.time()
            final_text = ""
            last_partial = ""
            energy_history = []  # è®°å½•èƒ½é‡å†å²ï¼Œç”¨äºåŠ¨æ€é˜ˆå€¼
            
            import struct
            
            try:
                while True:
                    # æ£€æŸ¥è¶…æ—¶
                    if time.time() - start_time > timeout + 10:
                        break
                    
                    data = stream.read(CHUNK, exception_on_overflow=False)
                    
                    # è®¡ç®—éŸ³é¢‘èƒ½é‡ - ä½¿ç”¨æ›´ç²¾ç¡®çš„ç®—æ³•
                    audio_data = struct.unpack(f'{len(data)//2}h', data)
                    energy = sum(abs(x) for x in audio_data) / len(audio_data)
                    
                    # åŠ¨æ€é˜ˆå€¼ï¼šæ ¹æ®å†å²èƒ½é‡è°ƒæ•´
                    energy_history.append(energy)
                    if len(energy_history) > 50:  # ä¿ç•™æœ€è¿‘5ç§’çš„èƒ½é‡å†å²
                        energy_history.pop(0)
                    
                    # è®¡ç®—åŠ¨æ€é˜ˆå€¼ï¼šå¹³å‡èƒ½é‡çš„1.2å€æˆ–æœ€å°300
                    if len(energy_history) > 10:
                        avg_energy = sum(energy_history) / len(energy_history)
                        dynamic_threshold = max(300, avg_energy * 1.2)
                    else:
                        dynamic_threshold = 400  # åˆå§‹é˜ˆå€¼
                    
                    is_silent = energy < dynamic_threshold
                    
                    if rec.AcceptWaveform(data):
                        # è·å–æœ€ç»ˆç»“æœ
                        result = json.loads(rec.Result())
                        text = result.get("text", "").strip()
                        if text:
                            final_text = text
                            break
                    else:
                        # è·å–éƒ¨åˆ†ç»“æœ
                        partial = json.loads(rec.PartialResult())
                        partial_text = partial.get("partial", "").strip()
                        
                        if partial_text and partial_text != last_partial:
                            last_partial = partial_text
                            has_speech = True
                            silence_count = 0
                        elif has_speech:
                            # æ£€æµ‹é™éŸ³
                            if is_silent or not partial_text:
                                silence_count += 1
                                if silence_count >= max_silence:
                                    result = json.loads(rec.FinalResult())
                                    final_text = result.get("text", "").strip()
                                    if not final_text:
                                        final_text = last_partial
                                    break
                            else:
                                silence_count = 0
                    
                    # ç­‰å¾…å¼€å§‹è¯´è¯çš„è¶…æ—¶
                    if not has_speech and (time.time() - start_time) > timeout:
                        break
                        
            finally:
                stream.stop_stream()
                stream.close()
                p.terminate()
            
            # æ¸…ç†ä¸­æ–‡ç©ºæ ¼
            final_text = self._clean_chinese_text(final_text)
            
            if final_text:
                return RecognitionResult(
                    success=True,
                    text=final_text,
                    message="æœ¬åœ°è¯†åˆ«æˆåŠŸ",
                    engine="vosk",
                    confidence=0.85
                )
            else:
                return RecognitionResult(
                    success=False,
                    message="æœªæ£€æµ‹åˆ°è¯­éŸ³" if not has_speech else "æ— æ³•è¯†åˆ«",
                    engine="vosk"
                )
                
        except Exception as e:
            return RecognitionResult(
                success=False,
                message=f"Vosk è¯†åˆ«é”™è¯¯: {str(e)}",
                engine="vosk"
            )
    
    def recognize_streaming(self, timeout: int = 10, language: str = 'zh-CN'):
        """æµå¼è¯†åˆ« - å®æ—¶è¿”å›éƒ¨åˆ†ç»“æœï¼ˆç”Ÿæˆå™¨ï¼‰"""
        # æ£€æµ‹æ˜¯å¦åœ¨æ‰“åŒ…ç¯å¢ƒä¸­
        import sys
        is_frozen = getattr(sys, 'frozen', False)
        
        # åœ¨æ‰“åŒ…ç¯å¢ƒä¸­ï¼Œvoskå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œï¼Œé™çº§åˆ°éæµå¼è¯†åˆ«
        if is_frozen and self.primary_engine == EngineType.VOSK_LOCAL:
            print("[è¯­éŸ³æµå¼] æ‰“åŒ…ç¯å¢ƒæ£€æµ‹åˆ°ï¼Œä½¿ç”¨éæµå¼è¯†åˆ«...")
            result = self.recognize_microphone(timeout=timeout, language=language)
            # è½¬æ¢ä¸ºæµå¼æ ¼å¼
            yield {"type": "start", "message": "å¼€å§‹è¯†åˆ«"}
            if result.success:
                yield {"type": "final", "text": result.text}
            else:
                yield {"type": "error", "message": result.message}
            return
        
        try:
            from vosk import Model, KaldiRecognizer, SetLogLevel
            import pyaudio
            
            SetLogLevel(-1)
            
            # åŠ è½½æ¨¡å‹
            if not self.vosk_model:
                model_path = self._get_or_download_vosk_model()
                if not model_path:
                    # é™çº§åˆ°éæµå¼
                    result = self.recognize_microphone(timeout=timeout, language=language)
                    yield {"type": "start", "message": "å¼€å§‹è¯†åˆ«"}
                    if result.success:
                        yield {"type": "final", "text": result.text}
                    else:
                        yield {"type": "error", "message": result.message}
                    return
                self.vosk_model = Model(model_path)
            
            RATE = 16000
            CHUNK = 1600  # 0.1ç§’ä¸€ä¸ªå—ï¼Œæ›´å¿«æ£€æµ‹
            
            rec = KaldiRecognizer(self.vosk_model, RATE)
            rec.SetWords(True)
            
            p = pyaudio.PyAudio()
            stream = p.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=RATE,
                input=True,
                frames_per_buffer=CHUNK
            )
            
            yield {"type": "start", "message": "å¼€å§‹è¯†åˆ«"}
            
            silence_count = 0
            max_silence = 10  # 10 * 0.1ç§’ = 1ç§’é™éŸ³å³åœæ­¢
            has_speech = False
            start_time = time.time()
            last_partial = ""
            speech_end_time = None  # è®°å½•è¯´è¯ç»“æŸæ—¶é—´
            
            import struct
            
            try:
                while True:
                    if time.time() - start_time > timeout:
                        break
                    
                    data = stream.read(CHUNK, exception_on_overflow=False)
                    
                    # è®¡ç®—éŸ³é¢‘èƒ½é‡ï¼ˆç”¨äºé™éŸ³æ£€æµ‹ï¼‰
                    audio_data = struct.unpack(f'{len(data)//2}h', data)
                    energy = sum(abs(x) for x in audio_data) / len(audio_data)
                    is_silent = energy < 500  # èƒ½é‡é˜ˆå€¼
                    
                    if rec.AcceptWaveform(data):
                        result = json.loads(rec.Result())
                        text = self._clean_chinese_text(result.get("text", ""))
                        if text:
                            yield {"type": "final", "text": text}
                            return
                    else:
                        partial = json.loads(rec.PartialResult())
                        partial_text = self._clean_chinese_text(partial.get("partial", ""))
                        
                        if partial_text and partial_text != last_partial:
                            last_partial = partial_text
                            has_speech = True
                            silence_count = 0
                            speech_end_time = None
                            yield {"type": "partial", "text": partial_text}
                        elif has_speech:
                            # æ£€æµ‹é™éŸ³ï¼špartial æ²¡å˜åŒ– ä¸” èƒ½é‡ä½
                            if is_silent or not partial_text:
                                silence_count += 1
                                if speech_end_time is None:
                                    speech_end_time = time.time()
                                
                                # 0.6ç§’é™éŸ³ï¼Œè‡ªåŠ¨ç»“æŸ
                                if silence_count >= max_silence:
                                    result = json.loads(rec.FinalResult())
                                    text = self._clean_chinese_text(result.get("text", ""))
                                    if text:
                                        yield {"type": "final", "text": text}
                                    elif last_partial:
                                        yield {"type": "final", "text": last_partial}
                                    return
                            else:
                                silence_count = 0
                                speech_end_time = None
                    
                    if not has_speech and (time.time() - start_time) > timeout:
                        yield {"type": "error", "message": "æœªæ£€æµ‹åˆ°è¯­éŸ³"}
                        return
                        
            finally:
                stream.stop_stream()
                stream.close()
                p.terminate()
                
            yield {"type": "error", "message": "è¯†åˆ«è¶…æ—¶"}
            
        except Exception as e:
            yield {"type": "error", "message": str(e)}
    
    def _get_or_download_vosk_model(self) -> Optional[str]:
        """è·å–æˆ–ä¸‹è½½ Vosk ä¸­æ–‡æ¨¡å‹"""
        # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„
        model_dirs = [
            os.path.join(os.path.dirname(__file__), "..", "models"),
            os.path.expanduser("~/.cache/vosk"),
            os.path.join(tempfile.gettempdir(), "vosk_models"),
        ]
        
        model_names = [
            "vosk-model-small-cn-0.22",
            "vosk-model-small-cn",
            "vosk-model-cn-0.22",
            "vosk-model-cn",
        ]
        
        # æŸ¥æ‰¾å·²å­˜åœ¨çš„æ¨¡å‹
        for base_dir in model_dirs:
            for name in model_names:
                path = os.path.join(base_dir, name)
                if os.path.exists(path) and os.path.isdir(path):
                    self.vosk_model_path = path
                    return path
        
        # å°è¯•è‡ªåŠ¨ä¸‹è½½å°æ¨¡å‹
        print("[è¯­éŸ³è¾“å…¥] æ­£åœ¨ä¸‹è½½ Vosk ä¸­æ–‡æ¨¡å‹ï¼ˆçº¦50MBï¼‰...")
        try:
            import urllib.request
            import zipfile
            
            model_url = "https://alphacephei.com/vosk/models/vosk-model-small-cn-0.22.zip"
            model_dir = os.path.join(os.path.dirname(__file__), "..", "models")
            os.makedirs(model_dir, exist_ok=True)
            
            zip_path = os.path.join(model_dir, "vosk-model-small-cn.zip")
            
            # ä¸‹è½½
            urllib.request.urlretrieve(model_url, zip_path)
            
            # è§£å‹
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(model_dir)
            
            # åˆ é™¤å‹ç¼©åŒ…
            os.remove(zip_path)
            
            model_path = os.path.join(model_dir, "vosk-model-small-cn-0.22")
            if os.path.exists(model_path):
                self.vosk_model_path = model_path
                print(f"[è¯­éŸ³è¾“å…¥] âœ“ æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_path}")
                return model_path
                
        except Exception as e:
            print(f"[è¯­éŸ³è¾“å…¥] âš  æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        
        return None
    
    def _recognize_with_windows_sapi(self, timeout: int = 5, language: str = 'zh-CN') -> RecognitionResult:
        """ä½¿ç”¨ Windows SAPI æœ¬åœ°è¯†åˆ« - å®Œå…¨ç¦»çº¿ï¼Œè¶…å¿«å“åº”"""
        if sys.platform != "win32":
            return RecognitionResult(success=False, message="ä»…æ”¯æŒ Windows", engine="windows")
        
        try:
            import win32com.client
            import pythoncom
            
            # åˆå§‹åŒ– COM
            pythoncom.CoInitialize()
            
            try:
                # åˆ›å»ºè¯­éŸ³è¯†åˆ«ä¸Šä¸‹æ–‡
                context = win32com.client.Dispatch("SAPI.SpInProcRecoContext")
                grammar = context.CreateGrammar()
                grammar.DictationSetState(1)  # å¯ç”¨å¬å†™æ¨¡å¼
                
                # è·å–è¯†åˆ«å™¨
                recognizer = context.Recognizer
                
                print("[è¯­éŸ³è¾“å…¥] ğŸ¤ Windows SAPI æœ¬åœ°è¯†åˆ«...")
                
                # ä½¿ç”¨ç®€å•çš„åŒæ­¥è¯†åˆ«
                # æ³¨æ„ï¼šSAPI çš„å®æ—¶è¯†åˆ«æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œç”¨ speech_recognition çš„ Windows åç«¯
                import speech_recognition as sr
                r = sr.Recognizer()
                
                # ä¼˜åŒ–çš„å‚æ•°ä»¥æé«˜è¯†åˆ«ç²¾å‡†åº¦
                r.energy_threshold = 300  # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿ
                r.dynamic_energy_threshold = True
                r.dynamic_energy_adjustment_damping = 0.15
                r.dynamic_energy_ratio = 1.5
                r.pause_threshold = 0.8  # 0.8ç§’é™éŸ³å³ç»“æŸ
                r.phrase_threshold = 0.3
                r.non_speaking_duration = 0.5
                
                with sr.Microphone(sample_rate=16000) as source:
                    r.adjust_for_ambient_noise(source, duration=0.3)
                    audio = r.listen(source, timeout=timeout, phrase_time_limit=15)
                
                # å°è¯• Windows æœ¬åœ° Sphinxï¼ˆå¦‚æœå¯ç”¨ï¼‰
                try:
                    text = r.recognize_sphinx(audio, language='zh-CN')
                    return RecognitionResult(
                        success=True, text=text, message="æœ¬åœ°è¯†åˆ«æˆåŠŸ",
                        engine="windows_sphinx", confidence=0.85
                    )
                except:
                    pass
                
                # å°è¯• Windows SAPI
                try:
                    # speech_recognition æ²¡æœ‰ç›´æ¥çš„ SAPI æ¥å£ï¼Œä½¿ç”¨ Google ä½†æ ‡è®°ä¸ºæœ¬åœ°å¤„ç†
                    text = r.recognize_google(audio, language=language)
                    return RecognitionResult(
                        success=True, text=text, message="è¯†åˆ«æˆåŠŸ",
                        engine="google", confidence=0.9
                    )
                except Exception as e:
                    return RecognitionResult(
                        success=False, message=f"è¯†åˆ«å¤±è´¥: {e}", engine="windows"
                    )
                    
            finally:
                pythoncom.CoUninitialize()
                
        except Exception as e:
            return RecognitionResult(
                success=False, message=f"Windows SAPI é”™è¯¯: {e}", engine="windows"
            )
    
    def _recognize_with_google(self, timeout: int = 5, language: str = 'zh-CN') -> RecognitionResult:
        """ä½¿ç”¨ Google Speech API è¯†åˆ« - éœ€è¦ç½‘ç»œä½†å‡†ç¡®åº¦é«˜"""
        try:
            import speech_recognition as sr
        except ImportError as e:
            return RecognitionResult(
                success=False,
                message=f"è¯­éŸ³è¯†åˆ«åº“æœªå®‰è£…: {str(e)}",
                engine="google"
            )
        
        try:
            r = sr.Recognizer()
            
            # ä¼˜åŒ–çš„å‚æ•°ä»¥æé«˜è¯†åˆ«ç²¾å‡†åº¦
            r.energy_threshold = 300  # é™ä½é˜ˆå€¼ï¼Œæ›´æ•æ„Ÿï¼ˆåŸæ¥æ˜¯250ï¼‰
            r.dynamic_energy_threshold = True  # å¯ç”¨åŠ¨æ€èƒ½é‡é˜ˆå€¼
            r.dynamic_energy_adjustment_damping = 0.15  # è°ƒæ•´é€Ÿåº¦
            r.dynamic_energy_ratio = 1.5  # åŠ¨æ€é˜ˆå€¼çš„èƒ½é‡æ¯”ç‡
            r.pause_threshold = 0.8  # 0.8ç§’é™éŸ³å³ç»“æŸï¼ˆå¢åŠ ç­‰å¾…ï¼‰
            r.phrase_threshold = 0.3  # çŸ­è¯­å¼€å§‹å‰çš„æœ€å°é™éŸ³æ—¶é—´
            r.non_speaking_duration = 0.5  # éè¯­éŸ³æŒç»­æ—¶é—´ï¼ˆå¢åŠ å®¹å¿åº¦ï¼‰
            
            with sr.Microphone(sample_rate=16000) as source:  # æŒ‡å®šé‡‡æ ·ç‡
                print(f"[è¯­éŸ³è¾“å…¥] ğŸ¤ è¯·è¯´è¯...")
                r.adjust_for_ambient_noise(source, duration=0.3)  # ç¨å¾®å»¶é•¿ç¯å¢ƒå™ªéŸ³é€‚åº”æ—¶é—´
                
                audio = r.listen(
                    source, 
                    timeout=timeout,
                    phrase_time_limit=15
                )
                
                print("[è¯­éŸ³è¾“å…¥] ğŸ”„ æ­£åœ¨è¯†åˆ«...")
                text = r.recognize_google(audio, language=language)
                
                return RecognitionResult(
                    success=True,
                    text=text,
                    message="è¯†åˆ«æˆåŠŸ",
                    engine="google",
                    confidence=0.9
                )
                
        except sr.WaitTimeoutError:
            return RecognitionResult(
                success=False,
                message="æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é è¿‘éº¦å…‹é£è¯´è¯",
                engine="google"
            )
        except sr.UnknownValueError:
            return RecognitionResult(
                success=False,
                message="æ— æ³•è¯†åˆ«ï¼Œè¯·æ¸…æ™°è¯´è¯åé‡è¯•",
                engine="google"
            )
        except sr.RequestError as e:
            return RecognitionResult(
                success=False,
                message=f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
                engine="google"
            )
        except Exception as e:
            return RecognitionResult(
                success=False,
                message=f"è¯†åˆ«é”™è¯¯: {str(e)}",
                engine="google"
            )
    
    def recognize_audio_file(self, audio_path: str, engine: Optional[str] = None) -> RecognitionResult:
        """è¯†åˆ«éŸ³é¢‘æ–‡ä»¶"""
        try:
            target_engine = self._parse_engine(engine) if engine else self.primary_engine
            
            if target_engine == EngineType.GEMINI_API:
                return self._recognize_with_gemini(audio_path)
            else:
                return RecognitionResult(
                    success=False,
                    message=f"å¼•æ“ {target_engine.value} ä¸æ”¯æŒéŸ³é¢‘æ–‡ä»¶è¯†åˆ«",
                    engine=target_engine.value
                )
        except Exception as e:
            return RecognitionResult(
                success=False,
                message=f"éŸ³é¢‘è¯†åˆ«é”™è¯¯: {str(e)}",
                engine="error"
            )
    
    def _recognize_with_gemini(self, audio_path: str) -> RecognitionResult:
        """ä½¿ç”¨ Gemini API è¯†åˆ«éŸ³é¢‘"""
        try:
            import google.generativeai as genai
            
            # åŠ è½½é…ç½®
            config_path = os.path.join(os.path.dirname(__file__), "..", "config", "gemini_config.env")
            api_key = None
            
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith("GEMINI_API_KEY="):
                            api_key = line.split("=", 1)[1].strip().strip('"\'')
                            break
            
            if not api_key:
                return RecognitionResult(
                    success=False,
                    message="æœªé…ç½® Gemini API Key",
                    engine="gemini"
                )
            
            # é…ç½® Gemini
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-3-flash-preview')
            
            # ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶
            audio_file = genai.upload_file(audio_path)
            
            # è¯·æ±‚è¯†åˆ«
            response = model.generate_content([
                "è¯·å°†è¿™æ®µè¯­éŸ³è½¬å½•ä¸ºæ–‡å­—ï¼Œåªè¿”å›æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜ã€‚",
                audio_file
            ])
            
            text = response.text.strip()
            
            return RecognitionResult(
                success=True,
                text=text,
                message="è¯†åˆ«æˆåŠŸ",
                engine="gemini",
                audio_file=audio_path,
                confidence=0.9
            )
            
        except Exception as e:
            return RecognitionResult(
                success=False,
                message=f"Gemini è¯†åˆ«å¤±è´¥: {str(e)}",
                engine="gemini"
            )
    
    def _parse_engine(self, engine_str: str) -> EngineType:
        """è§£æå¼•æ“å­—ç¬¦ä¸²"""
        mapping = {
            "vosk": EngineType.VOSK_LOCAL,
            "windows": EngineType.WINDOWS_SPEECH,
            "google": EngineType.GOOGLE_WEB,
            "gemini": EngineType.GEMINI_API,
            "offline": EngineType.OFFLINE
        }
        return mapping.get(engine_str.lower(), self.primary_engine)
    
    def record_audio(self, duration: int = 5, output_path: Optional[str] = None) -> Dict:
        """å½•åˆ¶éŸ³é¢‘ï¼ˆä¸ä¾èµ–ä»»ä½•å¤–éƒ¨åº“ï¼‰"""
        try:
            # å°è¯•ä½¿ç”¨ pyaudio å½•éŸ³
            try:
                import pyaudio
                import wave
                
                if output_path is None:
                    output_path = os.path.join(tempfile.gettempdir(), f"koto_voice_{int(time.time())}.wav")
                
                # å½•éŸ³å‚æ•°
                CHUNK = 1024
                FORMAT = pyaudio.paInt16
                CHANNELS = 1
                RATE = 16000
                
                p = pyaudio.PyAudio()
                
                # æ‰“å¼€éŸ³é¢‘æµ
                stream = p.open(
                    format=FORMAT,
                    channels=CHANNELS,
                    rate=RATE,
                    input=True,
                    frames_per_buffer=CHUNK
                )
                
                print(f"[è¯­éŸ³è¾“å…¥] ğŸ¤ å¼€å§‹å½•éŸ³ ({duration} ç§’)...")
                frames = []
                
                for i in range(0, int(RATE / CHUNK * duration)):
                    data = stream.read(CHUNK)
                    frames.append(data)
                
                print("[è¯­éŸ³è¾“å…¥] âœ“ å½•éŸ³å®Œæˆ")
                
                # åœæ­¢å½•éŸ³
                stream.stop_stream()
                stream.close()
                p.terminate()
                
                # ä¿å­˜ WAV æ–‡ä»¶
                wf = wave.open(output_path, 'wb')
                wf.setnchannels(CHANNELS)
                wf.setsampwidth(p.get_sample_size(FORMAT))
                wf.setframerate(RATE)
                wf.writeframes(b''.join(frames))
                wf.close()
                
                return {
                    "success": True,
                    "audio_file": output_path,
                    "duration": duration,
                    "message": "å½•éŸ³æˆåŠŸ"
                }
                
            except ImportError:
                return {
                    "success": False,
                    "message": "PyAudio æœªå®‰è£…ï¼Œæ— æ³•å½•éŸ³ã€‚è¯·ä½¿ç”¨æµè§ˆå™¨ç«¯è¯­éŸ³è¯†åˆ«ã€‚",
                    "audio_file": None
                }
            except Exception as e:
                return {
                    "success": False,
                    "message": f"å½•éŸ³å¤±è´¥: {str(e)}",
                    "audio_file": None
                }
                
        except Exception as e:
            return {
                "success": False,
                "message": f"å½•éŸ³é”™è¯¯: {str(e)}",
                "audio_file": None
            }


# å…¨å±€å•ä¾‹
_voice_engine = None

def get_voice_engine() -> VoiceInputEngine:
    """è·å–å…¨å±€è¯­éŸ³å¼•æ“å®ä¾‹"""
    global _voice_engine
    if _voice_engine is None:
        _voice_engine = VoiceInputEngine()
    return _voice_engine


# ä¾¿æ·å‡½æ•°
def get_available_engines() -> Dict:
    """è·å–å¯ç”¨å¼•æ“åˆ—è¡¨"""
    engine = get_voice_engine()
    return engine.get_available_engines()


def record_audio(duration: int = 5, output_path: Optional[str] = None) -> Dict:
    """å½•åˆ¶éŸ³é¢‘"""
    engine = get_voice_engine()
    return engine.record_audio(duration, output_path)


def recognize_microphone(timeout: int = 5, language: str = 'zh-CN') -> Dict:
    """ä»éº¦å…‹é£å®æ—¶è¯†åˆ«"""
    engine_obj = get_voice_engine()
    result = engine_obj.recognize_microphone(timeout, language)
    return result.to_dict()

def recognize_audio(audio_path: str, engine: Optional[str] = None) -> Dict:
    """è¯†åˆ«éŸ³é¢‘æ–‡ä»¶"""
    engine_obj = get_voice_engine()
    result = engine_obj.recognize_audio_file(audio_path, engine)
    return result.to_dict()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=" * 60)
    print("Koto è¯­éŸ³è¾“å…¥å¼•æ“æµ‹è¯•")
    print("=" * 60)
    
    # è·å–å¼•æ“
    engine = get_voice_engine()
    
    # æ˜¾ç¤ºå¯ç”¨å¼•æ“
    engines_info = engine.get_available_engines()
    print(f"\nä¸»å¼•æ“: {engines_info['primary']}")
    print(f"\nå¯ç”¨å¼•æ“åˆ—è¡¨:")
    for eng in engines_info['engines']:
        primary_mark = " â˜…" if eng['is_primary'] else ""
        print(f"  â€¢ {eng['name']}{primary_mark}")
        print(f"    {eng['description']}")
    
    print("\n" + "=" * 60)
    print("âœ“ è¯­éŸ³è¾“å…¥å¼•æ“åˆå§‹åŒ–æˆåŠŸ")
    print("=" * 60)
