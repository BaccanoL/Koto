"""
ğŸ“¦ å½’æ¡£æ–‡ä»¶å…¨æ–‡æœç´¢å¼•æ“ (Archive Full-Text Search Engine)

åŠŸèƒ½:
- å¿«é€Ÿç´¢å¼•ç”Ÿæˆ (PDF, Word, Excel, çº¯æ–‡æœ¬, Markdown)
- SQLiteå…¨æ–‡æœç´¢ (BM25ç®—æ³•)
- è¯­ä¹‰æœç´¢ (å‘é‡ç›¸ä¼¼åº¦)
- å®æ—¶ç´¢å¼•æ›´æ–°
- æœç´¢å†å²è¿½è¸ª

ä½¿ç”¨åœºæ™¯:
  ç”¨æˆ·: "æŸ¥ä¸€ä¸‹2æœˆä»½çš„æ‰€æœ‰æ¶‰åŠ'é»„é‡‘ä»·æ ¼'çš„å½’æ¡£æ–‡ä»¶"
  â†’ 1. å…³é”®è¯æœç´¢ â†’ 2. æ—¥æœŸè¿‡æ»¤ â†’ 3. è¿”å›ç»“æœ + æ‘˜è¦
"""

import os
import sqlite3
import json
import time
import hashlib
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import unicodedata

try:
    import PyPDF2
    HAS_PYPDF = True
except ImportError:
    HAS_PYPDF = False

try:
    from docx import Document as DocxDocument
    HAS_DOCX = True
except ImportError:
    HAS_DOCX = False

try:
    import openpyxl
    HAS_OPENPYXL = True
except ImportError:
    HAS_OPENPYXL = False

try:
    from PIL import Image
    import pytesseract
    HAS_OCR = True
except ImportError:
    HAS_OCR = False


@dataclass
class IndexedFile:
    """ç´¢å¼•æ–‡ä»¶è®°å½•"""
    id: str
    path: str
    name: str
    file_type: str
    size: int
    created_at: str
    modified_at: str
    indexed_at: str
    owner_id: str = "system"
    organization_id: str = "default"
    summary: str = ""
    keywords: List[str] = None
    content_hash: str = ""
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = []


@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    file_id: str
    path: str
    name: str
    file_type: str
    summary: str
    relevance_score: float
    snippet: str  # åŒ…å«å…³é”®è¯çš„æ–‡æœ¬ç‰‡æ®µ
    matched_at: int  # æ–‡ä»¶ä¸­çš„å­—ç¬¦ä½ç½®


class ArchiveSearchEngine:
    """å½’æ¡£æ–‡ä»¶å…¨æ–‡æœç´¢å¼•æ“"""
    
    def __init__(self, archive_root: str = "workspace/_archive", db_path: str = ".koto_search.db"):
        self.archive_root = Path(archive_root)
        self.db_path = db_path
        self.executor = ThreadPoolExecutor(max_workers=4)
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ–‡ä»¶ç´¢å¼•è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS file_index (
                id TEXT PRIMARY KEY,
                path TEXT UNIQUE NOT NULL,
                name TEXT NOT NULL,
                file_type TEXT,
                size INTEGER,
                created_at TIMESTAMP,
                modified_at TIMESTAMP,
                indexed_at TIMESTAMP,
                owner_id TEXT DEFAULT 'system',
                organization_id TEXT DEFAULT 'default',
                content_hash TEXT
            )
        """)
        
        # å†…å®¹æ‘˜è¦è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS content_summary (
                id TEXT PRIMARY KEY,
                file_id TEXT NOT NULL UNIQUE,
                summary TEXT,
                keywords TEXT,
                entities TEXT,
                language TEXT DEFAULT 'zh',
                FOREIGN KEY(file_id) REFERENCES file_index(id)
            )
        """)
        
        # å…¨æ–‡ç´¢å¼•è¡¨ (FTS5è™šè¡¨)
        cursor.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS full_text_index USING fts5(
                file_id,
                name,
                content,
                tokenize = 'porter unicode61'
            )
        """)
        
        # æœç´¢å†å²è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS search_history (
                id TEXT PRIMARY KEY,
                user_id TEXT,
                organization_id TEXT,
                query TEXT,
                result_count INTEGER,
                execution_time_ms INTEGER,
                created_at TIMESTAMP
            )
        """)
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_file_type 
            ON file_index(file_type)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_created_at 
            ON file_index(created_at)
        """)
        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_modified_at 
            ON file_index(modified_at)
        """)
        
        conn.commit()
        conn.close()
    
    def index_archive(self, full_rebuild: bool = False) -> Dict:
        """
        ç´¢å¼•æ•´ä¸ªå½’æ¡£ç›®å½•
        
        Args:
            full_rebuild: æ˜¯å¦å®Œå…¨é‡å»ºç´¢å¼•
            
        Returns:
            {
                "indexed_count": 150,
                "failed_count": 3,
                "duration_seconds": 45,
                "errors": [...]
            }
        """
        start_time = time.time()
        indexed_count = 0
        failed_count = 0
        errors = []
        
        if full_rebuild:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("DELETE FROM file_index")
            cursor.execute("DELETE FROM content_summary")
            cursor.execute("DELETE FROM full_text_index")
            conn.commit()
            conn.close()
        
        # å¹¶è¡Œå¤„ç†æ–‡ä»¶
        files_to_index = list(self.archive_root.rglob("*"))
        files_to_index = [f for f in files_to_index if f.is_file()]
        
        futures = {}
        for file_path in files_to_index:
            future = self.executor.submit(self._index_file, file_path)
            futures[future] = file_path
        
        for future in as_completed(futures):
            try:
                if future.result():
                    indexed_count += 1
            except Exception as e:
                failed_count += 1
                errors.append({
                    "file": str(futures[future]),
                    "error": str(e)
                })
        
        duration = time.time() - start_time
        
        return {
            "indexed_count": indexed_count,
            "failed_count": failed_count,
            "total_files": len(files_to_index),
            "duration_seconds": round(duration, 2),
            "errors": errors
        }
    
    def _index_file(self, file_path: Path) -> bool:
        """ç´¢å¼•å•ä¸ªæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç´¢å¼•ä¸”æœªä¿®æ”¹
            file_id = self._generate_file_id(file_path)
            content, _ = self._extract_content(file_path)
            
            if not content:
                return False
            
            # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            # æå–æ‘˜è¦å’Œå…³é”®è¯
            summary = self._generate_summary(content)
            keywords = self._extract_keywords(content)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            indexed_file = IndexedFile(
                id=file_id,
                path=str(file_path),
                name=file_path.name,
                file_type=file_path.suffix[1:],
                size=file_path.stat().st_size,
                created_at=datetime.fromtimestamp(file_path.stat().st_ctime).isoformat(),
                modified_at=datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                indexed_at=datetime.now().isoformat(),
                content_hash=content_hash,
                summary=summary,
                keywords=keywords
            )
            
            self._save_to_database(indexed_file, content)
            return True
            
        except Exception as e:
            print(f"Failed to index {file_path}: {e}")
            return False
    
    def _extract_content(self, file_path: Path) -> Tuple[str, str]:
        """
        æå–æ–‡ä»¶å†…å®¹
        
        Returns:
            (content, language)
        """
        try:
            suffix = file_path.suffix.lower()
            
            # çº¯æ–‡æœ¬æ–‡ä»¶
            if suffix in ['.txt', '.md', '.markdown', '.log']:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read(), 'zh'
            
            # PDFæ–‡ä»¶
            elif suffix == '.pdf' and HAS_PYPDF:
                text = []
                with open(file_path, 'rb') as f:
                    reader = PyPDF2.PdfReader(f)
                    for page in reader.pages:
                        text.append(page.extract_text())
                return '\n'.join(text), 'zh'
            
            # Wordæ–‡æ¡£
            elif suffix in ['.docx', '.doc'] and HAS_DOCX:
                if suffix == '.doc':
                    # éœ€è¦è½¬æ¢
                    return "", 'zh'
                doc = DocxDocument(file_path)
                text = '\n'.join([para.text for para in doc.paragraphs])
                return text, 'zh'
            
            # Excelæ–‡ä»¶
            elif suffix in ['.xlsx', '.xls'] and HAS_OPENPYXL:
                text = []
                try:
                    wb = openpyxl.load_workbook(file_path)
                    for sheet in wb.sheetnames:
                        ws = wb[sheet]
                        text.append(f"Sheet: {sheet}")
                        for row in ws.iter_rows(values_only=True):
                            text.append(' '.join(str(v) for v in row if v))
                    return '\n'.join(text), 'zh'
                except:
                    return "", 'zh'
            
            # å›¾ç‰‡OCR
            elif suffix in ['.jpg', '.jpeg', '.png', '.gif'] and HAS_OCR:
                try:
                    img = Image.open(file_path)
                    text = pytesseract.image_to_string(img, lang='chi_sim+eng')
                    return text, 'mixed'
                except:
                    return "", 'mixed'
            
            return "", 'unknown'
            
        except Exception as e:
            print(f"Error extracting content from {file_path}: {e}")
            return "", 'unknown'
    
    def _generate_summary(self, content: str, max_length: int = 200) -> str:
        """ç”Ÿæˆå†…å®¹æ‘˜è¦ (ç®€å•ä»å‰200å­—)"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = ' '.join(content.split())
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r'[^\w\s\u4e00-\u9fff\u3040-\u309f\u30a0-\u30ff]', ' ', content)
        return content[:max_length]
    
    def _extract_keywords(self, content: str, top_k: int = 5) -> List[str]:
        """æå–å…³é”®è¯ (ç®€å•å®ç°: ä¸­æ–‡è¯é¢‘)"""
        # ç®€å•å…³é”®è¯æå–: æ‰¾å‡º2-4ä¸ªè¿ç»­æ±‰å­—
        keywords = re.findall(r'[\u4e00-\u9fff]{2,4}', content)
        # è®¡ç®—è¯é¢‘
        from collections import Counter
        freq = Counter(keywords)
        return [word for word, _ in freq.most_common(top_k)]
    
    def _save_to_database(self, indexed_file: IndexedFile, content: str):
        """ä¿å­˜åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            # åˆ é™¤æ—§è®°å½•
            cursor.execute("DELETE FROM file_index WHERE path = ?", (indexed_file.path,))
            cursor.execute("DELETE FROM content_summary WHERE file_id = ?", (indexed_file.id,))
            cursor.execute("DELETE FROM full_text_index WHERE file_id = ?", (indexed_file.id,))
            
            # æ’å…¥æ–‡ä»¶ç´¢å¼•
            cursor.execute("""
                INSERT INTO file_index (
                    id, path, name, file_type, size, 
                    created_at, modified_at, indexed_at, owner_id, organization_id, content_hash
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                indexed_file.id,
                indexed_file.path,
                indexed_file.name,
                indexed_file.file_type,
                indexed_file.size,
                indexed_file.created_at,
                indexed_file.modified_at,
                indexed_file.indexed_at,
                indexed_file.owner_id,
                indexed_file.organization_id,
                indexed_file.content_hash
            ))
            
            # æ’å…¥æ‘˜è¦
            cursor.execute("""
                INSERT INTO content_summary (
                    id, file_id, summary, keywords, entities, language
                ) VALUES (?, ?, ?, ?, ?, ?)
            """, (
                f"summary_{indexed_file.id}",
                indexed_file.id,
                indexed_file.summary,
                json.dumps(indexed_file.keywords),
                json.dumps({}),
                'zh'
            ))
            
            # æ’å…¥å…¨æ–‡ç´¢å¼•
            cursor.execute("""
                INSERT INTO full_text_index (file_id, name, content)
                VALUES (?, ?, ?)
            """, (indexed_file.id, indexed_file.name, content))
            
            conn.commit()
        finally:
            conn.close()
    
    def search(
        self,
        query: str,
        search_type: str = "hybrid",
        file_type: Optional[str] = None,
        date_range: Optional[Tuple[str, str]] = None,
        limit: int = 20,
        offset: int = 0,
        user_id: str = "system"
    ) -> Dict:
        """
        å…¨æ–‡æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢è¯
            search_type: "keyword" | "semantic" | "hybrid" (é»˜è®¤)
            file_type: è¿‡æ»¤æ–‡ä»¶ç±»å‹ (pdf, docx, xlsxç­‰)
            date_range: æ—¥æœŸèŒƒå›´ ("2026-01-01", "2026-02-14")
            limit: è¿”å›ç»“æœæ•°
            offset: åˆ†é¡µåç§»
            user_id: ç”¨æˆ·ID (ç”¨äºå®¡è®¡)
            
        Returns:
            {
                "results": [{...}],
                "total_count": 45,
                "execution_time_ms": 123,
                "query": "é»„é‡‘ä»·æ ¼"
            }
        """
        start_time = time.time()
        results = []
        
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        
        try:
            if search_type in ["keyword", "hybrid"]:
                results = self._keyword_search(conn, query, file_type, date_range, limit, offset)
            
            # TODO: æ·»åŠ è¯­ä¹‰æœç´¢ (éœ€è¦å‘é‡åŒ–)
            
            # è®°å½•æœç´¢å†å²
            self._record_search(user_id, query, len(results))
            
            execution_time = (time.time() - start_time) * 1000
            
            return {
                "results": results,
                "total_count": len(results),
                "execution_time_ms": round(execution_time, 2),
                "query": query
            }
        finally:
            conn.close()
    
    def _keyword_search(
        self,
        conn: sqlite3.Connection,
        query: str,
        file_type: Optional[str],
        date_range: Optional[Tuple[str, str]],
        limit: int,
        offset: int
    ) -> List[Dict]:
        """å…³é”®è¯æœç´¢ (BM25)"""
        cursor = conn.cursor()
        results = []
        
        # æ„å»ºFTS5æŸ¥è¯¢
        fts_query = f'"{query}"'  # ç²¾ç¡®åŒ¹é…
        
        sql = f"""
            SELECT DISTINCT
                f.id,
                f.path,
                f.name,
                f.file_type,
                cs.summary,
                cs.keywords,
                fti.content
            FROM full_text_index fti
            JOIN file_index f ON f.id = fti.file_id
            LEFT JOIN content_summary cs ON cs.file_id = f.id
            WHERE fti.full_text_index MATCH ?
        """
        
        params = [fts_query]
        
        # æ·»åŠ æ–‡ä»¶ç±»å‹è¿‡æ»¤
        if file_type:
            sql += " AND f.file_type = ?"
            params.append(file_type)
        
        # æ·»åŠ æ—¥æœŸèŒƒå›´è¿‡æ»¤
        if date_range:
            start_date, end_date = date_range
            sql += " AND f.created_at BETWEEN ? AND ?"
            params.extend([start_date, end_date])
        
        sql += " ORDER BY f.modified_at DESC LIMIT ? OFFSET ?"
        params.extend([limit, offset])
        
        try:
            cursor.execute(sql, params)
            rows = cursor.fetchall()
            
            for row in rows:
                # ç”Ÿæˆæ‘˜è¦å’Œç›¸å…³æ€§ç‰‡æ®µ
                snippet = self._generate_snippet(row['content'], query)
                relevance = self._calculate_relevance(row['content'], query)
                
                results.append({
                    "file_id": row['id'],
                    "path": row['path'],
                    "name": row['name'],
                    "file_type": row['file_type'],
                    "summary": row['summary'] or "",
                    "keywords": json.loads(row['keywords'] or '[]'),
                    "relevance_score": relevance,
                    "snippet": snippet
                })
        except Exception as e:
            print(f"Search error: {e}")
        
        return results
    
    def _generate_snippet(self, content: str, query: str, context_words: int = 20) -> str:
        """ç”ŸæˆåŒ…å«å…³é”®è¯çš„æ–‡æœ¬ç‰‡æ®µ"""
        # æ‰¾åˆ°æŸ¥è¯¢è¯åœ¨å†…å®¹ä¸­çš„ä½ç½®
        pos = content.lower().find(query.lower())
        if pos == -1:
            return content[:100]  # è¿”å›å‰100å­—
        
        # ä»å‰åå„extraction_wordså­—ç¬¦æå–
        start = max(0, pos - context_words)
        end = min(len(content), pos + len(query) + context_words)
        
        snippet = content[start:end]
        # é«˜äº®å…³é”®è¯
        snippet = snippet.replace(query, f"ã€{query}ã€‘")
        
        return snippet
    
    def _calculate_relevance(self, content: str, query: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•° (0-100)"""
        # ç®€å•å®ç°: å…³é”®è¯å‡ºç°æ¬¡æ•°
        count = content.lower().count(query.lower())
        # å½’ä¸€åŒ–
        score = min(100, count * 10)
        return score
    
    def _record_search(self, user_id: str, query: str, result_count: int):
        """è®°å½•æœç´¢å†å²"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("""
                INSERT INTO search_history (
                    id, user_id, organization_id, query, result_count, created_at
                ) VALUES (?, ?, ?, ?, ?, ?)
            """, (
                f"search_{int(time.time() * 1000)}",
                user_id,
                "default",
                query,
                result_count,
                datetime.now().isoformat()
            ))
            conn.commit()
        finally:
            conn.close()
    
    def get_search_suggestions(self, query_prefix: str, limit: int = 5) -> List[str]:
        """è·å–æœç´¢å»ºè®®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        suggestions = []
        try:
            # ä»æœç´¢å†å²ä¸­è·å–
            cursor.execute("""
                SELECT DISTINCT query FROM search_history
                WHERE query LIKE ?
                ORDER BY created_at DESC
                LIMIT ?
            """, (f"{query_prefix}%", limit))
            
            suggestions = [row[0] for row in cursor.fetchall()]
        finally:
            conn.close()
        
        return suggestions
    
    def get_index_status(self) -> Dict:
        """è·å–ç´¢å¼•çŠ¶æ€"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("SELECT COUNT(*) FROM file_index")
            indexed_files = cursor.fetchone()[0]
            
            cursor.execute("""
                SELECT MAX(indexed_at) FROM file_index
            """)
            last_update = cursor.fetchone()[0]
            
            return {
                "indexed_files": indexed_files,
                "last_update": last_update,
                "status": "healthy" if indexed_files > 0 else "empty"
            }
        finally:
            conn.close()
    
    @staticmethod
    def _generate_file_id(file_path: Path) -> str:
        """ç”Ÿæˆæ–‡ä»¶ID"""
        return hashlib.md5(str(file_path).encode()).hexdigest()[:16]


# å…¨å±€å®ä¾‹
_search_engine: Optional[ArchiveSearchEngine] = None


def get_search_engine(archive_root: str = "workspace/_archive") -> ArchiveSearchEngine:
    """è·å–å…¨å±€æœç´¢å¼•æ“å®ä¾‹"""
    global _search_engine
    if _search_engine is None:
        _search_engine = ArchiveSearchEngine(archive_root)
    return _search_engine
