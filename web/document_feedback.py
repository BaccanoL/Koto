#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£æ™ºèƒ½åé¦ˆç³»ç»Ÿ - å®Œæ•´é—­ç¯
1. è¯»å–æ–‡æ¡£ â†’ 2. AIåˆ†æ â†’ 3. åº”ç”¨ä¿®æ”¹ æˆ– è‡ªåŠ¨æ ‡æ³¨
"""

import os
import json
import time
from typing import Dict, Any, Optional, List, Callable
from datetime import datetime


class DocumentFeedbackSystem:
    """æ–‡æ¡£æ™ºèƒ½åé¦ˆç³»ç»Ÿ"""
    
    def __init__(self, gemini_client=None, default_model_id: str = "gemini-3-pro-preview"):
        """
        Args:
            gemini_client: Gemini APIå®¢æˆ·ç«¯å®ä¾‹
            default_model_id: é»˜è®¤ä¼˜å…ˆæ¨¡å‹
        """
        self.client = gemini_client
        self.default_model_id = default_model_id
        from web.document_reader import DocumentReader
        from web.document_editor import DocumentEditor
        from web.document_annotator import DocumentAnnotator
        
        self.reader = DocumentReader()
        self.editor = DocumentEditor()
        self.annotator = DocumentAnnotator(annotation_mode="comment")  # é»˜è®¤ä½¿ç”¨æ°”æ³¡æ‰¹æ³¨
    
        self._model_cache = None
    def analyze_and_suggest(
        self,
        file_path: str,
        user_requirement: str = "",
        model_id: str = "gemini-3-flash-preview"
    ) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£å¹¶ç»™å‡ºAIä¿®æ”¹å»ºè®®
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            user_requirement: ç”¨æˆ·éœ€æ±‚ï¼ˆä¾‹å¦‚ï¼š"è¯·ä¼˜åŒ–æ ‡é¢˜ï¼Œè®©å®ƒæ›´ä¸“ä¸š"ï¼‰
            model_id: ä½¿ç”¨çš„æ¨¡å‹ID
        
        Returns:
            {
                "success": True,
                "original_content": {...},
                "ai_suggestions": "AIåˆ†ææ–‡æœ¬",
                "modifications": [...],
                "summary": "ä¿®æ”¹å»ºè®®æ‘˜è¦"
            }
        """
        # ç¬¬1æ­¥ï¼šè¯»å–æ–‡æ¡£
        print(f"[DocumentFeedback] ğŸ“– è¯»å–æ–‡æ¡£: {os.path.basename(file_path)}")
        doc_data = self.reader.read_document(file_path)
        
        if not doc_data.get("success"):
            return {
                "success": False,
                "error": f"è¯»å–æ–‡æ¡£å¤±è´¥: {doc_data.get('error')}"
            }
        
        # ç¬¬2æ­¥ï¼šæ ¼å¼åŒ–ç»™AI
        formatted_content = self.reader.format_for_ai(doc_data)
        
        # ç¬¬3æ­¥ï¼šæ„å»ºAIæç¤º
        prompt = self._build_analysis_prompt(
            doc_data.get("type"),
            formatted_content,
            user_requirement
        )
        
        # ç¬¬4æ­¥ï¼šè°ƒç”¨AIåˆ†æ
        if not self.client:
            return {
                "success": False,
                "error": "Geminiå®¢æˆ·ç«¯æœªåˆå§‹åŒ–"
            }
        
        selected_model, _ = self._select_best_model(model_id)
        print(f"[DocumentFeedback] ğŸ¤– AIåˆ†æä¸­...")
        
        try:
            from google.genai import types
            
            response = self.client.models.generate_content(
                model=selected_model,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    max_output_tokens=32000,
                )
            )
            
            ai_response = response.text if response else ""
            if not ai_response and getattr(response, "candidates", None):
                # å°è¯•ä»å€™é€‰ä¸­æå–æ–‡æœ¬
                try:
                    parts = response.candidates[0].content.parts
                    ai_response = "".join([getattr(p, "text", "") for p in parts if getattr(p, "text", "")])
                except Exception:
                    ai_response = ""
            
            if not ai_response:
                return {
                    "success": False,
                    "error": "AIåˆ†æå¤±è´¥: æ¨¡å‹æœªè¿”å›å†…å®¹"
                }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"AIåˆ†æå¤±è´¥: {str(e)}"
            }
        
        # ç¬¬5æ­¥ï¼šè§£æAIå»ºè®®
        print(f"[DocumentFeedback] ğŸ“‹ è§£æAIå»ºè®®...")
        modifications = self.editor.parse_ai_suggestions(ai_response)
        
        # æå–æ‘˜è¦ï¼ˆAIå“åº”ä¸­çš„æ–‡å­—è¯´æ˜éƒ¨åˆ†ï¼‰
        summary = self._extract_summary(ai_response)
        
        return {
            "success": True,
            "file_path": file_path,
            "doc_type": doc_data.get("type"),
            "original_content": doc_data,
            "ai_suggestions": ai_response,
            "modifications": modifications,
            "modification_count": len(modifications),
            "summary": summary
        }
    
    def apply_suggestions(
        self,
        file_path: str,
        modifications: list
    ) -> Dict[str, Any]:
        """
        åº”ç”¨AIå»ºè®®ï¼Œç”Ÿæˆæ–°æ–‡æ¡£
        
        Args:
            file_path: åŸæ–‡æ¡£è·¯å¾„
            modifications: ä¿®æ”¹æŒ‡ä»¤åˆ—è¡¨
        
        Returns:
            {
                "success": True,
                "new_file_path": "...",
                "applied_count": 5
            }
        """
        print(f"[DocumentFeedback] âœï¸ åº”ç”¨ä¿®æ”¹...")
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨å¯¹åº”çš„ç¼–è¾‘å™¨
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext in ['.ppt', '.pptx']:
            result = self.editor.edit_ppt(file_path, modifications)
        elif ext in ['.doc', '.docx']:
            result = self.editor.edit_word(file_path, modifications)
        elif ext in ['.xls', '.xlsx']:
            result = self.editor.edit_excel(file_path, modifications)
        else:
            return {
                "success": False,
                "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}"
            }
        
        if result.get("success"):
            print(f"[DocumentFeedback] âœ… ä¿®æ”¹å®Œæˆ: {os.path.basename(result['file_path'])}")
        
        return result
    
    def full_feedback_loop(
        self,
        file_path: str,
        user_requirement: str = "",
        auto_apply: bool = True
    ) -> Dict[str, Any]:
        """
        å®Œæ•´åé¦ˆé—­ç¯ï¼šè¯»å– â†’ åˆ†æ â†’ ä¿®æ”¹
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            user_requirement: ç”¨æˆ·éœ€æ±‚
            auto_apply: æ˜¯å¦è‡ªåŠ¨åº”ç”¨ä¿®æ”¹
        
        Returns:
            å®Œæ•´çš„å¤„ç†ç»“æœ
        """
        print("=" * 60)
        print("ğŸ”„ å¯åŠ¨æ–‡æ¡£æ™ºèƒ½åé¦ˆç³»ç»Ÿ")
        print("=" * 60)
        
        # ç¬¬1æ­¥ï¼šåˆ†æå¹¶è·å–å»ºè®®
        analysis_result = self.analyze_and_suggest(file_path, user_requirement)
        
        if not analysis_result.get("success"):
            return analysis_result
        
        print(f"\nğŸ“Š åˆ†æç»“æœ:")
        print(f"   ä¿®æ”¹å»ºè®®æ•°: {analysis_result['modification_count']}")
        print(f"   æ‘˜è¦: {analysis_result['summary'][:100]}...")
        
        # ç¬¬2æ­¥ï¼šåº”ç”¨ä¿®æ”¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if auto_apply and analysis_result['modification_count'] > 0:
            apply_result = self.apply_suggestions(
                file_path,
                analysis_result['modifications']
            )
            
            return {
                "success": True,
                "analysis": analysis_result,
                "edit_result": apply_result,
                "new_file_path": apply_result.get("file_path"),
                "applied_count": apply_result.get("applied_count", 0)
            }
        else:
            return {
                "success": True,
                "analysis": analysis_result,
                "message": "ä»…åˆ†æï¼Œæœªåº”ç”¨ä¿®æ”¹"
            }
    
    def _build_analysis_prompt(
        self,
        doc_type: str,
        formatted_content: str,
        user_requirement: str
    ) -> str:
        """æ„å»ºAIåˆ†ææç¤º"""
        
        base_prompt = f"""ä½ æ˜¯Kotoæ–‡æ¡£æ™ºèƒ½åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æä»¥ä¸‹{doc_type.upper()}æ–‡æ¡£ï¼Œå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ã€‚

## æ–‡æ¡£å†…å®¹
{formatted_content}

## ç”¨æˆ·éœ€æ±‚
{user_requirement if user_requirement else "è¯·å…¨é¢å®¡æŸ¥æ–‡æ¡£ï¼Œæä¾›ä¸“ä¸šçš„ä¼˜åŒ–å»ºè®®"}

## âš ï¸ ç‰¹åˆ«æ³¨æ„
**è¯·ç‰¹åˆ«å…³æ³¨ä»¥ä¸‹å†…å®¹ï¼Œä¸è¦é—æ¼ï¼š**
1. **æ ¼å¼å˜åŒ–éƒ¨åˆ†**ï¼šåŒ…å« **ç²—ä½“**ã€*æ–œä½“*ã€[é¢œè‰²æ ‡è®°] ç­‰æ ¼å¼çš„æ–‡æœ¬ï¼ˆæ–‡æ¡£ä¸­æ ‡è®°ä¸º"[æ­¤æ®µè½æœ‰æ ¼å¼å˜åŒ–]"ï¼‰
2. **å›¾æ ‡å’Œç‰¹æ®Šå­—ç¬¦**ï¼šå¦‚â—ã€â˜…ã€âœ“ã€â†’ã€â€¢ç­‰ç¬¦å·
3. **å­—ä½“å¤§å°å˜åŒ–**ï¼šæ ‡é¢˜ã€å°å­—æ³¨é‡Šç­‰ä¸åŒå­—å·çš„å†…å®¹
4. **æ··åˆå†…å®¹**ï¼šæ—¢æœ‰æ–‡å­—åˆæœ‰å›¾æ ‡çš„éƒ¨åˆ†ï¼Œä¾‹å¦‚"â— è¦ç‚¹ä¸€"ã€"â†’ æ­¥éª¤äºŒ"
5. **ä¸­è‹±æ–‡æ··æ’**ï¼šåŒ…å«è‹±æ–‡æœ¯è¯­çš„ä¸­æ–‡å¥å­
6. **æ•°å­—å’Œå•ä½**ï¼šå¦‚"10px"ã€"100%"ã€"3.5å€"ç­‰

## ä»»åŠ¡è¦æ±‚
1. ä»”ç»†é˜…è¯»æ–‡æ¡£å†…å®¹ï¼ˆ**åŒ…æ‹¬æ‰€æœ‰æ ¼å¼æ ‡è®°çš„éƒ¨åˆ†**ï¼‰
2. åˆ†ææ¯ä¸ªéƒ¨åˆ†çš„è´¨é‡å’Œå‡†ç¡®æ€§ï¼ˆ**å°¤å…¶æ˜¯æœ‰æ ¼å¼å˜åŒ–çš„æ®µè½**ï¼‰
3. ç»™å‡ºå…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼ˆ**ç¡®ä¿è¦†ç›–æ‰€æœ‰å†…å®¹ï¼Œä¸é—æ¼å›¾æ ‡å’Œç‰¹æ®Šæ ¼å¼éƒ¨åˆ†**ï¼‰

## è¾“å‡ºæ ¼å¼
è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºä¿®æ”¹å»ºè®®ï¼š

```json
{{
  "summary": "æ•´ä½“åˆ†æå’Œå»ºè®®æ‘˜è¦",
  "modifications": [
"""
        
        if doc_type == "ppt":
            base_prompt += """    {
      "slide_index": 0,
      "action": "update_title",
      "target": "title",
      "content": "ä¿®æ”¹åçš„æ ‡é¢˜",
      "reason": "ä¿®æ”¹åŸå› "
    },
    {
      "slide_index": 1,
      "action": "update_content",
      "target": "content",
      "position": 0,
      "content": "ä¿®æ”¹åçš„å†…å®¹ç‚¹",
      "reason": "ä¿®æ”¹åŸå› "
    },
    {
      "slide_index": 2,
      "action": "add_content",
      "target": "content",
      "content": "æ–°å¢çš„è¦ç‚¹",
      "reason": "æ·»åŠ åŸå› "
    }
  ]
}
```

å¯ç”¨çš„actionç±»å‹ï¼š
- update_title: ä¿®æ”¹æ ‡é¢˜
- update_content: ä¿®æ”¹å†…å®¹ç‚¹ï¼ˆéœ€è¦positionï¼‰
- add_content: æ·»åŠ æ–°å†…å®¹ç‚¹
- delete_content: åˆ é™¤å†…å®¹ç‚¹ï¼ˆéœ€è¦positionï¼‰
"""
        
        elif doc_type == "word":
            base_prompt += """    {
      "paragraph_index": 0,
      "action": "update",
      "content": "ä¿®æ”¹åçš„æ®µè½å†…å®¹",
      "reason": "ä¿®æ”¹åŸå› "
    },
    {
      "paragraph_index": 3,
      "action": "insert",
      "content": "æ–°æ’å…¥çš„æ®µè½",
      "reason": "æ’å…¥åŸå› "
    }
  ]
}
```

å¯ç”¨çš„actionç±»å‹ï¼š
- update: ä¿®æ”¹ç°æœ‰æ®µè½
- insert: æ’å…¥æ–°æ®µè½
- delete: åˆ é™¤æ®µè½
"""
        
        elif doc_type == "excel":
            base_prompt += """    {
      "sheet_name": "Sheet1",
      "row": 0,
      "col": 0,
      "action": "update",
      "value": "æ–°å€¼",
      "reason": "ä¿®æ”¹åŸå› "
    }
  ]
}
```

å¯ç”¨çš„actionç±»å‹ï¼š
- update: ä¿®æ”¹å•å…ƒæ ¼
- insert_row: æ’å…¥è¡Œ
- delete_row: åˆ é™¤è¡Œ
"""
        
        base_prompt += """

æ³¨æ„ï¼š
- åªè¾“å‡ºJSONï¼Œä¸è¦å…¶ä»–è§£é‡Š
- modificationsæ•°ç»„ä¸­æ¯ä¸ªä¿®æ”¹éƒ½è¦æœ‰æ˜ç¡®çš„ç†ç”±
- ç´¢å¼•ä»0å¼€å§‹
- ä¿æŒä¸“ä¸šå’Œå‡†ç¡®
"""
        
        return base_prompt
    
    def _extract_summary(self, ai_response: str) -> str:
        """ä»AIå“åº”ä¸­æå–æ‘˜è¦"""
        import json
        import re
        
        try:
            # å°è¯•æå–JSONä¸­çš„summaryå­—æ®µ
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', ai_response, re.DOTALL)
            if json_match:
                data = json.loads(json_match.group(1))
                return data.get("summary", "AIå»ºè®®å·²ç”Ÿæˆ")
            
            # æå–éJSONéƒ¨åˆ†ä½œä¸ºæ‘˜è¦
            summary = re.sub(r'```json.*?```', '', ai_response, flags=re.DOTALL).strip()
            return summary[:200] if summary else "AIå»ºè®®å·²ç”Ÿæˆ"
        
        except Exception:
            return "AIå»ºè®®å·²ç”Ÿæˆ"

    def _list_available_models(self) -> List[Dict[str, str]]:
        """åˆ—å‡ºå½“å‰ API å¯ç”¨æ¨¡å‹ï¼ˆä»…åŒ…å«æ”¯æŒ generateContent çš„æ¨¡å‹ï¼‰"""
        if self._model_cache is not None:
            return self._model_cache
        if not self.client:
            self._model_cache = []
            return self._model_cache
        try:
            models = []
            for m in self.client.models.list():
                supported = getattr(m, "supported_generation_methods", []) or []
                if "generateContent" not in supported:
                    continue
                name = getattr(m, "name", "")
                display_name = getattr(m, "display_name", "")
                base_name = name.split("/")[-1] if name else ""
                if base_name:
                    models.append({
                        "name": base_name,
                        "display_name": display_name or base_name
                    })
            self._model_cache = models
            return models
        except Exception:
            self._model_cache = []
            return self._model_cache

    def _select_best_model(self, preferred: str) -> (str, List[Dict[str, str]]):
        """æ ¹æ®å¯ç”¨æ¨¡å‹é€‰æ‹©æœ€é«˜è´¨é‡æ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨ preferredï¼‰"""
        models = self._list_available_models()
        available = {m["name"] for m in models}

        if not models:
            fallback = "gemini-2.5-pro" if preferred != "gemini-2.5-pro" else preferred
            return fallback, models

        priority = [
            preferred,
            "gemini-3-flash-preview",
            "gemini-3-pro-preview",
            "gemini-3-flash",
            "gemini-3-pro",
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.0-pro",
            "gemini-2.0-flash",
            "gemini-2.0-flash-001",
        ]

        for name in priority:
            if name in available:
                return name, models

        # è‹¥æ— æ³•è·å–åˆ—è¡¨ï¼Œæˆ–åˆ—è¡¨ä¸ºç©ºï¼Œåˆ™å›é€€ä¸ºç”¨æˆ·æŒ‡å®šæ¨¡å‹
        return preferred, models

    def _format_model_table(self, models: List[Dict[str, str]]) -> str:
        """ç”Ÿæˆå¯ç”¨æ¨¡å‹è¡¨æ ¼ï¼ˆMarkdownï¼‰"""
        if not models:
            return "ï¼ˆæš‚æ—¶æ— æ³•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼‰"

        rows = ["| æ¨¡å‹ID | æ˜¾ç¤ºåç§° |", "| --- | --- |"]
        for m in models:
            rows.append(f"| {m['name']} | {m['display_name']} |")
        return "\n".join(rows)
    
    # ==================== æ–‡æ¡£è‡ªåŠ¨æ ‡æ³¨åŠŸèƒ½ ====================
    
    def _analyze_chunk_for_annotations(
        self,
        chunk: str,
        doc_type: str,
        user_requirement: str,
        model_id: str,
        chunk_index: int,
        total_chunks: int,
        full_doc_context: str = "",
        max_retries: int = 3
    ) -> Optional[List[Dict[str, str]]]:
        """
        åˆ†æå•ä¸ªåˆ†æ®µå¹¶è¿”å›æ ‡æ³¨åˆ—è¡¨ï¼ˆä¸¥æ ¼é¡ºåºæ‰§è¡Œï¼Œä¸Šä¸€æ®µå®Œæˆåå†å¤„ç†ä¸‹ä¸€æ®µï¼‰
        """
        base_context = user_requirement + f"\n(æ³¨ï¼šè¿™æ˜¯æ–‡æ¡£çš„ç¬¬{chunk_index}éƒ¨åˆ†ï¼Œå…±{total_chunks}éƒ¨åˆ†)"
        def _call_model(contents: str):
            from google.genai import types
            return self.client.models.generate_content(
                model=model_id,
                contents=contents,
                config=types.GenerateContentConfig(
                    temperature=0.2,
                    max_output_tokens=12000,
                )
            )

        def _call_with_timeout(contents: str, timeout_seconds: int = 120):
            import threading
            result_holder = {"response": None, "error": None}

            def _runner():
                try:
                    print(f"[DocumentFeedback] ğŸŒ è°ƒç”¨AI API (è¶…æ—¶: {timeout_seconds}s)...")
                    result_holder["response"] = _call_model(contents)
                    print(f"[DocumentFeedback] âœ… AIå“åº”æˆåŠŸ")
                except Exception as e:
                    print(f"[DocumentFeedback] âŒ AIè°ƒç”¨å¼‚å¸¸: {e}")
                    result_holder["error"] = e

            t = threading.Thread(target=_runner, daemon=True)
            t.start()
            t.join(timeout_seconds)
            if t.is_alive():
                print(f"[DocumentFeedback] â±ï¸ AIè°ƒç”¨è¶…æ—¶ ({timeout_seconds}s)")
                return None, TimeoutError(f"Chunk timeout after {timeout_seconds}s")
            if result_holder["error"]:
                return None, result_holder["error"]
            return result_holder["response"], None

        all_annotations: List[Dict[str, str]] = []
        seen_texts = set()
        # è´¨é‡ä¼˜å…ˆï¼šç¡®ä¿å¤šè½®å®¡é˜…ï¼Œé¿å…â€œåªè·‘ä¸€è½®â€å¯¼è‡´é—æ¼
        if len(chunk) <= 1800:
            max_rounds = 2
        else:
            max_rounds = 3
        
        min_new_per_round = 3

        for round_idx in range(1, max_rounds + 1):
            print(f"[DocumentFeedback] ğŸ” ç¬¬{chunk_index}æ®µè¿›å…¥ç¬¬{round_idx}/{max_rounds}è½®å®¡é˜…")
            # ç¬¬ä¸€è½®æ³¨é‡å…¨é¢æ‰«æï¼Œå¦‚æœåªæœ‰ä¸€è½®ï¼Œåˆ™ç›´æ¥è¦æ±‚å…¨é¢
            current_focus_prompt = ""
            if round_idx == 1:
                target_count = max(5, len(chunk) // 200) # åŠ¨æ€è®¡ç®—ç›®æ ‡æ•°é‡ï¼Œæ¯200å­—1ä¸ª
                current_focus_prompt = f"\n\næœ¬è½®é‡ç‚¹ï¼šå…¨é¢æ‰«æï¼Œæ‰¾å‡ºæ‰€æœ‰æ˜æ˜¾çš„è¯­ç—…ã€ç¿»è¯‘è…”å’Œæ‹—å£è¡¨è¾¾ã€‚ç›®æ ‡çº¦{target_count}å¤„ä¿®æ”¹ã€‚"
            else:
                current_focus_prompt = "\n\næœ¬è½®é‡ç‚¹ï¼šæŸ¥æ¼è¡¥ç¼ºï¼Œå…³æ³¨ä¸Šä¸€è½®å¯èƒ½å¿½ç•¥çš„é€»è¾‘è¿æ¥è¯ã€æ ‡ç‚¹ç¬¦å·å’Œæ·±å±‚æ¶¦è‰²ã€‚ç›®æ ‡çº¦5-10å¤„è¡¥å……ä¿®æ”¹ã€‚"

            prompt = self._build_annotation_prompt(
                doc_type,
                chunk,
                base_context + f"\n\nè¿™æ˜¯ç¬¬{round_idx}/{max_rounds}è½®å®¡é˜…ã€‚è¯·æ‰¾å‡ºçœŸæ­£éœ€è¦ä¿®æ”¹çš„é—®é¢˜ï¼Œæœ¬è½®ç›®æ ‡çº¦{target_count if round_idx == 1 else 10}å¤„ä¿®æ”¹ã€‚" + current_focus_prompt,
                full_doc_context=full_doc_context
            )
            strict_prompt = self._build_annotation_prompt(
                doc_type,
                chunk,
                base_context + f"\n\nè¯·åŠ¡å¿…ä»…è¾“å‡ºJSONæ•°ç»„ï¼Œæœ¬è½®æœ€å¤š15æ¡ï¼›è‹¥ç¡®å®æ²¡æœ‰é—®é¢˜å†è¿”å›ç©ºæ•°ç»„ã€‚"
            )

            for retry in range(max_retries):
                try:
                    response, err = _call_with_timeout(prompt)  # ä½¿ç”¨é»˜è®¤180ç§’è¶…æ—¶
                    if err:
                        raise err
                    if response and response.text:
                        annotations = self._parse_annotation_response(response.text)
                        if annotations:
                            # äºŒæ¬¡å¿«é€Ÿæ£€æµ‹ï¼šä»…è¿‡æ»¤æ‰"åŸæ–‡åœ¨æ–‡æ¡£ä¸­å®Œå…¨æ‰¾ä¸åˆ°"çš„å¹»è§‰é¡¹
                            try:
                                from web.document_validator import DocumentValidator
                                validation = DocumentValidator.validate_modifications(chunk, annotations)
                                if validation.get('risk_level') == 'HIGH':
                                    # åªè¿‡æ»¤çœŸæ­£æ‰¾ä¸åˆ°åŸæ–‡çš„é¡¹ï¼ˆå¹»è§‰ï¼‰ï¼Œä¸è¿‡æ»¤å…¶ä»– warning
                                    rejected_indices = set()
                                    for issue in validation.get('issues', []):
                                        import re
                                        m = re.match(r"#(\d+):\s*åŸæ–‡æœªæ‰¾åˆ°", issue)
                                        if m:
                                            rejected_indices.add(int(m.group(1)) - 1)
                                    
                                    if rejected_indices:
                                        before = len(annotations)
                                        annotations = [a for i, a in enumerate(annotations) if i not in rejected_indices]
                                        print(f"[DocumentFeedback] ğŸ›¡ï¸ äºŒæ¬¡æ£€æµ‹: è¿‡æ»¤ {before - len(annotations)} æ¡å¹»è§‰é¡¹ï¼Œä¿ç•™ {len(annotations)} æ¡")
                            except Exception as e:
                                print(f"[DocumentFeedback] âš ï¸ äºŒæ¬¡æ£€æµ‹è·³è¿‡: {e}")
                            
                            new_items = []
                            for item in annotations:
                                text = (item.get("åŸæ–‡ç‰‡æ®µ") or "").strip()
                                if text and text not in seen_texts:
                                    seen_texts.add(text)
                                    new_items.append(item)
                            all_annotations.extend(new_items)
                            if len(new_items) < min_new_per_round:
                                if round_idx < max_rounds:
                                    print(f"[DocumentFeedback] â„¹ï¸ ç¬¬{chunk_index}æ®µç¬¬{round_idx}è½®æ–°å¢è¾ƒå°‘({len(new_items)}æ¡)ï¼Œç»§ç»­ä¸‹ä¸€è½®æŸ¥æ¼")
                                    break
                                return all_annotations
                            break
                        if retry < max_retries - 1:
                            prompt = strict_prompt
                            continue
                        return all_annotations
                    if retry < max_retries - 1:
                        prompt = strict_prompt
                        continue
                except Exception as e:
                    error_msg = str(e)[:100]
                    if retry < max_retries - 1:
                        print(f"[DocumentFeedback] âš ï¸ ç¬¬{chunk_index}æ®µç¬¬{round_idx}è½®å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: {error_msg}")
                        continue
                    print(f"[DocumentFeedback] âŒ ç¬¬{chunk_index}æ®µç¬¬{round_idx}è½®å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {error_msg}")
                    return self._fallback_annotations_from_chunk(chunk)

        return all_annotations

    @staticmethod
    def _fallback_annotations_from_chunk(chunk: str) -> List[Dict[str, str]]:
        """AIå¤±è´¥æ—¶çš„æœ¬åœ°å…œåº•æ ‡æ³¨ - ä¼˜åŒ–ç‰ˆï¼ˆæ›´å…·ä½“çš„å»ºè®®+å‡åŒ€åˆ†å¸ƒï¼‰"""
        import re
        annotations = []
        
        # ==================== Helper: ç”Ÿæˆå…·ä½“çš„ä¿®æ”¹å»ºè®® ====================
        def suggest_remove_bei(match_obj, original_text):
            """å»æ‰è¢«åŠ¨è¯­æ€ï¼šè¢«Xçš„ â†’ Xçš„"""
            verb = match_obj.group(1) if match_obj.lastindex >= 1 else ""
            # æå–å®Œæ•´ç‰‡æ®µ
            full_match = match_obj.group(0)
            new_text = full_match.replace("è¢«", "", 1)
            return {
                "åŸæ–‡ç‰‡æ®µ": full_match,
                "ä¿®æ”¹å»ºè®®": f"å»é™¤è¢«åŠ¨è¯­æ€",
                "ä¿®æ”¹åæ–‡æœ¬": new_text,
                "ç†ç”±": "å»é™¤è¢«åŠ¨è¯­æ€ï¼Œä½¿è¡¨æ„æ›´ç›´æ¥"
            }
        
        def suggest_remove_jinxing(match_obj, original_text):
            """å»æ‰åè¯åŒ–ï¼šå¯¹Xè¿›è¡ŒY â†’ YX"""
            obj = match_obj.group(1) if match_obj.lastindex >= 1 else ""
            action = match_obj.group(2) if match_obj.lastindex >= 2 else "å¤„ç†"
            new_text = f"{action}{obj}"
            return {
                "åŸæ–‡ç‰‡æ®µ": match_obj.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–åè¯åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": new_text,
                "ç†ç”±": "é¿å…åè¯åŒ–è¡¨è¾¾ï¼Œæ›´ç¬¦åˆä¸­æ–‡ä¹ æƒ¯"
            }
        
        def suggest_simplify_tongguo(match_obj, original_text):
            """ç®€åŒ–é€šè¿‡ï¼šé€šè¿‡Xæ¥Y â†’ ç”¨Xæˆ–XY"""
            method = match_obj.group(1) if match_obj.lastindex >= 1 else ""
            if method:
                new_text = f"ç”¨{method}"
                return {
                    "åŸæ–‡ç‰‡æ®µ": match_obj.group(0),
                    "ä¿®æ”¹å»ºè®®": "ç®€åŒ–è¿æ¥è¯",
                    "ä¿®æ”¹åæ–‡æœ¬": new_text,
                    "ç†ç”±": "åˆ é™¤å†—ä½™çš„è¿æ¥è¯ï¼Œè¡¨æ„æ›´ç®€æ´"
                }
            return {
                "åŸæ–‡ç‰‡æ®µ": match_obj.group(0),
                "ä¿®æ”¹å»ºè®®": "åˆ é™¤å†—ä½™è¿æ¥è¯",
                "ä¿®æ”¹åæ–‡æœ¬": "ç›´æ¥è¡¨è¾¾",
                "ç†ç”±": "åˆ é™¤'é€šè¿‡...æ¥'ï¼Œç›´æ¥è¡¨è¾¾"
            }
        
        def suggest_remove_suo(match_obj, original_text):
            """å»æ‰æ‰€å­—ç»“æ„ï¼šæ‰€Xçš„Y â†’ Xçš„Y"""
            verb = match_obj.group(1) if match_obj.lastindex >= 1 else ""
            full_match = match_obj.group(0)
            new_text = full_match.replace("æ‰€", "", 1)
            return {
                "åŸæ–‡ç‰‡æ®µ": full_match,
                "ä¿®æ”¹å»ºè®®": "å»é™¤å†—ä½™çš„'æ‰€'å­—ç»“æ„",
                "ä¿®æ”¹åæ–‡æœ¬": new_text,
                "ç†ç”±": "å»é™¤å†—ä½™çš„'æ‰€'å­—ç»“æ„ï¼Œç®€åŒ–è¡¨è¾¾"
            }
        
        # ==================== ä¼˜åŒ–ç­–ç•¥1ï¼šè¢«åŠ¨å¥é—®é¢˜ ====================
        # è¢«+åŠ¨è¯ â†’ å»æ‰"è¢«"
        passive_patterns = [
            (r'è¢«(\w{2,6})(?:çš„|äº†|ç€)', suggest_remove_bei),
            (r'è¢«(\w{2,4})å‘ˆç°', suggest_remove_bei),
            (r'è¢«(\w{2,4})è®°å½•', suggest_remove_bei),
            (r'è¢«(\w{2,4})æ„ŸçŸ¥', suggest_remove_bei),
            (r'è¢«(ç”¨)ä¸º', suggest_remove_bei),  # æ–°å¢ï¼šè¢«ç”¨ä¸º
            (r'è¢«(ç§°)ä¸º', suggest_remove_bei),  # æ–°å¢ï¼šè¢«ç§°ä¸º
            (r'è¢«(è§†)ä¸º', suggest_remove_bei),  # æ–°å¢ï¼šè¢«è§†ä¸º
            (r'è¢«(è®¤)ä¸º', suggest_remove_bei),  # æ–°å¢ï¼šè¢«è®¤ä¸º
        ]
        for pattern, suggest_func in passive_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 3 <= len(text) <= 15:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥2ï¼šåè¯åŒ–é—®é¢˜ ====================
        # å¯¹Xè¿›è¡ŒY â†’ YX
        nominalization_patterns = [
            (r'å¯¹(\w{2,4})è¿›è¡Œ(\w{2,4})', suggest_remove_jinxing),
            (r'è¿›è¡Œ(\w{2,4})(?:çš„)', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–åè¯åŒ–",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(1)}çš„",
                "ç†ç”±": "é¿å…'è¿›è¡Œ'çš„å†—ä½™è¡¨è¾¾"
            }),
            (r'è¿›è¡Œäº†(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–åè¯åŒ–",
                "ä¿®æ”¹åæ–‡æœ¬": m.group(1),
                "ç†ç”±": "å»æ‰'è¿›è¡Œäº†'ï¼Œç›´æ¥ç”¨åŠ¨è¯"
            }),
        ]
        for pattern, suggest_func in nominalization_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 4 <= len(text) <= 12:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥3ï¼šå†—ä½™è½¬ç§»è¯ ====================
        # é€šè¿‡Xæ¥Y â†’ ç”¨Xæˆ–XY
        connector_patterns = [
            (r'é€šè¿‡(\w{2,6})(?:å¾—ä»¥|æ¥|ä»¥)', suggest_simplify_tongguo),
            (r'ä»è€Œ(\w{2,4})(?:å¾—ä»¥|ä½¿|è®©)', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–é€»è¾‘è¿æ¥",
                "ä¿®æ”¹åæ–‡æœ¬": f"ä½¿{m.group(1)}",
                "ç†ç”±": "ç”¨'ä½¿'ç®€åŒ–'ä»è€Œ'çš„è¡¨è¾¾"
            }),
            (r'ç”±äº(\w{2,4})(?:ï¼Œ|ï¼Œ)ä»è€Œ', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–é€»è¾‘è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": f"ç”±äº{m.group(1)}ï¼Œå› æ­¤",
                "ç†ç”±": "ç”¨'å› æ­¤'æˆ–'æ‰€ä»¥'ç®€åŒ–"
            }),
            (r'ä»¥ä¾¿(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "æ”¹ç”¨æ›´è‡ªç„¶çš„è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": f"ä¸ºäº†{m.group(1)}",
                "ç†ç”±": "'ä¸ºäº†'æ¯”'ä»¥ä¾¿'æ›´è‡ªç„¶"
            }),
        ]
        for pattern, suggest_func in connector_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 4 <= len(text) <= 15:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥4ï¼šå­¦æœ¯è™šè¯ ====================
        # å½±å“/ä½œç”¨ç­‰ â†’ å…·ä½“åŠ¨è¯
        abstract_terms = [
            (r'\bå½±å“\b(?!åŠ›)', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "æ›¿æ¢ä¸ºå…·ä½“åŠ¨è¯",
                "ä¿®æ”¹åæ–‡æœ¬": "ï¼ˆå†³å®š/åˆ¶çº¦/æ”¹å˜/é©±åŠ¨ï¼‰",
                "ç†ç”±": "é¿å…ä½¿ç”¨ç©ºæ³›çš„å­¦æœ¯è™šè¯"
            }),
            (r'\bä½œç”¨\b(?!åŸŸ)', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "æ›¿æ¢ä¸ºå…·ä½“åŠ¨è¯",
                "ä¿®æ”¹åæ–‡æœ¬": "ï¼ˆé©±åŠ¨/ä¿ƒè¿›/æŠ‘åˆ¶/æ¨åŠ¨ï¼‰",
                "ç†ç”±": "é¿å…ä½¿ç”¨ç©ºæ³›çš„å­¦æœ¯è™šè¯"
            }),
            (r'\bå…³ç³»\b', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å…·ä½“åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": "ï¼ˆå› æœå…³ç³»/ç›¸å…³æ€§/å¯¹åº”å…³ç³»ï¼‰",
                "ç†ç”±": "ç”¨å…·ä½“çš„å…³ç³»ç±»å‹æ›¿ä»£æ³›æŒ‡"
            }),
            (r'\bæœºåˆ¶\b', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å…·ä½“åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": "ï¼ˆåŸç†/è¿‡ç¨‹/æ–¹æ³•/è§„å¾‹ï¼‰",
                "ç†ç”±": "ç”¨å…·ä½“æ¦‚å¿µæ›¿ä»£ç©ºæ³›çš„'æœºåˆ¶'"
            }),
            (r'\bå› ç´ \b', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å…·ä½“åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": "ï¼ˆæ¡ä»¶/å‚æ•°/å˜é‡/è¦ç´ ï¼‰",
                "ç†ç”±": "ç”¨å…·ä½“æ¦‚å¿µæ›¿ä»£ç©ºæ³›çš„'å› ç´ '"
            }),
        ]
        for pattern, suggest_func in abstract_terms:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 2 <= len(text) <= 6:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥5ï¼šæ‰€å­—ç»“æ„ ====================
        # æ‰€+åŠ¨è¯+çš„ â†’ å»æ‰"æ‰€"
        suo_patterns = [
            (r'æ‰€(\w{2,4})çš„', suggest_remove_suo),
            (r'æ‰€è°“(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "åˆ é™¤å†—ä½™ä¿®é¥°",
                "ä¿®æ”¹åæ–‡æœ¬": m.group(1),
                "ç†ç”±": "ç›´æ¥è¡¨è¾¾ï¼Œåˆ é™¤'æ‰€è°“'çš„å†—ä½™ä¿®é¥°"
            }),
        ]
        for pattern, suggest_func in suo_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 3 <= len(text) <= 10:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥6ï¼šæ¨¡ç³Šé™å®šè¯ ====================
        # éå¸¸/æå…¶ç­‰ â†’ åˆ é™¤æˆ–æ¢æ›´å¼ºåŠ¨è¯
        hedge_patterns = [
            (r'(?:éå¸¸|æå…¶|ååˆ†|ç‰¹åˆ«)(\w{2,6})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "åˆ é™¤æ¨¡ç³Šé™å®šè¯",
                "ä¿®æ”¹åæ–‡æœ¬": m.group(1),
                "ç†ç”±": "ç›´æ¥ä½¿ç”¨å½¢å®¹è¯ï¼Œåˆ é™¤æ¨¡ç³Šä¿®é¥°"
            }),
            (r'(?:ä¼¼ä¹|å¥½åƒ|ä»¿ä½›)(\w{2,6})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "æ”¹ä¸ºæ›´ç¡®å®šçš„è¡¨è¿°",
                "ä¿®æ”¹åæ–‡æœ¬": f"å¯èƒ½æ˜¯{m.group(1)}/è¡¨æ˜{m.group(1)}",
                "ç†ç”±": "é¿å…æ¨¡ç³Šçš„æ¨æµ‹è¡¨è¾¾"
            }),
        ]
        for pattern, suggest_func in hedge_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)[:12]
                if 3 <= len(text) <= 12:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥7ï¼šå†—é•¿è¡¨è¾¾ ====================
        # Xä¹‹é—´çš„Y â†’ Xçš„Y
        redundant_patterns = [
            (r'(\w{2,6})ä¹‹é—´(?:çš„)?', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å»æ‰å†—ä½™çš„'ä¹‹é—´'",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(1)}çš„",
                "ç†ç”±": "ç®€åŒ–å†—ä½™çš„'ä¹‹é—´'è¡¨è¾¾"
            }),
            (r'(\w{2,4})(?:å½¢å¼|æ–¹å¼|è¿‡ç¨‹)çš„(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å»é™¤å†—ä½™åè¯",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(1)}{m.group(2)}",
                "ç†ç”±": "åˆ é™¤'å½¢å¼/æ–¹å¼/è¿‡ç¨‹'ç­‰å†—ä½™åè¯"
            }),
        ]
        for pattern, suggest_func in redundant_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 4 <= len(text) <= 15:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥8ï¼šæ¶ˆæè¡¨è¿° ====================
        negative_patterns = [
            (r'(?:ä¸|æ— |æ²¡æœ‰)(?:èƒ½|æ³•|åŠæ³•)(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "æ”¹ä¸ºæ­£é¢è¡¨è¿°",
                "ä¿®æ”¹åæ–‡æœ¬": f"éš¾ä»¥{m.group(1)}/å°šæœª{m.group(1)}",
                "ç†ç”±": "ç”¨æ­£é¢è¡¨è¿°æ›¿ä»£å¦å®šå½¢å¼"
            }),
            (r'éš¾ä»¥(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "æ”¹ä¸ºæ›´è‡ªç„¶çš„è¡¨è¿°",
                "ä¿®æ”¹åæ–‡æœ¬": f"å¾ˆéš¾{m.group(1)}",
                "ç†ç”±": "'å¾ˆéš¾X'æ¯”'éš¾ä»¥X'æ›´è‡ªç„¶"
            }),
        ]
        for pattern, suggest_func in negative_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)[:10]
                if 3 <= len(text) <= 10:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥9ï¼šé«˜é¢‘è™šè¯æå– ====================
        freq_words = [
            ('èƒ½å¤Ÿ', {
                "ä¿®æ”¹å»ºè®®": "åˆ é™¤å†—ä½™è™šè¯",
                "ä¿®æ”¹åæ–‡æœ¬": "ï¼ˆç›´æ¥ç”¨å…·ä½“åŠ¨è¯ï¼‰",
                "ç†ç”±": "åˆ é™¤è™šè¯ï¼Œç›´æ¥è¡¨è¾¾åŠ¨ä½œ"
            }),
            ('å¯ä»¥', {
                "ä¿®æ”¹å»ºè®®": "æ ¹æ®è¯­å¢ƒä¼˜åŒ–",
                "ä¿®æ”¹åæ–‡æœ¬": "ï¼ˆåˆ é™¤æˆ–æ¢å…·ä½“åŠ¨è¯ï¼‰",
                "ç†ç”±": "'å¯ä»¥'å¾€å¾€å¯ä»¥çœç•¥æˆ–ç”¨æ›´å…·ä½“çš„åŠ¨è¯æ›¿ä»£"
            }),
            ('è¿›ä¸€æ­¥', {
                "ä¿®æ”¹å»ºè®®": "æ”¹ç”¨æ›´è‡ªç„¶çš„è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": "è¿›è€Œ/æ¥ç€/éšå",
                "ç†ç”±": "'è¿›ä¸€æ­¥'çš„æ›¿ä»£è¡¨è¾¾æ›´è‡ªç„¶"
            }),
        ]
        for word, anno_template in freq_words:
            for match in re.finditer(re.escape(word), chunk):
                # æå–ä¸Šä¸‹æ–‡
                start = max(0, match.start() - 3)
                end = min(len(chunk), match.end() + 5)
                context = chunk[start:end].replace('\n', '').strip()
                if len(context) > len(word) and len(context) <= 20:
                    annotations.append({
                        "åŸæ–‡ç‰‡æ®µ": context[:15],
                        "ä¿®æ”¹å»ºè®®": anno_template["ä¿®æ”¹å»ºè®®"],
                        "ä¿®æ”¹åæ–‡æœ¬": anno_template["ä¿®æ”¹åæ–‡æœ¬"],
                        "ç†ç”±": anno_template["ç†ç”±"]
                    })
                    break  # æ¯ä¸ªè¯æœ€å¤šæå–ä¸€æ¬¡ä¸Šä¸‹æ–‡
        
        # ==================== ä¼˜åŒ–ç­–ç•¥10ï¼šè¡¨è¾¾å†—ä½™æ£€æµ‹ ====================
        redundancy_patterns = [
            (r'è¿›è¡Œ(\w{1,4})(å¤„ç†|åˆ†æ|ç ”ç©¶|æ“ä½œ|è§‚å¯Ÿ|æµ‹è¯•|éªŒè¯)', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å»æ‰å†—ä½™çš„'è¿›è¡Œ'",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(2)}{m.group(1)}",
                "ç†ç”±": "'è¿›è¡Œ'ä¸åé¢çš„åŠ¨è¯æ„æˆå†—ä½™"
            }),
            (r'å¯¹(\w{1,4})çš„(åˆ†æ|ç ”ç©¶|å¤„ç†|è§‚å¯Ÿ|ç†è§£|è®¤è¯†)', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–å†—ä½™ç»“æ„",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(2)}{m.group(1)}",
                "ç†ç”±": "å»é™¤'å¯¹...çš„'å†—ä½™ç»“æ„"
            }),
        ]
        for pattern, suggest_func in redundancy_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 4 <= len(text) <= 12:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== ä¼˜åŒ–ç­–ç•¥11ï¼šå­¦æœ¯ç¿»è¯‘å¸¸è§é—®é¢˜ ====================
        academic_patterns = [
            # "å‘ˆç°å‡º" â†’ "å‘ˆç°"
            (r'å‘ˆç°å‡º(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å»æ‰å†—ä½™çš„'å‡º'",
                "ä¿®æ”¹åæ–‡æœ¬": f"å‘ˆç°{m.group(1)}",
                "ç†ç”±": "'å‘ˆç°å‡º'ä¸­çš„'å‡º'æ˜¯å†—ä½™çš„"
            }),
            # "æ˜¾ç¤ºå‡º" â†’ "æ˜¾ç¤º"
            (r'æ˜¾ç¤ºå‡º(\w{2,4})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "å»æ‰å†—ä½™çš„'å‡º'",
                "ä¿®æ”¹åæ–‡æœ¬": f"æ˜¾ç¤º{m.group(1)}",
                "ç†ç”±": "'æ˜¾ç¤ºå‡º'ä¸­çš„'å‡º'æ˜¯å†—ä½™çš„"
            }),
            # "å…·æœ‰...æ€§" â†’ "æœ‰...æ€§" æˆ–ç›´æ¥ç”¨å½¢å®¹è¯
            (r'å…·æœ‰(\w{2,4})æ€§', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": f"æœ‰{m.group(1)}æ€§",
                "ç†ç”±": "'å…·æœ‰'å¯ç®€åŒ–ä¸º'æœ‰'"
            }),
            # "å…·æœ‰...çš„ç‰¹å¾" â†’ "æœ‰...ç‰¹å¾"
            (r'å…·æœ‰(\w{2,4})çš„ç‰¹å¾', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": f"æœ‰{m.group(1)}ç‰¹å¾",
                "ç†ç”±": "ç®€åŒ–å†—ä½™è¡¨è¾¾"
            }),
            # "åœ¨...æ–¹é¢" å¯èƒ½æ˜¯å†—ä½™çš„
            (r'åœ¨(\w{2,4})æ–¹é¢', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "è€ƒè™‘åˆ é™¤",
                "ä¿®æ”¹åæ–‡æœ¬": f"åœ¨{m.group(1)}ä¸Š",
                "ç†ç”±": "'åœ¨...æ–¹é¢'å¯èƒ½å†—ä½™ï¼Œè€ƒè™‘ç®€åŒ–"
            }),
            # "è·å¾—äº†...çš„" â†’ "è·å¾—..."
            (r'è·å¾—äº†(\w{2,4})çš„', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": f"è·å¾—{m.group(1)}",
                "ç†ç”±": "å»æ‰å†—ä½™çš„'äº†...çš„'"
            }),
            # "äº§ç”Ÿäº†...çš„" â†’ "äº§ç”Ÿ..."
            (r'äº§ç”Ÿäº†(\w{2,4})çš„', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ç®€åŒ–è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": f"äº§ç”Ÿ{m.group(1)}",
                "ç†ç”±": "å»æ‰å†—ä½™çš„'äº†...çš„'"
            }),
        ]
        for pattern, suggest_func in academic_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 3 <= len(text) <= 15:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== æ–°å¢ï¼šä¼˜åŒ–ç­–ç•¥12ï¼šå›¾æ ‡å’Œç‰¹æ®Šç¬¦å·ä¼˜åŒ– ====================
        # è¯†åˆ«å›¾æ ‡å’Œç‰¹æ®Šå­—ç¬¦ï¼Œæä¾›ä¼˜åŒ–å»ºè®®
        icon_patterns = [
            # â—ã€â€¢ ç­‰åœ†ç‚¹ç¬¦å·
            (r'[â—â—‹â—†â—‡â– â–¡â–ªâ–«â€¢][\s]*([^\n]{1,30})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0)[:20],
                "ä¿®æ”¹å»ºè®®": "ç»Ÿä¸€åˆ—è¡¨ç¬¦å·",
                "ä¿®æ”¹åæ–‡æœ¬": f"â€¢ {m.group(1)}",
                "ç†ç”±": "å»ºè®®ä½¿ç”¨ç»Ÿä¸€çš„åˆ—è¡¨ç¬¦å·'â€¢'ï¼Œæå‡æ–‡æ¡£ä¸€è‡´æ€§"
            }),
            # â†’ â† â†‘ â†“ ç­‰ç®­å¤´ç¬¦å·
            (r'[â†’â†â†‘â†“â‡’â‡â‡‘â‡“â”âœ][\s]*([^\n]{1,30})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0)[:20],
                "ä¿®æ”¹å»ºè®®": "ä¼˜åŒ–ç®­å¤´è¡¨è¾¾",
                "ä¿®æ”¹åæ–‡æœ¬": f"â†’ {m.group(1)}" if m.group(0)[0] in 'â†’â‡’â”âœ' else f"â† {m.group(1)}",
                "ç†ç”±": "ç»Ÿä¸€ç®­å¤´ç¬¦å·æ ·å¼ï¼Œå»ºè®®ä½¿ç”¨'â†’'æˆ–ç”¨æ–‡å­—è¡¨è¾¾"
            }),
            # â˜… â˜† ç­‰æ˜Ÿå·ç¬¦å·
            (r'[â˜…â˜†âœ“âœ”âœ•âœ–âœ—âœ˜][\s]*([^\n]{1,30})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0)[:20],
                "ä¿®æ”¹å»ºè®®": "è§„èŒƒç‰¹æ®Šæ ‡è®°",
                "ä¿®æ”¹åæ–‡æœ¬": f"âœ“ {m.group(1)}" if m.group(0)[0] in 'â˜…â˜†âœ“âœ”' else f"âœ— {m.group(1)}",
                "ç†ç”±": "ä½¿ç”¨æ ‡å‡†ç¬¦å·ï¼Œæå‡å¯è¯»æ€§"
            }),
            # æ•°å­—åºå·åè·Ÿå†…å®¹ï¼Œä½†æ ¼å¼ä¸è§„èŒƒ
            (r'(\d+)[\\.ã€ã€‚\s]{0,2}([^\n]{5,30})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0)[:20],
                "ä¿®æ”¹å»ºè®®": "è§„èŒƒåºå·æ ¼å¼",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(1)}. {m.group(2).strip()}",
                "ç†ç”±": "ç»Ÿä¸€ä½¿ç”¨'æ•°å­—.'çš„åºå·æ ¼å¼"
            }),
        ]
        for pattern, suggest_func in icon_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)[:20]
                if 2 <= len(text) <= 20:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== æ–°å¢ï¼šä¼˜åŒ–ç­–ç•¥13ï¼šä¸­è‹±æ–‡æ··æ’ä¼˜åŒ– ====================
        # è¯†åˆ«ä¸­è‹±æ–‡æ··æ’ï¼Œæ£€æŸ¥ç©ºæ ¼å’Œæ ¼å¼
        mixed_patterns = [
            # ä¸­æ–‡+è‹±æ–‡æ— ç©ºæ ¼
            (r'([\u4e00-\u9fa5])([A-Za-z]{2,})', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ä¸­è‹±æ–‡é—´åŠ ç©ºæ ¼",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(1)} {m.group(2)}",
                "ç†ç”±": "ä¸­è‹±æ–‡æ··æ’æ—¶å»ºè®®åŠ ç©ºæ ¼ï¼Œæå‡å¯è¯»æ€§"
            }),
            # è‹±æ–‡+ä¸­æ–‡æ— ç©ºæ ¼
            (r'([A-Za-z]{2,})([\u4e00-\u9fa5])', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "ä¸­è‹±æ–‡é—´åŠ ç©ºæ ¼",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(1)} {m.group(2)}",
                "ç†ç”±": "ä¸­è‹±æ–‡æ··æ’æ—¶å»ºè®®åŠ ç©ºæ ¼ï¼Œæå‡å¯è¯»æ€§"
            }),
            # ä¸­æ–‡+æ•°å­—æ— ç©ºæ ¼ï¼ˆæŸäº›æƒ…å†µï¼‰
            (r'([\u4e00-\u9fa5])(\d{2,}[A-Za-z%]+)', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(0),
                "ä¿®æ”¹å»ºè®®": "æ•°å­—å•ä½å‰åŠ ç©ºæ ¼",
                "ä¿®æ”¹åæ–‡æœ¬": f"{m.group(1)} {m.group(2)}",
                "ç†ç”±": "ä¸­æ–‡ä¸å¸¦å•ä½æ•°å­—é—´å»ºè®®åŠ ç©ºæ ¼"
            }),
        ]
        for pattern, suggest_func in mixed_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(0)
                if 2 <= len(text) <= 15:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== æ–°å¢ï¼šä¼˜åŒ–ç­–ç•¥14ï¼šæ ¼å¼å˜åŒ–æ£€æµ‹ ====================
        # æ£€æµ‹HTMLæ ‡è®°ï¼ˆä»å¢å¼ºçš„æå–ä¸­æ¥çš„ï¼‰
        format_patterns = [
            # <b>xxx</b> ç²—ä½“æ ‡è®°
            (r'<b>([^<]{2,20})</b>', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(1),
                "ä¿®æ”¹å»ºè®®": "æ£€æŸ¥ç²—ä½“ä½¿ç”¨",
                "ä¿®æ”¹åæ–‡æœ¬": m.group(1),
                "ç†ç”±": "æ­¤å¤„ä½¿ç”¨äº†ç²—ä½“å¼ºè°ƒï¼Œè¯·ç¡®è®¤æ˜¯å¦å¿…è¦"
            }),
            # <i>xxx</i> æ–œä½“æ ‡è®°
            (r'<i>([^<]{2,20})</i>', lambda m, t: {
                "åŸæ–‡ç‰‡æ®µ": m.group(1),
                "ä¿®æ”¹å»ºè®®": "æ£€æŸ¥æ–œä½“ä½¿ç”¨",
                "ä¿®æ”¹åæ–‡æœ¬": m.group(1),
                "ç†ç”±": "æ­¤å¤„ä½¿ç”¨äº†æ–œä½“ï¼Œç¡®è®¤æ˜¯å¦ç¬¦åˆæ–‡æ¡£è§„èŒƒ"
            }),
        ]
        for pattern, suggest_func in format_patterns:
            for m in re.finditer(pattern, chunk):
                text = m.group(1) if m.lastindex >= 1 else m.group(0)
                if 2 <= len(text) <= 20:
                    anno = suggest_func(m, text)
                    if isinstance(anno, dict):
                        annotations.append(anno)
        
        # ==================== å»é‡+å‡åŒ€åˆ†å¸ƒ ====================
        # æŒ‰ç­–ç•¥åˆ†ç»„ç»Ÿè®¡ï¼Œç¡®ä¿å„ç±»éƒ½æœ‰ä»£è¡¨
        unique_annos = {}  # {åŸæ–‡ç‰‡æ®µ: å®Œæ•´æ ‡æ³¨}
        for anno in annotations:
            text = anno.get("åŸæ–‡ç‰‡æ®µ", "").strip()
            # æ”¾å®½é•¿åº¦é™åˆ¶ï¼š2-25å­—ç¬¦ï¼ˆä¹‹å‰æ˜¯2-20ï¼‰
            if text and 2 <= len(text) <= 25:
                if text not in unique_annos:
                    unique_annos[text] = anno  # ä¿ç•™å®Œæ•´çš„æ ‡æ³¨å¯¹è±¡
        
        # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æŒ‰é•¿åº¦åˆ†å¸ƒæ’åºï¼ˆå®ç°å‡åŒ€åˆ†å¸ƒï¼‰
        unique_list = list(unique_annos.values())
        unique_list.sort(key=lambda x: len(x.get("åŸæ–‡ç‰‡æ®µ", "")))
        
        # å¤§å¹…æå‡æ¯ä¸ªchunkçš„æ ‡æ³¨æ•°é‡ï¼š50 â†’ 80
        return unique_list[:80]

    @staticmethod
    def _split_into_chunks_by_paragraphs(formatted_content: str, max_chars: int) -> List[str]:
        """æŒ‰æ®µè½åˆ‡åˆ†ï¼Œä¿è¯æ¯æ®µä¸è¶…è¿‡max_chars"""
        paragraphs = [p for p in formatted_content.split("\n\n") if p.strip()]
        chunks = []
        current = []
        current_len = 0

        for para in paragraphs:
            para_len = len(para) + 2  # é¢„ç•™åˆ†éš”ç¬¦é•¿åº¦
            if current_len + para_len > max_chars and current:
                chunks.append("\n\n".join(current))
                current = [para]
                current_len = para_len
            else:
                current.append(para)
                current_len += para_len

        if current:
            chunks.append("\n\n".join(current))

        return chunks

    def analyze_for_annotation_chunked(
        self,
        file_path: str,
        user_requirement: str = "",
        model_id: Optional[str] = None,
        chunk_size: int = 5000,  # å¢å¤§åˆ†å—å¤§å°ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•° (åŸ: 3000)
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> Dict[str, Any]:
        """
        åˆ†æ®µå¤„ç†å¤§æ–‡æ¡£æ ‡æ³¨ï¼ˆé€‚ç”¨äºè¶…å¤§æ–‡æ¡£ï¼‰
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            user_requirement: ç”¨æˆ·éœ€æ±‚
            model_id: LLMæ¨¡å‹ID
            chunk_size: æ¯æ®µæœ€å¤§å­—ç¬¦æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(current, total, message)
            
        Returns:
            åˆå¹¶åçš„æ ‡æ³¨ç»“æœ
        """
        effective_model_id = model_id or self.default_model_id
        print(f"[DocumentFeedback] ğŸ“– è¯»å–å¤§æ–‡æ¡£: {os.path.basename(file_path)}")
        
        # è¯»å–æ–‡æ¡£
        doc_data = self.reader.read_document(file_path)
        if not doc_data.get("success"):
            return {"success": False, "error": f"è¯»å–æ–‡æ¡£å¤±è´¥: {doc_data.get('error')}"}
        
        formatted_content = self.reader.format_for_ai(doc_data)
        total_length = len(formatted_content)
        
        # å¦‚æœæ–‡æ¡£ä¸å¤§ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†æ–¹æ³•
        if total_length <= chunk_size:
            print(f"[DocumentFeedback] ğŸ“„ æ–‡æ¡£è¾ƒå°({total_length}å­—ç¬¦)ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†")
            return self.analyze_for_annotation(file_path, user_requirement, effective_model_id)

        # åˆ†æ®µå¤„ç†ï¼ˆæŒ‰æ®µè½åˆ‡åˆ†ï¼Œä¿è¯ä¸æ‰“æ–­å¥å­ï¼‰
        print(f"[DocumentFeedback] ğŸ“š æ–‡æ¡£è¾ƒå¤§({total_length}å­—ç¬¦)ï¼Œåˆ†æ®µå¤„ç†")
        chunks = self._split_into_chunks_by_paragraphs(formatted_content, chunk_size)
        
        # AIç¦ç”¨æ—¶ï¼Œå¯¹æ¯ä¸ªchunkåº”ç”¨æœ¬åœ°å…œåº•
        if os.getenv("KOTO_DISABLE_AI") == "1":
            print(f"[DocumentFeedback] âš ï¸ KOTO_DISABLE_AI=1ï¼Œä½¿ç”¨æœ¬åœ°å…œåº•æ ‡æ³¨ï¼ˆ{len(chunks)}æ®µï¼‰")
            
            # ç¬¬ä¸€è½®ï¼šæ”¶é›†æ‰€æœ‰å€™é€‰æ ‡æ³¨ï¼ŒåŒæ—¶ç»Ÿè®¡æ¯æ®µçš„ä¿¡æ¯å¯†åº¦
            all_candidates = []  # [(åŸæ–‡ç‰‡æ®µ, ä¿®æ”¹å»ºè®®, chunk_index, ä¼˜å…ˆçº§)]
            chunk_densities = []  # è®°å½•æ¯æ®µçš„è¯æ±‡å¯†åº¦
            
            print(f"\n[DocumentFeedback] ğŸ“‹ ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ ‡æ³¨å€™é€‰...\n")
            for i, chunk in enumerate(chunks):
                chunk_fallback = self._fallback_annotations_from_chunk(chunk)
                density = len(chunk_fallback) / max(1, len(chunk) / 1000)  # æ¯1000å­—çš„æ ‡æ³¨å¯†åº¦
                chunk_densities.append(density)
                
                for anno in chunk_fallback:
                    all_candidates.append({
                        "åŸæ–‡ç‰‡æ®µ": anno.get("åŸæ–‡ç‰‡æ®µ", ""),
                        "ä¿®æ”¹å»ºè®®": anno.get("ä¿®æ”¹å»ºè®®", ""),
                        "ä¿®æ”¹åæ–‡æœ¬": anno.get("ä¿®æ”¹åæ–‡æœ¬", ""),
                        "ç†ç”±": anno.get("ç†ç”±", ""),
                        "chunk_idx": i,
                        "density": density
                    })
                
                progress = ((i + 1) / len(chunks)) * 100
                bar_filled = int(10 * (i + 1) / len(chunks))
                bar = 'â–ˆ' * bar_filled + 'â–‘' * (10 - bar_filled)
                print(f"\r[DocumentFeedback] ğŸ” [{bar}] {i+1}/{len(chunks)} | å¯†åº¦: {density:.1f}/åƒå­—", end="")
            
            print()  # æ¢è¡Œ
            
            # è®¡ç®—å¹³å‡å¯†åº¦å’Œç›®æ ‡æ ‡æ³¨æ•°
            avg_density = sum(chunk_densities) / len(chunk_densities) if chunk_densities else 0
            target_count = len(formatted_content) // 1000 * 10
            
            # ç¬¬äºŒè½®ï¼šæŒ‰å¯†åº¦å‡è¡¡é€‰æ‹©æ ‡æ³¨
            print(f"\n[DocumentFeedback] âš–ï¸ ç¬¬äºŒé˜¶æ®µï¼šå‡è¡¡åˆ†å¸ƒï¼ˆç›®æ ‡{target_count}æ¡ï¼‰...\n")
            
            # åˆ†chunké€‰æ‹©ï¼Œç¡®ä¿æ¯æ®µéƒ½æœ‰é€‚å½“æ•°é‡
            target_per_chunk = max(1, target_count // len(chunks))
            selected_annotations = []
            seen_texts = set()
            
            for chunk_idx in range(len(chunks)):
                chunk_candidates = [c for c in all_candidates if c["chunk_idx"] == chunk_idx]
                
                # å»é‡ï¼ˆæ”¾å®½é•¿åº¦é™åˆ¶ï¼‰
                unique_candidates = {}
                for c in chunk_candidates:
                    text = c["åŸæ–‡ç‰‡æ®µ"].strip()
                    if text and 2 <= len(text) <= 20:  # æ”¾å®½ä¸Šé™
                        if text not in unique_candidates:
                            unique_candidates[text] = c
                
                # æŒ‰å¯†åº¦è°ƒæ•´è¯¥æ®µåº”å–çš„æ•°é‡
                if chunk_idx < 2:
                    # å‰ä¸¤æ®µè¯æ±‡å¯†é›†ï¼Œå¤šå–
                    take_count = min(len(unique_candidates), target_per_chunk + 12)
                elif chunk_idx == 2:
                    # ç¬¬3æ®µç›¸å¯¹ç¨€ç–
                    take_count = min(len(unique_candidates), target_per_chunk + 8)
                elif chunk_idx == 3:
                    # ç¬¬4æ®µå¯†é›†
                    take_count = min(len(unique_candidates), target_per_chunk + 12)
                else:
                    # ç¬¬5æ®µæœ€åçš„éƒ½å–
                    take_count = len(unique_candidates)
                
                # é€‰æ‹©è¯¥æ®µçš„æ ‡æ³¨
                chunk_selection = list(unique_candidates.values())[:take_count]
                
                for anno in chunk_selection:
                    text = anno["åŸæ–‡ç‰‡æ®µ"].strip()
                    if text not in seen_texts:
                        seen_texts.add(text)
                        selected_annotations.append({
                            "åŸæ–‡ç‰‡æ®µ": anno["åŸæ–‡ç‰‡æ®µ"],
                            "ä¿®æ”¹å»ºè®®": anno["ä¿®æ”¹å»ºè®®"],
                            "ä¿®æ”¹åæ–‡æœ¬": anno.get("ä¿®æ”¹åæ–‡æœ¬", ""),
                            "ç†ç”±": anno.get("ç†ç”±", "")
                        })
                
                progress = ((chunk_idx + 1) / len(chunks)) * 100
                bar_filled = int(20 * (chunk_idx + 1) / len(chunks))
                bar = 'â–ˆ' * bar_filled + 'â–‘' * (20 - bar_filled)
                print(f"\r[DocumentFeedback] ğŸ“Š [{bar}] {chunk_idx+1}/{len(chunks)} ({progress:.0f}%) | " +
                      f"æœ¬æ®µ{len(chunk_selection)}æ¡ | ç´¯è®¡{len(selected_annotations)}æ¡", end="")
            
            print()  # æ¢è¡Œ
            
            # å¦‚æœæ ‡æ³¨æ•°ä¸è¶³ç›®æ ‡ï¼Œè¡¥å……
            if len(selected_annotations) < target_count:
                shortage = target_count - len(selected_annotations)
                for c in all_candidates:
                    if shortage <= 0:
                        break
                    text = c["åŸæ–‡ç‰‡æ®µ"].strip()
                    if text not in seen_texts and 2 <= len(text) <= 25:  # æ”¾å®½é™åˆ¶
                        seen_texts.add(text)
                        selected_annotations.append({
                            "åŸæ–‡ç‰‡æ®µ": text,
                            "ä¿®æ”¹å»ºè®®": c["ä¿®æ”¹å»ºè®®"],
                            "ä¿®æ”¹åæ–‡æœ¬": c.get("ä¿®æ”¹åæ–‡æœ¬", ""),
                            "ç†ç”±": c.get("ç†ç”±", "")
                        })
                        shortage -= 1
            
            return {
                "success": True,
                "file_path": file_path,
                "annotations": selected_annotations[:target_count],
                "summary": f"æœ¬åœ°å…œåº•åˆ†{len(chunks)}æ®µç”Ÿæˆ{len(selected_annotations)}æ¡æ ‡æ³¨ï¼ˆç›®æ ‡ï¼š{target_count}æ¡ï¼‰",
                "annotation_count": len(selected_annotations),
                "chunks_processed": len(chunks),
                "chunk_densities": chunk_densities
            }

        selected_model, available_models = self._select_best_model(effective_model_id)
        model_note = f"æ¨¡å‹: {selected_model}"
        if selected_model != effective_model_id:
            model_note += f"ï¼ˆé¦–é€‰: {effective_model_id}ï¼Œå·²è‡ªåŠ¨é™çº§ï¼‰"
        model_table = self._format_model_table(available_models)

        print(f"[DocumentFeedback] ï¿½ æ–‡æ¡£è¾ƒå¤§({total_length}å­—ç¬¦)ï¼Œåˆ†{len(chunks)}æ®µå¤„ç†")
        print(f"[DocumentFeedback] ğŸ¯ ç›®æ ‡æ ‡æ³¨æ•°: çº¦{total_length//1000*10}æ¡ï¼ˆæ¯1000å­—10æ¡ï¼‰\n")

        # å¤„ç†æ¯ä¸€æ®µï¼ˆä¸¥æ ¼é¡ºåºæ‰§è¡Œï¼Œå¤±è´¥è‡ªåŠ¨æ‹†åˆ†é‡è¯•ï¼‰
        from collections import deque
        all_annotations = []
        seen_texts = set()
        processed = 0
        min_chunk_size = 800
        start_time = time.time()
        queue = deque(chunks)
        total_chunks_initial = len(chunks)

        while queue:
            chunk = queue.popleft()
            processed += 1
            current_total = processed + len(queue)

            elapsed = time.time() - start_time
            progress_pct = (processed / max(1, current_total)) * 100
            bar_length = 20
            bar_filled = int(bar_length * processed / max(1, current_total))
            progress_bar = 'â–ˆ' * bar_filled + 'â–‘' * (bar_length - bar_filled)

            print(f"\n[DocumentFeedback] ğŸ“Š [{progress_bar}] {processed}/{current_total} ({progress_pct:.0f}%)")
            print(f"[DocumentFeedback] â±ï¸ å·²ç”¨æ—¶: {elapsed:.1f}s | ç´¯è®¡{len(all_annotations)}æ¡æ ‡æ³¨ | å‰©ä½™{len(queue)}æ®µ")

            annotations = self._analyze_chunk_for_annotations(
                chunk=chunk,
                doc_type=doc_data.get("type"),
                user_requirement=user_requirement,
                model_id=selected_model,
                chunk_index=processed,
                total_chunks=current_total,
                full_doc_context=formatted_content,
                max_retries=2
            )

            if annotations is None:
                if len(chunk) <= min_chunk_size:
                    return {
                        "success": False,
                        "error": f"åˆ†æ®µå†…å®¹è¿‡å°ä»å¤±è´¥ï¼ˆ{len(chunk)}å­—ç¬¦ï¼‰ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–APIé…ç½®åé‡è¯•",
                        "file_path": file_path
                    }
                sub_chunks = self._split_into_chunks_by_paragraphs(chunk, max(min_chunk_size, len(chunk) // 2))
                if len(sub_chunks) <= 1:
                    return {
                        "success": False,
                        "error": f"åˆ†æ®µæ‹†åˆ†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†ï¼ˆ{len(chunk)}å­—ç¬¦ï¼‰",
                        "file_path": file_path
                    }
                for sc in reversed(sub_chunks):
                    queue.appendleft(sc)
                print(f"[DocumentFeedback] ğŸ” åˆ†æ®µå¤±è´¥ï¼Œå·²æ‹†åˆ†ä¸º{len(sub_chunks)}æ®µé‡è¯•")
                continue

            new_count = 0
            for item in annotations:
                text = (item.get("åŸæ–‡ç‰‡æ®µ") or "").strip()
                if text and text not in seen_texts:
                    seen_texts.add(text)
                    all_annotations.append(item)
                    new_count += 1

            msg = f"å·²å®Œæˆ {processed}/{current_total} æ®µ (æœ¬æ®µ+{new_count}æ¡ï¼Œç´¯è®¡{len(all_annotations)}æ¡)"
            print(f"[DocumentFeedback] âœ… ç¬¬ {processed} æ®µå®Œæˆ: æ–°å¢ {new_count} æ¡æ ‡æ³¨")
            if progress_callback:
                progress_callback(processed, current_total, msg)

        elapsed_total = time.time() - start_time

        print(f"\n[DocumentFeedback] ğŸ‰ åˆ†æ®µå¤„ç†å®Œæˆ")
        print(f"[DocumentFeedback] â±ï¸ æ€»è€—æ—¶: {elapsed_total:.1f}s")
        print(f"[DocumentFeedback] ğŸ“Š å…±ç”Ÿæˆ{len(all_annotations)}æ¡æ ‡æ³¨ï¼ˆç›®æ ‡: çº¦{total_length//1000*10}æ¡ï¼‰\n")
        
        return {
            "success": True,
            "file_path": file_path,
            "annotations": all_annotations,
            "summary": (
                f"åˆ†æ®µé¡ºåºå¤„ç†ï¼ˆåˆå§‹{total_chunks_initial}æ®µï¼‰ï¼Œå…±ç”Ÿæˆ{len(all_annotations)}æ¡æ ‡æ³¨ï¼ˆè€—æ—¶{elapsed_total:.1f}sï¼‰ã€‚"
                f"{model_note}\n\nå¯ç”¨æ¨¡å‹ï¼š\n{model_table}"
            ),
            "annotation_count": len(all_annotations),
            "chunks_processed": processed,
            "target_count": total_length // 1000 * 10
        }
    
    def analyze_for_annotation(
        self,
        file_path: str,
        user_requirement: str = "",
        model_id: str = "gemini-3-flash-preview"
    ) -> Dict[str, Any]:
        """
        åˆ†ææ–‡æ¡£ï¼Œç”Ÿæˆæ ‡æ³¨æ ¼å¼çš„å»ºè®®
        æ”¹è¿›ç‰ˆï¼šé€æ®µæ ‡æ³¨ï¼Œç¡®ä¿è¦†ç›–å…¨æ–‡
        """
        print(f"[DocumentFeedback] ğŸ“– è¯»å–æ–‡æ¡£: {os.path.basename(file_path)}")
        
        # ç¬¬1æ­¥ï¼šè¯»å–æ–‡æ¡£
        doc_data = self.reader.read_document(file_path)
        if not doc_data.get("success"):
            return {
                "success": False,
                "error": f"è¯»å–æ–‡æ¡£å¤±è´¥: {doc_data.get('error')}"
            }
        
        # ç¬¬2æ­¥ï¼šæ ¼å¼åŒ–å†…å®¹
        formatted_content = self.reader.format_for_ai(doc_data)
        
        # ç¬¬3æ­¥ï¼šæŒ‰æ®µè½åˆ‡åˆ†
        paragraphs = [p.strip() for p in formatted_content.split("\n\n") if p.strip()]
        print(f"[DocumentFeedback] ğŸ“ æ–‡æ¡£å…± {len(paragraphs)} æ®µï¼Œå°†é€æ®µåˆ†æ...")
        
        # å¦‚æœæ®µè½å¤ªå¤šï¼ˆè¶…è¿‡20æ®µï¼‰ï¼Œå»ºè®®ä½¿ç”¨æœ¬åœ°å…œåº•ï¼ˆæ›´å¿«ï¼‰
        if len(paragraphs) > 20:
            print(f"[DocumentFeedback] âš ï¸ æ–‡æ¡£æ®µè½è¾ƒå¤šï¼ˆ{len(paragraphs)}æ®µï¼‰ï¼Œä½¿ç”¨æœ¬åœ°å¿«é€Ÿæ ‡æ³¨æ¨¡å¼")
            print(f"[DocumentFeedback] ğŸ’¡ æç¤ºï¼šå¦‚éœ€æ›´è¯¦ç»†çš„ AI åˆ†æï¼Œè¯·ä½¿ç”¨è¾ƒå°çš„æ–‡æ¡£æˆ–åˆ†æ®µä¸Šä¼ ")
            
            all_annotations: List[Dict[str, str]] = []
            seen_texts = set()
            
            # ä½¿ç”¨æœ¬åœ°å…œåº•å¤„ç†æ‰€æœ‰æ®µè½
            for idx, para in enumerate(paragraphs):
                if para and len(para) > 20:
                    annotations = self._fallback_annotations_from_chunk(para)
                    for ann in annotations[:5]:  # æ¯æ®µæœ€å¤šå–5æ¡
                        text = (ann.get("åŸæ–‡ç‰‡æ®µ") or "").strip()
                        if text and text not in seen_texts:
                            seen_texts.add(text)
                            all_annotations.append(ann)
                
                # æ˜¾ç¤ºè¿›åº¦
                if (idx + 1) % 10 == 0 or idx == len(paragraphs) - 1:
                    print(f"[DocumentFeedback] ğŸ“Š å·²å¤„ç† {idx + 1}/{len(paragraphs)} æ®µï¼Œå·²æ”¶é›† {len(all_annotations)} æ¡æ ‡æ³¨")
            
            return {
                "success": True,
                "annotations": all_annotations[:150],  # é™åˆ¶åˆ°150æ¡
                "summary": f"ä½¿ç”¨æœ¬åœ°è§„åˆ™å¿«é€Ÿç”Ÿæˆ {len(all_annotations[:150])} æ¡æ ‡æ³¨å»ºè®®ï¼ˆå…±{len(paragraphs)}æ®µï¼‰"
            }
        
        # ç¬¬4æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ ‡æ³¨
        all_annotations: List[Dict[str, str]] = []
        seen_texts = set()
        
        # å¦‚æœæ²¡æœ‰AIå®¢æˆ·ç«¯ï¼Œç›´æ¥ç”¨æœ¬åœ°æ ‡æ³¨
        if not self.client:
            print(f"[DocumentFeedback] âš ï¸ æœªé…ç½®AIå®¢æˆ·ç«¯ï¼Œä½¿ç”¨æœ¬åœ°å…œåº•æ ‡æ³¨")
            for idx, para in enumerate(paragraphs):
                if para:
                    annotations = self._fallback_annotations_from_chunk(para)
                    for ann in annotations:
                        text = (ann.get("åŸæ–‡ç‰‡æ®µ") or "").strip()
                        if text and text not in seen_texts:
                            seen_texts.add(text)
                            all_annotations.append(ann)
            return {
                "success": True,
                "annotations": all_annotations[:100],  # é™åˆ¶åˆ°100æ¡
                "summary": f"ä½¿ç”¨æœ¬åœ°è§„åˆ™ç”Ÿæˆ{len(all_annotations)}æ¡æ ‡æ³¨å»ºè®®"
            }
        
        # ç¬¬5æ­¥ï¼šé€æ®µç”¨AIæ ‡æ³¨
        selected_model, available_models = self._select_best_model(model_id)
        
        for para_idx, paragraph in enumerate(paragraphs):
            if not paragraph or len(paragraph) < 20:
                continue  # è·³è¿‡å¤ªçŸ­çš„æ®µè½
            
            print(f"[DocumentFeedback] ğŸ”„ åˆ†æç¬¬ {para_idx + 1}/{len(paragraphs)} æ®µ...")
            
            # ä¸ºæ¯æ®µæ„å»ºPrompt
            para_prompt = self._build_annotation_prompt(
                doc_data.get("type"),
                paragraph,
                user_requirement + "\n\nè¯·ä¸ºè¿™ä¸€æ®µè¾“å‡º3-8æ¡æ”¹è¿›å»ºè®®ã€‚"
            )
            
            try:
                from google.genai import types
                
                response = self.client.models.generate_content(
                    model=selected_model,
                    contents=para_prompt,
                    config=types.GenerateContentConfig(
                        temperature=0.2,
                        max_output_tokens=2000,
                    )
                )
                
                # è§£ææ ‡æ³¨
                annotations = self._parse_annotation_response(response.text)
                for ann in annotations:
                    text = (ann.get("åŸæ–‡ç‰‡æ®µ") or "").strip()
                    if text and text not in seen_texts and len(text) <= 30:
                        seen_texts.add(text)
                        all_annotations.append(ann)
                
                # å¦‚æœè¿™æ®µæ²¡æœ‰æ ‡æ³¨åˆ°é—®é¢˜ï¼Œç”¨æœ¬åœ°å…œåº•
                if not annotations:
                    fallback = self._fallback_annotations_from_chunk(paragraph)
                    for ann in fallback[:3]:  # æ¯æ®µæœ€å¤šè¡¥å……3æ¡æœ¬åœ°æ ‡æ³¨
                        text = (ann.get("åŸæ–‡ç‰‡æ®µ") or "").strip()
                        if text and text not in seen_texts:
                            seen_texts.add(text)
                            all_annotations.append(ann)
                
            except Exception as e:
                print(f"[DocumentFeedback] âš ï¸ ç¬¬ {para_idx + 1} æ®µåˆ†æå¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°æ ‡æ³¨: {str(e)[:50]}")
                fallback = self._fallback_annotations_from_chunk(paragraph)
                for ann in fallback[:5]:  # æœ¬åœ°æ ‡æ³¨æœ€å¤šåŠ 5æ¡
                    text = (ann.get("åŸæ–‡ç‰‡æ®µ") or "").strip()
                    if text and text not in seen_texts:
                        seen_texts.add(text)
                        all_annotations.append(ann)
        
        # ç¬¬6æ­¥ï¼šè¿”å›ç»“æœ
        summary = f"å…±æ ‡æ³¨ {len(all_annotations)} å¤„éœ€è¦æ”¹è¿›"
        print(f"[DocumentFeedback] âœ… åˆ†æå®Œæˆï¼Œ{summary}")
        
        return {
            "success": True,
            "annotations": all_annotations[:150],  # é™åˆ¶åˆ°150æ¡
            "summary": summary
        }
    
    def annotate_document(
        self,
        file_path: str,
        annotations: List[Dict[str, str]]
    ) -> Dict[str, Any]:
        """
        åº”ç”¨æ ‡æ³¨åˆ°æ–‡æ¡£å‰¯æœ¬
        
        Args:
            file_path: åŸWordæ–‡æ¡£è·¯å¾„
            annotations: æ ‡æ³¨åˆ—è¡¨
                [
                    {"åŸæ–‡ç‰‡æ®µ": "éœ€è¦ä¿®æ”¹çš„æ–‡æœ¬", "ä¿®æ”¹å»ºè®®": "å»ºè®®ä¿®æ”¹ä¸º..."},
                    ...
                ]
        
        Returns:
            {
                "success": True,
                "original_file": "åŸæ–‡ä»¶è·¯å¾„",
                "revised_file": "æ ‡æ³¨åçš„æ–‡ä»¶è·¯å¾„",
                "applied": æˆåŠŸåº”ç”¨æ•°,
                "failed": å¤±è´¥æ•°
            }
        """
        print(f"[DocumentFeedback] âœï¸ åº”ç”¨æ ‡æ³¨...")
        
        result = self.annotator.annotate_document(file_path, annotations)
        
        if result.get("success"):
            print(f"[DocumentFeedback] âœ… æ ‡æ³¨å®Œæˆ: {os.path.basename(result.get('revised_file', ''))}")
        
        return result
    
    def full_annotation_loop_streaming(
        self,
        file_path: str,
        user_requirement: str = "",
        task_id: str = None,
        model_id: Optional[str] = None,
        cancel_check: Optional[Callable[[], bool]] = None
    ):
        """
        å®Œæ•´æ ‡æ³¨é—­ç¯ï¼ˆæµå¼ç‰ˆæœ¬ï¼Œæ”¯æŒè¿›åº¦åé¦ˆå’Œä»»åŠ¡å–æ¶ˆï¼‰
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            user_requirement: ç”¨æˆ·éœ€æ±‚
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºæ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆï¼‰
        
        Yields:
            {'stage': 'xxx', 'progress': 0-100, 'message': '...'}
        """
        from datetime import datetime
        import shutil
        from web.task_scheduler import check_task_cancelled
        effective_model_id = model_id or self.default_model_id

        def _is_cancelled() -> bool:
            try:
                if cancel_check and cancel_check():
                    return True
            except Exception:
                pass
            return bool(task_id and check_task_cancelled(task_id))
        
        print("=" * 60)
        print("ğŸ”„ å¯åŠ¨æ–‡æ¡£è‡ªåŠ¨æ ‡æ³¨ç³»ç»Ÿï¼ˆå®Œæ•´é—­ç¯-æµå¼ï¼‰")
        print("=" * 60)
        
        # ===== Stage 1: è¯»å–æ–‡æ¡£ =====
        if _is_cancelled():
            yield {
                'stage': 'cancelled',
                'progress': 0,
                'message': 'â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ',
                'detail': ''
            }
            return
        
        yield {
            'stage': 'reading',
            'progress': 5,
            'message': f'ğŸ“– æ­£åœ¨è¯»å–æ–‡æ¡£: {os.path.basename(file_path)}',
            'detail': 'è§£æWordæ–‡ä»¶ç»“æ„'
        }
        
        try:
            doc_data = self.reader.read_document(file_path)
            if not doc_data.get("success"):
                yield {
                    'stage': 'error',
                    'progress': 0,
                    'message': f'âŒ è¯»å–å¤±è´¥: {doc_data.get("error")}',
                    'detail': ''
                }
                return
            
            total_paras = len(doc_data.get("paragraphs", []))
            total_chars = sum(len(p.get("text", "")) for p in doc_data.get("paragraphs", []))
            
            yield {
                'stage': 'reading_complete',
                'progress': 10,
                'message': 'âœ… æ–‡æ¡£è¯»å–å®Œæˆ',
                'detail': f'{total_paras} æ®µï¼Œ{total_chars} å­—'
            }
        except Exception as e:
            yield {
                'stage': 'error',
                'progress': 0,
                'message': f'âŒ è¯»å–é”™è¯¯: {str(e)[:100]}',
                'detail': ''
            }
            return
        
        # ===== Stage 2: åˆ†æç”Ÿæˆæ ‡æ³¨å»ºè®® =====
        if _is_cancelled():
            yield {
                'stage': 'cancelled',
                'progress': 0,
                'message': 'â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ',
                'detail': 'åˆ†æå‰ä¸­æ–­'
            }
            return
        
        yield {
            'stage': 'analyzing',
            'progress': 15,
            'message': f'ğŸ¤– æ­£åœ¨åˆ†ææ–‡æ¡£...',
            'detail': f'ä½¿ç”¨ AI({effective_model_id}) æ£€æŸ¥ {total_paras} æ®µæ–‡æœ¬'
        }
        
        chunk_size = 4000 if (self.client and os.getenv("KOTO_DISABLE_AI") != "1") else 10000
        
        # ===== ä½¿ç”¨çº¿ç¨‹ + Queue å®ç°çœŸæ­£å®æ—¶è¿›åº¦æ¨é€ =====
        import queue as queue_module
        import threading
        
        progress_q = queue_module.Queue()
        result_holder = {"result": None, "error": None}
        last_yield_time = [time.time()]
        
        _SENTINEL = object()  # çº¿ç¨‹å®Œæˆçš„æ ‡è®°
        
        def on_analysis_progress(current, total, message):
            """è¿›åº¦å›è°ƒ â€” åœ¨åˆ†æçº¿ç¨‹ä¸­è°ƒç”¨ï¼Œé€šè¿‡ Queue å‘é€åˆ°ä¸»çº¿ç¨‹"""
            progress = 15 + int((current / total) * 35)
            current_time = time.time()
            if current_time - last_yield_time[0] >= 0.3:
                last_yield_time[0] = current_time
                # å°†è¯¦ç»†è¿›åº¦æ‹¼æ¥åˆ° message ä¸­ï¼Œç¡®ä¿å‰ç«¯çœ‹åˆ°
                detail_msg = message
                if "å·²å®Œæˆ" in message:
                     # ç®€åŒ–ä¸€ä¸‹ä»¥å…è¿‡é•¿
                     detail_msg = message.split('(')[0].strip() + "..."
                
                progress_q.put({
                    'stage': 'analyzing',
                    'progress': progress,
                    'message': f'ğŸ¤– {message}',  # ç›´æ¥æ˜¾ç¤ºå…·ä½“è¿›åº¦
                    'detail': message
                })
        
        def run_analysis():
            """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œåˆ†æ"""
            try:
                result_holder["result"] = self.analyze_for_annotation_chunked(
                    file_path,
                    user_requirement,
                    model_id=effective_model_id,
                    chunk_size=chunk_size,
                    progress_callback=on_analysis_progress
                )
            except Exception as e:
                result_holder["error"] = e
            finally:
                progress_q.put(_SENTINEL)
        
        analysis_thread = threading.Thread(target=run_analysis, daemon=True)
        analysis_thread.start()
        
        # ä¸»çº¿ç¨‹ï¼šå®æ—¶ä» Queue å–å‡ºè¿›åº¦äº‹ä»¶å¹¶ yieldï¼ˆSSE æ¨é€ç»™æµè§ˆå™¨ï¼‰
        heartbeat_interval = 3.0  # æ¯3ç§’å‘ä¸€æ¬¡å¿ƒè·³é˜²æ­¢ SSE è¶…æ—¶
        last_heartbeat = time.time()
        current_progress = [15]  # è·Ÿè¸ªå½“å‰è¿›åº¦ï¼Œé˜²æ­¢å¿ƒè·³æ•°å­—å›é€€
        
        try:
            while True:
                if _is_cancelled():
                    yield {
                        'stage': 'cancelled',
                        'progress': 0,
                        'message': 'â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ',
                        'detail': 'åˆ†æè¿‡ç¨‹ä¸­ä¸­æ–­'
                    }
                    return
                try:
                    evt = progress_q.get(timeout=1.0)
                    if evt is _SENTINEL:
                        break
                    current_progress[0] = max(current_progress[0], evt.get('progress', 15))
                    yield evt
                    last_heartbeat = time.time()
                except queue_module.Empty:
                    if not analysis_thread.is_alive():
                        break
                    # å‘é€å¿ƒè·³é˜²æ­¢æµè§ˆå™¨ SSE è¶…æ—¶æ–­è¿
                    now = time.time()
                    if now - last_heartbeat >= heartbeat_interval:
                        last_heartbeat = now
                        yield {
                            'stage': 'analyzing',
                            'progress': current_progress[0],
                            'message': 'ğŸ¤– æ­£åœ¨åˆ†ææ–‡æ¡£...',
                            'detail': 'ç­‰å¾… AI å“åº”ä¸­...'
                        }
            
            analysis_thread.join(timeout=10)
            
            # æ£€æŸ¥çº¿ç¨‹æ‰§è¡Œç»“æœ
            if result_holder["error"]:
                raise result_holder["error"]
            
            analysis_result = result_holder["result"]
            
            if not analysis_result or not analysis_result.get("success"):
                error_msg = (analysis_result or {}).get("error", "æœªçŸ¥é”™è¯¯")
                yield {
                    'stage': 'error',
                    'progress': 0,
                    'message': f'âŒ åˆ†æå¤±è´¥: {error_msg}',
                    'detail': ''
                }
                return
            
            annotations = analysis_result.get("annotations", [])
            
            yield {
                'stage': 'analysis_complete',
                'progress': 50,
                'message': f'âœ… åˆ†æå®Œæˆ',
                'detail': f'æ‰¾åˆ° {len(annotations)} å¤„ä¿®æ”¹'
            }
            
        except Exception as e:
            yield {
                'stage': 'error',
                'progress': 0,
                'message': f'âŒ åˆ†æé”™è¯¯: {str(e)[:100]}',
                'detail': ''
            }
            return
        
        if len(annotations) == 0:
            yield {
                'stage': 'complete',
                'progress': 100,
                'message': 'âœ… åˆ†æå®Œæˆï¼Œæœªæ‰¾åˆ°éœ€è¦ä¿®æ”¹çš„åœ°æ–¹',
                'detail': '',
                'result': {
                    'success': True,
                    'message': 'æœªæ‰¾åˆ°ä¿®æ”¹ç‚¹',
                    'original_file': file_path,
                    'applied': 0
                }
            }
            return
        
        # ===== Stage 3: åº”ç”¨æ ‡æ³¨åˆ°æ–‡æ¡£ =====
        if _is_cancelled():
            yield {
                'stage': 'cancelled',
                'progress': 0,
                'message': 'â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ',
                'detail': 'åº”ç”¨ä¿®æ”¹å‰ä¸­æ–­'
            }
            return
        
        yield {
            'stage': 'applying',
            'progress': 55,
            'message': 'ğŸ“ æ­£åœ¨åº”ç”¨ä¿®æ”¹åˆ°æ–‡æ¡£...',
            'detail': f'å°†ä½¿ç”¨ Track Changes æ ‡æ³¨ {len(annotations)} å¤„'
        }
        
        try:
            from web.track_changes_editor import TrackChangesEditor
            
            editor = TrackChangesEditor(author="Koto AI")
            
            # åˆ›å»ºå‰¯æœ¬
            base_name = os.path.splitext(file_path)[0]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            simple_revised = f"{base_name}_revised.docx"
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«å ç”¨
            try:
                if os.path.exists(simple_revised):
                    with open(simple_revised, 'a'):
                        pass
                    revised_file = simple_revised
                else:
                    revised_file = simple_revised
            except (PermissionError, IOError):
                revised_file = f"{base_name}_revised_{timestamp}.docx"
                print(f"[DocumentFeedback] âš ï¸ åŸä¿®æ”¹ç‰ˆè¢«å ç”¨ï¼Œåˆ›å»ºæ–°ç‰ˆæœ¬: {os.path.basename(revised_file)}")
            
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(file_path, revised_file)
            
            # å†æ¬¡æ£€æŸ¥å–æ¶ˆ
            if _is_cancelled():
                yield {
                    'stage': 'cancelled',
                    'progress': 0,
                    'message': 'â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ',
                    'detail': 'åº”ç”¨ä¿®æ”¹è¿‡ç¨‹ä¸­ä¸­æ–­'
                }
                return
            
            yield {
                'stage': 'applying',
                'progress': 60,
                'message': 'ğŸ“ æ­£åœ¨åº”ç”¨æ··åˆæ ‡æ³¨...',
                'detail': f'ç²¾ç¡®ä¿®æ”¹+æ–¹å‘å»ºè®® å…±{len(annotations)}é¡¹'
            }
            
            # åº”ç”¨æ··åˆæ ‡æ³¨ â€” è‡ªåŠ¨åŒºåˆ†ç²¾ç¡®ä¿®æ”¹å’Œæ–¹å‘å»ºè®®
            # âœï¸ çŸ­æ–‡æœ¬ï¼ˆ<=30å­—ï¼‰ä¸”æœ‰æ›¿æ¢æ–‡æœ¬ â†’ Track Changesï¼ˆä¿®è®¢æ ‡è®°ï¼‰
            # ğŸ’¬ é•¿æ–‡æœ¬ï¼ˆ>30å­—ï¼‰æˆ–åªæœ‰å»ºè®® â†’ Commentï¼ˆæ‰¹æ³¨æ°”æ³¡ï¼‰
            apply_q = queue_module.Queue()
            apply_result_holder = {"result": None, "error": None}
            
            def on_apply_progress(current, total, status, detail):
                pct = 60 + int((current / total) * 25) if total > 0 else 60
                apply_q.put({
                    'stage': 'applying',
                    'progress': pct,
                    'message': f'ğŸ“ {status}...',
                    'detail': detail
                })
            
            def run_apply():
                try:
                    apply_result_holder["result"] = editor.apply_hybrid_changes(
                        revised_file,
                        annotations,
                        progress_callback=on_apply_progress
                    )
                except Exception as e:
                    apply_result_holder["error"] = e
                finally:
                    apply_q.put(_SENTINEL)
            
            apply_thread = threading.Thread(target=run_apply, daemon=True)
            apply_thread.start()
            
            while True:
                if _is_cancelled():
                    yield {
                        'stage': 'cancelled',
                        'progress': 0,
                        'message': 'â¸ï¸ ä»»åŠ¡å·²è¢«å–æ¶ˆ',
                        'detail': 'åº”ç”¨ä¿®æ”¹è¿‡ç¨‹ä¸­ä¸­æ–­'
                    }
                    return
                try:
                    evt = apply_q.get(timeout=1.0)
                    if evt is _SENTINEL:
                        break
                    yield evt
                except queue_module.Empty:
                    if not apply_thread.is_alive():
                        break
            
            apply_thread.join(timeout=10)
            
            if apply_result_holder["error"]:
                raise apply_result_holder["error"]
            
            edit_result = apply_result_holder["result"]
            
            applied = edit_result.get("applied", 0)
            failed = edit_result.get("failed", 0)
            
            yield {
                'stage': 'applying_complete',
                'progress': 85,
                'message': f'âœ… ä¿®æ”¹åº”ç”¨å®Œæˆ',
                'detail': f'æˆåŠŸ: {applied}, å¤±è´¥: {failed}'
            }
            
        except Exception as e:
            import traceback
            yield {
                'stage': 'error',
                'progress': 0,
                'message': f'âŒ åº”ç”¨é”™è¯¯: {str(e)[:100]}',
                'detail': traceback.format_exc()[:200]
            }
            return
        
        # ===== Stage 4: å®Œæˆ =====
        
        # æ·»åŠ åˆ°æ–‡ä»¶ç½‘ç»œç´¢å¼•
        try:
            from web.processed_file_network import get_file_network
            file_network = get_file_network()
            file_network.record_processing(
                file_path=file_path,
                operation="annotate",
                changes_count=applied,
                output_file=revised_file,
                status="success" if applied > 0 else "partial",
                details={
                    "requirement": user_requirement,
                    "total_annotations": len(annotations),
                    "applied": applied,
                    "failed": failed
                }
            )
        except Exception as e:
            print(f"[DocumentFeedback] æ–‡ä»¶ç½‘ç»œç´¢å¼•è®°å½•å¤±è´¥: {e}")
        
        yield {
            'stage': 'complete',
            'progress': 100,
            'message': 'âœ… æ–‡æ¡£ä¿®æ”¹å®Œæˆï¼',
            'detail': f'ä¿®æ”¹ä½ç½®: {applied}ï¼Œå®šä½å¤±è´¥: {failed}',
            'result': {
                'success': edit_result.get("success", False),
                'original_file': file_path,
                'revised_file': revised_file,
                'applied': applied,
                'failed': failed,
                'total': len(annotations),
                'analysis_summary': analysis_result.get("summary")
            }
        }

    def full_annotation_loop(
        self,
        file_path: str,
        user_requirement: str = "",
        model_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        å®Œæ•´æ ‡æ³¨é—­ç¯ï¼šè¯»å– -> åˆ†æ -> å®šä½ -> æ³¨å…¥
        
        Args:
            file_path: Wordæ–‡æ¡£è·¯å¾„
            user_requirement: ç”¨æˆ·éœ€æ±‚
        
        Returns:
            {
                "success": True,
                "original_file": "...",
                "revised_file": "...",
                "applied": 5,
                "failed": 1
            }
        """
        print("=" * 60)
        print("ğŸ”„ å¯åŠ¨æ–‡æ¡£è‡ªåŠ¨æ ‡æ³¨ç³»ç»Ÿï¼ˆå®Œæ•´é—­ç¯ï¼‰")
        print("=" * 60)
        
        effective_model_id = model_id or self.default_model_id

        # ç¬¬1æ­¥ï¼šåˆ†æç”Ÿæˆæ ‡æ³¨å»ºè®®ï¼ˆä½¿ç”¨åˆ†æ®µæ–¹æ³•å¤„ç†å¤§æ–‡æ¡£ï¼‰
        chunk_size = 4000 if (self.client and os.getenv("KOTO_DISABLE_AI") != "1") else 10000
        analysis_result = self.analyze_for_annotation_chunked(
            file_path,
            user_requirement,
            model_id=effective_model_id,
            chunk_size=chunk_size
        )
        
        if not analysis_result.get("success"):
            return analysis_result
        
        annotations = analysis_result.get("annotations", [])
        print(f"\nğŸ“Š åˆ†æç»“æœ: ç”Ÿæˆ {len(annotations)} ä¸ªæ ‡æ³¨å»ºè®®")
        
        if len(annotations) == 0:
            return {
                "success": True,
                "message": "AIåˆ†æå®Œæˆï¼Œä½†æœªæ‰¾åˆ°éœ€è¦ä¿®æ”¹çš„åœ°æ–¹",
                "original_file": file_path
            }
        
        # ç¬¬2æ­¥ï¼šåº”ç”¨æ ‡æ³¨åˆ°æ–‡æ¡£ï¼ˆä½¿ç”¨Track Changesä¿®è®¢æ¨¡å¼ï¼‰
        print(f"\n[DocumentFeedback] ï¿½ ä»¥å³ä¾§æ‰¹æ³¨æ°”æ³¡æ·»åŠ ä¿®æ”¹å»ºè®®...")
        
        # ä½¿ç”¨æ‰¹æ³¨æ–¹å¼ â€” åŸæ–‡ä¸å˜ï¼Œä¿®æ”¹å»ºè®®ä»¥å³ä¾§æ°”æ³¡æ˜¾ç¤º
        from web.track_changes_editor import TrackChangesEditor
        import shutil
        from datetime import datetime
        
        editor = TrackChangesEditor(author="Koto AI")
        
        # åˆ›å»ºå‰¯æœ¬ï¼ˆå¸¦æ—¶é—´æˆ³é¿å…å†²çªï¼‰
        base_name = os.path.splitext(file_path)[0]
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        simple_revised = f"{base_name}_revised.docx"
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«å ç”¨
        try:
            if os.path.exists(simple_revised):
                # å°è¯•æ‰“å¼€æ£€æµ‹æ˜¯å¦è¢«å ç”¨
                with open(simple_revised, 'a'):
                    pass
                revised_file = simple_revised
            else:
                revised_file = simple_revised
        except (PermissionError, IOError):
            # æ–‡ä»¶è¢«å ç”¨ï¼Œä½¿ç”¨æ—¶é—´æˆ³ç‰ˆæœ¬
            revised_file = f"{base_name}_revised_{timestamp}.docx"
            print(f"[DocumentFeedback] âš ï¸ åŸä¿®æ”¹ç‰ˆè¢«å ç”¨ï¼Œåˆ›å»ºæ–°ç‰ˆæœ¬: {os.path.basename(revised_file)}")
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(file_path, revised_file)
        
        # åº”ç”¨æ··åˆæ ‡æ³¨ï¼ˆç²¾ç¡®ä¿®æ”¹+æ–¹å‘å»ºè®®ï¼‰
        edit_result = editor.apply_hybrid_changes(revised_file, annotations)
        
        # æ·»åŠ åˆ°æ–‡ä»¶ç½‘ç»œç´¢å¼•
        try:
            from web.processed_file_network import get_file_network
            file_network = get_file_network()
            file_network.record_processing(
                file_path=file_path,
                operation="annotate",
                changes_count=edit_result.get("applied", 0),
                output_file=revised_file,
                status="success" if edit_result.get("applied", 0) > 0 else "partial",
                details={
                    "requirement": user_requirement,
                    "total_annotations": len(annotations),
                    "applied": edit_result.get("applied"),
                    "failed": edit_result.get("failed")
                }
            )
        except Exception as e:
            print(f"[DocumentFeedback] æ–‡ä»¶ç½‘ç»œç´¢å¼•è®°å½•å¤±è´¥: {e}")
        
        return {
            "success": edit_result.get("success", False),
            "original_file": file_path,
            "revised_file": revised_file,
            "applied": edit_result.get("applied"),
            "failed": edit_result.get("failed"),
            "total": edit_result.get("total"),
            "analysis_summary": analysis_result.get("summary")
        }
    
    def _build_annotation_prompt(
        self,
        doc_type: str,
        formatted_content: str,
        user_requirement: str,
        full_doc_context: str = ""
    ) -> str:
        """æ„å»ºç”¨äºæ ‡æ³¨çš„AI Prompt - ç²¾å‡†ä¿®æ”¹ç‰ˆï¼Œç”Ÿæˆå¯ç›´æ¥æ›¿æ¢çš„æ–‡æœ¬ä¿®è®¢"""
        
        # ç»Ÿè®¡æ®µè½æ•°ï¼Œç”¨äºæŒ‡å¯¼ AI å‡åŒ€åˆ†å¸ƒ
        paragraphs = [p.strip() for p in formatted_content.split("\n\n") if p.strip()]
        para_count = len(paragraphs)
        
        # å¦‚æœæä¾›äº†å…¨æ–‡èƒŒæ™¯ï¼Œé™åˆ¶é•¿åº¦ä»¥å…è¶…é™(ä¿ç•™å¼€å¤´ç»“å°¾å’Œç›®å½•å¤§çº²ä¿¡æ¯)
        global_ctx_prompt = ""
        if full_doc_context and len(full_doc_context) > len(formatted_content) * 1.5:
            # åªæœ‰å½“èƒŒæ™¯æ˜æ˜¾é•¿äºå½“å‰ç‰‡æ®µæ—¶æ‰åŒ…å«èƒŒæ™¯ï¼Œé¿å…ç¬¬ä¸€æ®µè‡ªæˆ‘é‡å¤å¹²æ‰°
            ctx_len = len(full_doc_context)
            if ctx_len > 30000:
                # æˆªå–å¼€å¤´3000å­—å’Œç»“å°¾2000å­—ä½œä¸ºèƒŒæ™¯
                global_ctx_prompt = f"""
## å…¨æ–‡èƒŒæ™¯å‚è€ƒï¼ˆèŠ‚é€‰ï¼‰
...ï¼ˆå‰æ–‡å¿½ç•¥ï¼‰
{full_doc_context[:3000]}
...
{full_doc_context[-2000:]}
"""
            else:
                global_ctx_prompt = f"""
## å…¨æ–‡å®Œæ•´èƒŒæ™¯ï¼ˆä¾›è¿è´¯æ€§åˆ†æå‚è€ƒï¼‰
{full_doc_context}
"""

        base_prompt = f"""ä½ æ˜¯ä¸€åèµ„æ·±å­¦æœ¯ç¼–è¾‘ã€‚è¯·é€æ®µå®¡é˜…æ­¤{doc_type.upper()}æ–‡æ¡£ç‰‡æ®µï¼ˆå…±{para_count}æ®µï¼‰ï¼ŒåŸºäºå…¨æ–‡èƒŒæ™¯è¿›è¡Œ**ç›´æ¥ä¿®æ”¹**ã€‚

{global_ctx_prompt}

## å½“å‰å¾…å®¡é˜…æ–‡æ¡£ç‰‡æ®µï¼ˆå…±{para_count}æ®µï¼‰
{formatted_content}

## ä»»åŠ¡è¦æ±‚
{user_requirement if user_requirement else "å¯¹æ–‡æ¡£è¿›è¡Œå­¦æœ¯æ¶¦è‰²ï¼Œæå‡è¡¨è¾¾çš„ä¸“ä¸šæ€§ã€å‡†ç¡®æ€§å’Œè¿è´¯æ€§"}

## ğŸš« é‡è¦æŒ‡ä»¤ï¼š
1. **å°‘åºŸè¯ï¼Œå¤šå¹²æ´»**ï¼šä¸è¦ç»™å‡º"å»ºè®®ä¿®æ”¹..."çš„ç©ºæ´æ‰¹æ³¨ï¼Œè€Œæ˜¯**ç›´æ¥æä¾›ä¿®æ”¹åçš„æ–‡æœ¬**ã€‚
2. **ç²¾å‡†å®šä½**ï¼šåŸæ–‡ç‰‡æ®µå¿…é¡»ä¸æ–‡æ¡£ä¸­çš„æ–‡æœ¬å®Œå…¨ä¸€è‡´ï¼Œä¸è¦çœç•¥æˆ–ä¿®æ”¹åŸæ–‡ã€‚
3. **é€‚åº¦ä¿®æ”¹**ï¼šåªä¿®æ”¹çœŸæ­£æœ‰è¯­ç—…ã€ç¿»è¯‘è…”ã€é€»è¾‘ä¸é€šé¡ºæˆ–ç”Ÿç¡¬çš„åœ°æ–¹ã€‚ä¸è¦ä¸ºäº†ä¿®æ”¹è€Œä¿®æ”¹ã€‚ä¿æŒåŸæ„ä¸å˜ã€‚

## âš ï¸ å»AIå‘³ â€” å¿…é¡»ä¸¥æ ¼éµå®ˆçš„è¯­è¨€é£æ ¼ï¼š
ä½ æ”¹å†™åçš„æ–‡æœ¬**ç»å¯¹ä¸èƒ½æœ‰AIå‘³**ã€‚ä»¥ä¸‹æ˜¯å…·ä½“ç¦ä»¤ï¼š
- **ç¦ç”¨ç ´æŠ˜å·**ï¼ˆâ€”â€”ï¼‰æ¥åšè§£é‡Šæˆ–æ’å…¥è¯­ï¼Œæ”¹ç”¨é€—å·ã€æ‹¬å·æˆ–æ‹†æˆä¸¤å¥ã€‚
- **ç¦ç”¨å¼•å·å¼ºè°ƒ**ï¼šä¸è¦ç”¨"XXX"æ¥å¼ºè°ƒæ¦‚å¿µï¼Œç›´æ¥å†™å‡ºæ¥ã€‚
- **ç¦ç”¨æ’æ¯”å †ç Œ**ï¼šä¸è¦æŠŠä¸‰ä¸ªä»¥ä¸Šå¹¶åˆ—çŸ­è¯­æ’åœ¨ä¸€èµ·ï¼Œåƒ"æå‡äº†æ•ˆç‡ã€ä¼˜åŒ–äº†æµç¨‹ã€å¢å¼ºäº†ä½“éªŒ"è¿™ç§è¦ç æ‰ã€‚
- **ç¦ç”¨ä»¥ä¸‹AIé«˜é¢‘å¥—è¯**ï¼šå€¼å¾—æ³¨æ„çš„æ˜¯ã€éœ€è¦æŒ‡å‡ºçš„æ˜¯ã€æ€»è€Œè¨€ä¹‹ã€ç»¼ä¸Šæ‰€è¿°ã€ä¸ä»…...è€Œä¸”...ã€ä¸€æ–¹é¢...å¦ä¸€æ–¹é¢...ã€ä»...è§’åº¦æ¥çœ‹ã€åœ¨...èƒŒæ™¯ä¸‹ã€å…·æœ‰é‡è¦æ„ä¹‰ã€å‘æŒ¥ç€å…³é”®ä½œç”¨ã€æä¾›äº†æœ‰åŠ›æ”¯æ’‘ã€‚
- **å°‘ç”¨"è¿›è¡Œ""å®ç°""å¼€å±•"ç­‰ä¸‡èƒ½åŠ¨è¯**ï¼Œæ¢æˆå…·ä½“çš„åŠ¨ä½œã€‚
- **è¯­è¨€è¦æœ´å®è‡ªç„¶**ï¼Œåƒä¸€ä¸ªæœ‰ç»éªŒçš„äººå†™çš„ï¼Œä¸åƒAIç”Ÿæˆçš„ã€‚è¯»èµ·æ¥åº”è¯¥åƒæ­£å¸¸äººè¯´çš„è¯ï¼Œè€Œä¸æ˜¯æœºå™¨æ‹¼å‡ºæ¥çš„ã€‚
- **å¥å­è¦çŸ­**ï¼Œä¸€å¥è¯è¯´ä¸€ä»¶äº‹ã€‚ä¸è¦æŠŠå¤šä¸ªä¿¡æ¯å¡è¿›ä¸€ä¸ªé•¿ä»å¥é‡Œã€‚

### ç¤ºä¾‹ï¼ˆé”™è¯¯ vs æ­£ç¡®ï¼‰ï¼š
- âŒ "è¯¥æ–¹æ³•åœ¨æå‡æ•ˆç‡ã€ä¼˜åŒ–æµç¨‹ã€å¢å¼ºä½“éªŒç­‰æ–¹é¢å…·æœ‰é‡è¦æ„ä¹‰"
- âœ… "è¯¥æ–¹æ³•èƒ½æœ‰æ•ˆæå‡å·¥ä½œæ•ˆç‡"
- âŒ "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸€å‘ç°ä¸ºåç»­ç ”ç©¶æä¾›äº†æœ‰åŠ›æ”¯æ’‘"
- âœ… "è¿™ä¸€å‘ç°å¯¹åç»­ç ”ç©¶æœ‰å‚è€ƒä»·å€¼"
- âŒ "é€šè¿‡å¯¹æ•°æ®çš„æ·±å…¥åˆ†æâ€”â€”åŒ…æ‹¬ç»Ÿè®¡æ£€éªŒå’Œå›å½’å»ºæ¨¡â€”â€”æˆ‘ä»¬å‘ç°..."
- âœ… "ç»Ÿè®¡æ£€éªŒå’Œå›å½’å»ºæ¨¡çš„åˆ†æç»“æœè¡¨æ˜..."

## æ ‡æ³¨ç±»å‹

### ç±»å‹Aï¼šæœ¯è¯­ä¸ç”¨è¯ä¿®æ­£ï¼ˆç›´æ¥æ›¿æ¢ï¼‰
çº æ­£ä¸åœ°é“çš„è¡¨è¾¾ã€å£è¯­åŒ–è¯æ±‡ã€ç¿»è¯‘è…”ã€‚
*   åŸæ–‡="è¢«å¹¿æ³›åœ°è¿›è¡Œä½¿ç”¨" -> æ”¹ä¸º="å·²å¹¿æ³›ä½¿ç”¨"
*   åŸæ–‡="åœ¨è¿™ä¸ªå›¾åƒä¸­" -> æ”¹ä¸º="è¯¥å›¾åƒ"

### ç±»å‹Bï¼šå¥å­é‡å†™ä¸æ¶¦è‰²ï¼ˆç›´æ¥æ›¿æ¢ï¼‰
é‡åˆ°é•¿éš¾å¥ã€é€»è¾‘ä¸é€šé¡ºçš„å¥å­ï¼Œ**ç›´æ¥é‡å†™**æ•´å¥ã€‚ç”¨çŸ­å¥ï¼Œå°‘ç”¨ä»å¥ã€‚
*   åŸæ–‡="[é€‰ä¸­æ‹—å£çš„æ•´å¥]" -> æ”¹ä¸º="[é‡å†™åçš„ç®€æ´å¥å­]"
*   âš ï¸ **æ³¨æ„**ï¼šé€‰ä¸­æ–‡æœ¬ä¸è¦è¶…è¿‡ä¸€æ®µï¼Œæœ€å¥½ä»¥å¥ä¸ºå•ä½ã€‚

### ç±»å‹Cï¼šç»“æ„æ€§æ‰¹æ³¨ï¼ˆä»…é™å¿…è¦æ—¶ï¼‰
åªæœ‰å½“ç¡®å®é€šè¿‡ä¿®æ”¹æ— æ³•è§£å†³ï¼ˆå¦‚å®Œå…¨è·‘é¢˜ã€æ®µè½ç¼ºå¤±ï¼‰æ—¶ï¼Œæ‰ä½¿ç”¨å»ºè®®ã€‚
*   æ”¹ä¸º="å»ºè®®ï¼š..."

## è¾“å‡ºæ ¼å¼
åªè¿”å›JSONæ•°ç»„ï¼Œç¦æ­¢å…¶ä»–æ–‡å­—ï¼š
[
  {{"åŸæ–‡": "è¢«å¹¿æ³›åœ°è¿›è¡Œä½¿ç”¨", "æ”¹ä¸º": "å·²å¹¿æ³›ä½¿ç”¨", "åŸå› ": "ç²¾ç®€"}},
  {{"åŸæ–‡": "åœ¨å½“å‰æ•°å­—è‰ºæœ¯å‘å±•çš„ä¸»æµå®è·µä¸­ï¼Œç ”ç©¶è€…ä»¬ä¸»è¦èšç„¦äº...ï¼ˆåŸé•¿å¥ï¼‰", "æ”¹ä¸º": "ç›®å‰æ•°å­—è‰ºæœ¯ç ”ç©¶ä¸»è¦é›†ä¸­åœ¨...ï¼ˆé‡å†™åï¼‰", "åŸå› ": "å»å†—ä½™"}},
  {{"åŸæ–‡": "è¿™ä¸ºåç»­ç ”ç©¶æä¾›äº†æœ‰åŠ›æ”¯æ’‘", "æ”¹ä¸º": "è¿™å¯¹åç»­ç ”ç©¶æœ‰å‚è€ƒä»·å€¼", "åŸå› ": "å»å¥—è¯"}}
]
"""
        return base_prompt
    
    def _parse_annotation_response(self, ai_response: str) -> List[Dict[str, str]]:
        """è§£æAIå“åº”ä¸ºæ ‡æ³¨æ ¼å¼"""
        import re
        import json
        
        try:
            # å°è¯•æå–JSONæ•°ç»„
            json_match = re.search(r'```json\s*(\[.*?\])\s*```', ai_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥æ‰¾æ•°ç»„
                json_match = re.search(r'\[.*\]', ai_response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    json_str = ai_response
            
            data = json.loads(json_str)
            
            # éªŒè¯æ ¼å¼
            if isinstance(data, list):
                valid_annotations = []
                for item in data:
                    if isinstance(item, dict):
                        # æå–åŸæ–‡
                        original = item.get("åŸæ–‡ç‰‡æ®µ") or item.get("åŸæ–‡") or item.get("original")
                        # æå–ä¿®æ”¹å»ºè®®
                        modified = item.get("ä¿®æ”¹å»ºè®®") or item.get("æ”¹ä¸º") or item.get("ä¿®æ”¹åæ–‡æœ¬") or item.get("modified")
                        # æå–åŸå› 
                        reason = item.get("ä¿®æ”¹åŸå› ") or item.get("åŸå› ") or item.get("reason")
                        
                        if original and modified:
                            entry = {
                                "åŸæ–‡ç‰‡æ®µ": str(original).strip(),
                                "ä¿®æ”¹å»ºè®®": str(modified).strip()
                            }
                            if reason:
                                entry["ä¿®æ”¹åŸå› "] = str(reason).strip()
                            valid_annotations.append(entry)
                return valid_annotations
            
            # å¦‚æœæ˜¯å¯¹è±¡åŒ…è£¹æ ¼å¼ï¼Œå°è¯•æå– annotations/modifications/suggestions
            if isinstance(data, dict):
                for key in ["annotations", "modifications", "suggestions"]:
                    if key in data and isinstance(data[key], list):
                        valid_annotations = []
                        for item in data[key]:
                            if isinstance(item, dict):
                                # åŒæ ·æ”¯æŒå¤šç§æ ¼å¼
                                original = item.get("åŸæ–‡ç‰‡æ®µ") or item.get("åŸæ–‡") or item.get("original")
                                modified = item.get("ä¿®æ”¹å»ºè®®") or item.get("æ”¹ä¸º") or item.get("ä¿®æ”¹åæ–‡æœ¬") or item.get("modified")
                                reason = item.get("ä¿®æ”¹åŸå› ") or item.get("åŸå› ") or item.get("reason")
                                
                                if original and modified:
                                    entry = {
                                        "åŸæ–‡ç‰‡æ®µ": str(original).strip(),
                                        "ä¿®æ”¹å»ºè®®": str(modified).strip()
                                    }
                                    if reason:
                                        entry["ä¿®æ”¹åŸå› "] = str(reason).strip()
                                    valid_annotations.append(entry)
                        if valid_annotations:
                            return valid_annotations
            
            return []
        
        except Exception as e:
            print(f"[DocumentFeedback] è§£ææ ‡æ³¨å¤±è´¥: {e}")
            return []


if __name__ == "__main__":
    print("æ–‡æ¡£æ™ºèƒ½åé¦ˆç³»ç»Ÿå‡†å¤‡å°±ç»ª")
