import time
import socket
import json
import requests

class LocalModelRouter:
    """
    ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹è¿›è¡Œä»»åŠ¡åˆ†ç±»ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
    
    - å¦‚æœå®‰è£…äº† Ollama + Qwenï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆæ›´å¿«ã€æ›´å‡†ï¼‰
    - å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œè‡ªåŠ¨é™çº§åˆ° SmartDispatcherï¼ˆçº¯è§„åˆ™+è¯­æ–™åŒ¹é…ï¼‰
    - å¯¹ç”¨æˆ·é€æ˜ï¼Œä¸å½±å“æ­£å¸¸ä½¿ç”¨
    """
    
    _initialized = False
    _model_name = None
    _available = None  # ç¼“å­˜å¯ç”¨æ€§çŠ¶æ€
    _check_time = 0    # ä¸Šæ¬¡æ£€æŸ¥æ—¶é—´
    
    # æ¨èçš„å¿«é€Ÿæ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    OLLAMA_MODELS = [
        "qwen3:8b",          # â˜… æœ€ä½³ä¸­è‹±æ–‡èƒ½åŠ›ï¼ŒRTX 4090 æµç•…è¿è¡Œ
        "qwen3:4b",          # å¿«é€Ÿå¤‡é€‰
        "qwen3:1.7b",        # è½»é‡å¤‡é€‰
        "qwen2.5:7b",        # æ—§ç‰ˆä½†è´¨é‡å¥½
        "qwen2.5:3b",        # æ—§ç‰ˆå¿«é€Ÿ
        "qwen2.5:1.5b",      # æ—§ç‰ˆè½»é‡
        "llama3.2:3b",       # è‹±æ–‡ä¸ºä¸»
    ]
    
    # åˆ†ç±» Promptï¼ˆå›ºå®š JSON æ ¼å¼ï¼Œç¡®ä¿è¾“å‡ºä¸€è‡´ï¼‰
    # Qwen3 æ”¯æŒ /no_think æ¨¡å¼ï¼Œè·³è¿‡æ€è€ƒç›´æ¥è¾“å‡ºï¼ŒåŠ é€Ÿåˆ†ç±»
    CLASSIFY_PROMPT = '''/no_think
ä½ æ˜¯ä»»åŠ¡åˆ†ç±»å™¨ã€‚åªè¾“å‡º JSONï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚

ä»»åŠ¡ç±»å‹:
- PAINTER: ç”¨æˆ·è¦æ±‚ç”Ÿæˆ/ç»˜åˆ¶/åˆ›ä½œä¸€å¼ å›¾ç‰‡ã€ç…§ç‰‡ã€å£çº¸ã€å¤´åƒ
- FILE_GEN: ç”¨æˆ·è¦æ±‚ç”Ÿæˆ Word/PDF/Excel/PPT ç­‰æ–‡ä»¶æ–‡æ¡£
- DOC_ANNOTATE: ç”¨æˆ·è¦æ±‚å¯¹å·²æœ‰æ–‡æ¡£è¿›è¡Œæ ‡æ³¨/æ‰¹æ³¨/æ¶¦è‰²/æ ¡å¯¹
- RESEARCH: ç”¨æˆ·è¦æ±‚æ·±åº¦åˆ†ææŸä¸ªä¸»é¢˜ï¼ˆéœ€è¦é•¿ç¯‡ã€ç³»ç»Ÿæ€§å›ç­”ï¼‰
- CODER: ç”¨æˆ·è¦æ±‚å†™ä»£ç /ç¼–ç¨‹/è°ƒè¯•/å®ç°åŠŸèƒ½
- SYSTEM: ç”¨æˆ·ä¸‹å‘½ä»¤è®©ä½ æ‰§è¡Œç³»ç»Ÿæ“ä½œï¼ˆå¦‚"æ‰“å¼€å¾®ä¿¡""å…³æœº"ï¼‰
- AGENT: ç”¨æˆ·è¦æ±‚æ‰§è¡Œå·¥å…·æ“ä½œï¼ˆå‘å¾®ä¿¡æ¶ˆæ¯ã€è®¾æé†’ã€æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼‰
- WEB_SEARCH: ç”¨æˆ·è¯¢é—®éœ€è¦å®æ—¶ä¿¡æ¯çš„é—®é¢˜ï¼ˆå¤©æ°”ã€è‚¡ä»·ã€æ–°é—»ã€æ¯”èµ›ç»“æœï¼‰
- CHAT: æ—¥å¸¸å¯¹è¯ã€çŸ¥è¯†é—®ç­”ã€é—²èŠã€å»ºè®®ã€æ¦‚å¿µè§£é‡Š

æ ¸å¿ƒåˆ¤æ–­è§„åˆ™:
1. ç”¨æˆ·åœ¨**é—®é—®é¢˜/æ±‚çŸ¥è¯†** â†’ CHATï¼ˆå³ä½¿æåˆ°"å¯åŠ¨""æ‰“å¼€"ç­‰è¯ï¼‰
2. ç”¨æˆ·åœ¨**ä¸‹å‘½ä»¤/è¦æ±‚æ‰§è¡Œ** â†’ å¯¹åº”æ“ä½œç±»å‹
3. "æ€ä¹ˆ/å¦‚ä½•/ä»€ä¹ˆåŠæ³•/ä¸ºä»€ä¹ˆ/æ˜¯ä»€ä¹ˆ" = æé—® â†’ é€šå¸¸æ˜¯ CHAT
4. çŸ­å¥+å‘½ä»¤è¯­æ°”+å…·ä½“åº”ç”¨å = ç³»ç»Ÿæ“ä½œ â†’ SYSTEM
5. æåˆ°"å†™ä»£ç /è„šæœ¬/å‡½æ•°/python" = ç¼–ç¨‹ â†’ CODER
6. æåˆ°å¤©æ°”/è‚¡ä»·/æ–°é—»+éœ€è¦å®æ—¶æ•°æ® â†’ WEB_SEARCH
7. åªæåˆ°"ç ”ç©¶"ä½†æ²¡æœ‰æ·±åº¦è¦æ±‚ â†’ CHATï¼ˆ"ç ”ç©¶"æ—¥å¸¸ç”¨æ³•=äº†è§£ï¼‰

æ­£ä¾‹:
è¾“å…¥: æ‰“å¼€å¾®ä¿¡
è¾“å‡º: {{"task":"SYSTEM","confidence":0.95}}
è¾“å…¥: ç”»ä¸€åªçŒ«
è¾“å‡º: {{"task":"PAINTER","confidence":0.92}}
è¾“å…¥: å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°
è¾“å‡º: {{"task":"CODER","confidence":0.93}}
è¾“å…¥: æŸ¥ä¸‹æ˜å¤©åŒ—äº¬å¤©æ°”
è¾“å‡º: {{"task":"WEB_SEARCH","confidence":0.90}}
è¾“å…¥: å¸®æˆ‘åšä¸€ä¸ªPPT
è¾“å‡º: {{"task":"FILE_GEN","confidence":0.88}}
è¾“å…¥: æ ‡æ³¨è¿™ç¯‡æ–‡æ¡£çš„ä¸å½“ä¹‹å¤„
è¾“å‡º: {{"task":"DOC_ANNOTATE","confidence":0.88}}
è¾“å…¥: ç»™å¼ ä¸‰å‘å¾®ä¿¡è¯´æ˜å¤©å¼€ä¼š
è¾“å‡º: {{"task":"AGENT","confidence":0.90}}

åä¾‹ï¼ˆå®¹æ˜“è¯¯åˆ¤ï¼Œæ³¨æ„åŒºåˆ†ï¼‰:
è¾“å…¥: åœ¨Windowsç¯å¢ƒé‡Œå¿«é€Ÿå¯åŠ¨bashè™šæ‹Ÿç¯å¢ƒï¼Œä¸€èˆ¬ç”¨ä»€ä¹ˆåŠæ³•
è¾“å‡º: {{"task":"CHAT","confidence":0.92}}
ï¼ˆåˆ†æï¼šè™½å«"å¯åŠ¨"ä½†è¿™æ˜¯çŸ¥è¯†æé—®ï¼Œä¸æ˜¯è®©ä½ æ‰§è¡Œå¯åŠ¨æ“ä½œï¼‰
è¾“å…¥: pythonæ€ä¹ˆå®‰è£…ç¬¬ä¸‰æ–¹åº“
è¾“å‡º: {{"task":"CHAT","confidence":0.88}}
ï¼ˆåˆ†æï¼šé—®"æ€ä¹ˆ"=æ±‚çŸ¥è¯†ï¼Œä¸æ˜¯è®©ä½ å†™ä»£ç ï¼‰
è¾“å…¥: ä»€ä¹ˆæ˜¯docker
è¾“å‡º: {{"task":"CHAT","confidence":0.90}}
è¾“å…¥: äº†è§£ä¸€ä¸‹æœºå™¨å­¦ä¹ 
è¾“å‡º: {{"task":"CHAT","confidence":0.85}}
è¾“å…¥: ç ”ç©¶ä¸€ä¸‹è¿™ä¸ªé—®é¢˜
è¾“å‡º: {{"task":"CHAT","confidence":0.80}}
ï¼ˆåˆ†æï¼š"ç ”ç©¶ä¸€ä¸‹"=æ—¥å¸¸"äº†è§£ä¸€ä¸‹"ï¼Œä¸æ˜¯æ·±åº¦ç ”ç©¶ï¼‰
è¾“å…¥: å¸®æˆ‘æ·±å…¥ç ”ç©¶MicroLEDçš„æŠ€æœ¯åŸç†å’Œå‘å±•å†ç¨‹
è¾“å‡º: {{"task":"RESEARCH","confidence":0.90}}
ï¼ˆåˆ†æï¼š"æ·±å…¥ç ”ç©¶"+å…·ä½“ä¸»é¢˜+"æŠ€æœ¯åŸç†"=éœ€è¦ç³»ç»Ÿæ€§é•¿æ–‡ï¼‰
è¾“å…¥: å†™ä¸€æ®µè‡ªæˆ‘ä»‹ç»
è¾“å‡º: {{"task":"CHAT","confidence":0.85}}
ï¼ˆåˆ†æï¼š"å†™ä¸€æ®µ"=çŸ­æ–‡æœ¬è¾“å‡ºï¼Œä¸æ˜¯ç”Ÿæˆæ–‡ä»¶ï¼‰
è¾“å…¥: å¸®æˆ‘å†™ä»½é¡¹ç›®æ€»ç»“Wordæ–‡æ¡£
è¾“å‡º: {{"task":"FILE_GEN","confidence":0.90}}
è¾“å…¥: æœç´¢æ€ä¹ˆç”¨git
è¾“å‡º: {{"task":"CHAT","confidence":0.82}}
ï¼ˆåˆ†æï¼šç”¨æˆ·è¦çš„æ˜¯çŸ¥è¯†/æ•™ç¨‹ï¼Œä¸æ˜¯å»ç½‘ä¸Šæœå®æ—¶ä¿¡æ¯ï¼‰
è¾“å…¥: ä»Šå¤©Aè‚¡æ¶¨äº†å—
è¾“å‡º: {{"task":"WEB_SEARCH","confidence":0.92}}
è¾“å…¥: å¸®æˆ‘æŠŠè¿™ä¸ªæ–‡ä»¶ä¼˜åŒ–ä¸€ä¸‹
è¾“å‡º: {{"task":"DOC_ANNOTATE","confidence":0.85}}

åªè¾“å‡º JSONï¼š
{{"task":"...","confidence":0.0-1.0}}
'''

    @classmethod
    def is_ollama_available(cls) -> bool:
        """æ£€æŸ¥ Ollama æ˜¯å¦å¯ç”¨ï¼ˆå¸¦ç¼“å­˜ï¼Œé¿å…é¢‘ç¹æ£€æµ‹ï¼‰"""
        
        # ç¼“å­˜ 30 ç§’
        if cls._available is not None and (time.time() - cls._check_time) < 30:
            return cls._available
        
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(0.1)
            result = sock.connect_ex(('127.0.0.1', 11434))
            sock.close()
            cls._available = (result == 0)
            cls._check_time = time.time()
            return cls._available
        except:
            cls._available = False
            cls._check_time = time.time()
            return False
    
    @classmethod
    def init_model(cls, model_name: str = None) -> bool:
        """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹ï¼ˆé™é»˜å¤±è´¥ï¼Œä¸å½±å“ä½¿ç”¨ï¼‰"""
        if cls._initialized and cls._model_name:
            return True
        
        if not cls.is_ollama_available():
            # é™é»˜è¿”å›ï¼Œä¸æ‰“å°é”™è¯¯ï¼ˆé¿å…åˆ·å±ï¼‰
            return False
        
        # è·å–å·²å®‰è£…çš„æ¨¡å‹
        try:
            resp = requests.get("http://localhost:11434/api/tags", timeout=2)
            if resp.status_code != 200:
                return False
            installed = [m['name'].split(':')[0] + ':' + m['name'].split(':')[1] if ':' in m['name'] else m['name'] 
                        for m in resp.json().get('models', [])]
        except:
            return False
        
        if not installed:
            return False
        
        # é€‰æ‹©å¯ç”¨çš„æœ€å¿«æ¨¡å‹
        target_model = model_name
        if not target_model:
            for m in cls.OLLAMA_MODELS:
                base_name = m.split(':')[0]
                if any(base_name in im for im in installed):
                    for im in installed:
                        if base_name in im:
                            target_model = im
                            break
                    break
        
        if not target_model:
            return False
        
        cls._model_name = target_model
        cls._initialized = True
        print(f"[LocalModelRouter] âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {target_model}")
        return True
    
    @classmethod
    def classify(cls, user_input: str, timeout: float = 4.0) -> tuple:
        """
        ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹åˆ†ç±»ä»»åŠ¡
        
        è¿”å›: (task_type, confidence_str, source) æˆ– (None, reason, source)
        """
        start = time.time()
        
        # ç¡®ä¿æ¨¡å‹å¯ç”¨
        if not cls._initialized:
            if not cls.init_model():
                return None, "âŒ ModelNotReady", "Local"
        
        prompt = cls.CLASSIFY_PROMPT
        
        try:
            resp = requests.post(
                "http://localhost:11434/api/chat",
                json={
                    "model": cls._model_name,
                    "messages": [
                        {"role": "system", "content": prompt},
                        {"role": "user", "content": user_input[:500]},
                    ],
                    "stream": False,
                    "format": "json",
                    "think": False,  # Qwen3: ç¦ç”¨æ€è€ƒæ¨¡å¼ï¼ŒåŠ é€Ÿåˆ†ç±»
                    "options": {
                        "temperature": 0.0,
                        "num_predict": 80,
                    }
                },
                timeout=timeout
            )
            
            latency = (time.time() - start) * 1000
            
            if resp.status_code != 200:
                return None, f"âŒ API Error {resp.status_code}", "Local"
            
            data_json = resp.json()
            raw = (data_json.get("message", {}) or {}).get("content", "")
            if not raw:
                raw = data_json.get("response", "")
            raw = (raw or "").strip()
            valid_tasks = ["PAINTER", "FILE_GEN", "DOC_ANNOTATE", "RESEARCH", "CODER", "SYSTEM", "AGENT", "WEB_SEARCH", "CHAT"]

            # è§£æ JSON è¾“å‡º
            task_type = None
            confidence = 0.0
            try:
                import json as _json
                data = _json.loads(raw)
                task_type = str(data.get("task", "")).strip().upper()
                confidence = float(data.get("confidence", 0.0))
            except Exception:
                # å›é€€ï¼šå°è¯•ä»çº¯æ–‡æœ¬ä¸­æå–
                raw_upper = raw.upper()
                for t in valid_tasks:
                    if t in raw_upper:
                        task_type = t
                        confidence = 0.5
                        break

            if task_type in valid_tasks and "|" not in task_type and 0.0 <= confidence <= 1.0 and confidence >= 0.45:
                conf_str = f"ğŸ¤– Local {confidence:.2f} ({latency:.0f}ms)"
                print(f"[LocalModelRouter] {task_type} {conf_str}")
                return task_type, conf_str, "Local"
            else:
                print(f"[LocalModelRouter] æ— æ³•è§£æç»“æœ: {raw[:80]}")
                return None, f"âš ï¸ ParseError", "Local"
                
        except requests.exceptions.Timeout:
            return None, f"â±ï¸ Timeout ({timeout}s)", "Local"
        except Exception as e:
            print(f"[LocalModelRouter] é”™è¯¯: {e}")
            return None, f"âŒ Error", "Local"

    # â”€â”€ æœ¬åœ°æ¨¡å‹å“åº”ç”Ÿæˆï¼ˆç®€å•é—®é¢˜å¿«é€Ÿé€šé“ï¼‰ â”€â”€

    # ç”¨äºå“åº”ç”Ÿæˆçš„æ¨¡å‹ï¼ˆæŒ‰åå¥½æ’åºï¼Œæ¯”åˆ†ç±»æ¨¡å‹å¯ä»¥æ›´å¤§ï¼‰
    OLLAMA_RESPONSE_MODELS = [
        "qwen3:8b",          # â˜… æœ€ä½³ï¼Œä¸­è‹±æ–‡æµç•…
        "qwen3:4b",          # å¿«é€Ÿå¤‡é€‰
        "qwen2.5:7b",        # æ—§ç‰ˆè´¨é‡å¥½
        "qwen2.5:3b",        # æ—§ç‰ˆå¿«é€Ÿ
        "llama3.2:3b",
    ]

    _response_model = None   # ç”¨äºç”Ÿæˆçš„æ¨¡å‹ï¼ˆå¯èƒ½æ¯”åˆ†ç±»æ¨¡å‹å¤§ï¼‰
    _response_model_inited = False

    @classmethod
    def _init_response_model(cls) -> bool:
        """åˆå§‹åŒ–ç”¨äºå“åº”ç”Ÿæˆçš„æœ¬åœ°æ¨¡å‹"""
        if cls._response_model_inited and cls._response_model:
            return True
        if not cls.is_ollama_available():
            return False

        try:
            resp = requests.get("http://localhost:11434/api/tags", timeout=2)
            if resp.status_code != 200:
                return False
            installed = [m['name'] for m in resp.json().get('models', [])]
        except Exception:
            return False

        if not installed:
            return False

        # ä¼˜å…ˆé€‰æ‹©æ›´å¤§çš„ç”Ÿæˆæ¨¡å‹
        for want in cls.OLLAMA_RESPONSE_MODELS:
            base = want.split(':')[0]
            for im in installed:
                if base in im:
                    cls._response_model = im
                    cls._response_model_inited = True
                    print(f"[LocalModelRouter] âœ… å“åº”ç”Ÿæˆæ¨¡å‹: {im}")
                    return True

        # å›é€€åˆ°åˆ†ç±»æ¨¡å‹
        if cls._model_name:
            cls._response_model = cls._model_name
            cls._response_model_inited = True
            return True
        return False

    @classmethod
    def is_simple_query(cls, user_input: str, task_type: str, history: list = None) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºç®€å•é—®é¢˜ï¼Œå¯ä»¥ç”¨æœ¬åœ°æ¨¡å‹å¿«é€Ÿå›ç­”ã€‚
        
        æ ‡å‡†ï¼š
        - ä»»åŠ¡ç±»å‹æ˜¯ CHAT
        - è¾“å…¥è¾ƒçŸ­ï¼ˆâ‰¤120 å­—ç¬¦ï¼‰
        - ä¸éœ€è¦è”ç½‘ä¿¡æ¯ / æ·±åº¦åˆ†æ
        - ä¸æ¶‰åŠæ–‡ä»¶æˆ–å›¾ç‰‡
        - å†å²å¯¹è¯ä¸è¶…è¿‡ 4 è½®ï¼ˆä¸Šä¸‹æ–‡ä¸å¤ªå¤æ‚ï¼‰
        """
        if task_type != "CHAT":
            return False

        if not cls.is_ollama_available():
            return False

        text = user_input.strip()

        # é•¿è¾“å…¥ â†’ ä¸é€‚åˆæœ¬åœ°å°æ¨¡å‹
        if len(text) > 120:
            return False

        # å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å¤ªå¤š â†’ äº‘æ¨¡å‹æ›´å¥½
        if history and len(history) > 8:  # 4 è½® = 8 æ¡ (user+model)
            return False

        # éœ€è¦å®æ—¶æ•°æ®çš„å…³é”®è¯ â†’ å¿…é¡»è”ç½‘
        realtime_kw = [
            "ä»Šå¤©", "ç°åœ¨", "æœ€æ–°", "å®æ—¶", "å¤©æ°”", "è‚¡ä»·", "æ±‡ç‡",
            "æ–°é—»", "çƒ­ç‚¹", "ä»·æ ¼", "å¤šå°‘é’±", "æ¶¨", "è·Œ", "æ¯”èµ›",
            "æˆç»©", "æ’å", "é€‰ä¸¾", "ç–«æƒ…", "èˆªç­", "ç«è½¦ç¥¨", "é«˜é“",
        ]
        if any(kw in text for kw in realtime_kw):
            return False

        # éœ€è¦æ·±åº¦/ä¸“ä¸šåˆ†æçš„å…³é”®è¯ â†’ äº‘æ¨¡å‹è´¨é‡æ›´å¥½
        deep_kw = [
            "æ·±å…¥", "è¯¦ç»†", "æ·±åº¦", "ç³»ç»Ÿæ€§", "å…¨é¢åˆ†æ",
            "å†™ä¸€ç¯‡", "å†™ä¸€ä»½", "æŠ¥å‘Š", "è®ºæ–‡", "æ–‡æ¡£",
        ]
        if any(kw in text for kw in deep_kw):
            return False

        # æ¶‰åŠä»£ç çš„ â†’ äº‘æ¨¡å‹æ›´å¯é 
        code_kw = ["ä»£ç ", "å‡½æ•°", "è„šæœ¬", "python", "java", "javascript", "ä»£ç ", "debug", "bug"]
        if any(kw in text.lower() for kw in code_kw):
            return False

        return True

    @classmethod
    def generate_stream(cls, user_input: str, history: list = None, 
                        system_instruction: str = None, timeout: float = 30.0):
        """
        ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹æµå¼ç”Ÿæˆå“åº”ã€‚
        
        Returns: generator of text chunks, or None if unavailable
        """
        if not cls._init_response_model():
            return None

        # æ„å»º messages
        messages = []
        sys_prompt = system_instruction or (
            "ä½ æ˜¯ Kotoï¼Œä¸€ä¸ªå‹å–„ã€ä¸“ä¸šçš„ AI åŠ©æ‰‹ã€‚"
            "ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå¦‚æœç”¨æˆ·ç”¨è‹±æ–‡åˆ™ç”¨è‹±æ–‡å›ç­”ã€‚"
            "å›ç­”è¦ç®€æ´æ˜äº†ã€å‡†ç¡®å¯é ã€‚"
            "å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜ã€‚"
        )
        messages.append({"role": "system", "content": sys_prompt})

        # åŠ å…¥å†å²å¯¹è¯ï¼ˆæœ€å¤šæœ€è¿‘ 4 è½®ï¼‰
        if history:
            recent = history[-8:]  # æœ€å¤š 4 è½®
            for turn in recent:
                role = "assistant" if turn.get("role") == "model" else turn.get("role", "user")
                parts_text = " ".join(turn.get("parts", []))
                if parts_text.strip():
                    messages.append({"role": role, "content": parts_text[:500]})

        messages.append({"role": "user", "content": user_input})

        def _stream():
            try:
                resp = requests.post(
                    "http://localhost:11434/api/chat",
                    json={
                        "model": cls._response_model,
                        "messages": messages,
                        "stream": True,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": 2048,
                        }
                    },
                    stream=True,
                    timeout=timeout
                )
                if resp.status_code != 200:
                    return

                import re as _re
                _in_think = False
                _think_buf = ""
                for line in resp.iter_lines():
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        content = data.get("message", {}).get("content", "")
                        if content:
                            # è¿‡æ»¤ Qwen3 çš„ <think>...</think> æ€è€ƒæ ‡ç­¾
                            _think_buf += content
                            while True:
                                if _in_think:
                                    end_idx = _think_buf.find("</think>")
                                    if end_idx >= 0:
                                        _think_buf = _think_buf[end_idx + 8:]
                                        _in_think = False
                                    else:
                                        _think_buf = ""  # ä»åœ¨æ€è€ƒä¸­ï¼Œä¸¢å¼ƒ
                                        break
                                else:
                                    start_idx = _think_buf.find("<think>")
                                    if start_idx >= 0:
                                        before = _think_buf[:start_idx]
                                        if before:
                                            yield before
                                        _think_buf = _think_buf[start_idx + 7:]
                                        _in_think = True
                                    else:
                                        # æ²¡æœ‰ think æ ‡ç­¾ï¼Œç›´æ¥è¾“å‡º
                                        # ä¿ç•™æœ€åå‡ ä¸ªå­—ç¬¦ä»¥é˜²æ ‡ç­¾è¢«æˆªæ–­
                                        if len(_think_buf) > 10:
                                            yield _think_buf[:-10]
                                            _think_buf = _think_buf[-10:]
                                        break
                        if data.get("done"):
                            # è¾“å‡ºå‰©ä½™ç¼“å†²
                            if _think_buf and not _in_think:
                                yield _think_buf
                            break
                    except Exception:
                        continue
            except Exception as e:
                print(f"[LocalModelRouter] æµå¼ç”Ÿæˆé”™è¯¯: {e}")

        return _stream()
