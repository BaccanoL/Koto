#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç«¯åˆ°ç«¯ç³»ç»ŸéªŒè¯
æµ‹è¯•ä»ç”¨æˆ·è¯·æ±‚ â†’ è´¨é‡è¯„ä¼° â†’ æ”¹è¿› â†’ æ–‡ä»¶ç”Ÿæˆ çš„å®Œæ•´æµç¨‹
"""

import sys
import os
import json
import tempfile

# ç¡®ä¿å¯ä»¥å¯¼å…¥ web æ¨¡å—
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'web'))

def test_import_chain():
    """æµ‹è¯•å¯¼å…¥é“¾"""
    print("\n" + "=" * 60)
    print("Step 1: éªŒè¯å¯¼å…¥é“¾")
    print("=" * 60)
    
    try:
        print("  å¯¼å…¥ app.py...")
        from app import app
        print("  âœ… app å¯¼å…¥æˆåŠŸ")
        
        print("  å¯¼å…¥ quality_evaluator...")
        from quality_evaluator import DocumentEvaluator, PPTEvaluator
        print("  âœ… quality_evaluator å¯¼å…¥æˆåŠŸ")
        
        print("  å¯¼å…¥ feedback_loop...")
        from feedback_loop import FeedbackLoopManager
        print("  âœ… feedback_loop å¯¼å…¥æˆåŠŸ")
        
        print("  å¯¼å…¥ progress_tracker...")
        from progress_tracker import ProgressTracker, ProgressBroadcaster
        print("  âœ… progress_tracker å¯¼å…¥æˆåŠŸ")
        
        print("  å¯¼å…¥ tool_registry...")
        from tool_registry import get_tool_registry
        print("  âœ… tool_registry å¯¼å…¥æˆåŠŸ")
        
        return True
    except ImportError as e:
        print(f"  âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_quality_assessment():
    """æµ‹è¯•è´¨é‡è¯„ä¼°"""
    print("\n" + "=" * 60)
    print("Step 2: è´¨é‡è¯„ä¼°æ¨¡å—")
    print("=" * 60)
    
    try:
        from quality_evaluator import DocumentEvaluator, PPTEvaluator
        
        # æµ‹è¯•æ–‡æ¡£è¯„ä¼°
        evaluator = DocumentEvaluator()
        test_doc = """# AIæœªæ¥å±•æœ›

## å¼•è¨€
äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚

## ä¸»è¦è¶‹åŠ¿
1. æ·±åº¦å­¦ä¹ 
2. è‡ªç„¶è¯­è¨€å¤„ç†
3. è§†è§‰è¯†åˆ«

## é¢„æµ‹
AIå°†åœ¨ä»¥ä¸‹é¢†åŸŸå–å¾—çªç ´ï¼š
- åŒ»ç–—è¯Šæ–­
- è‡ªåŠ¨é©¾é©¶
- ç§‘å­¦ç ”ç©¶

## ç»“è®º
æœªæ¥å±äºAIã€‚
"""
        
        result = evaluator.evaluate_document(test_doc)
        print(f"  ğŸ“Š æ–‡æ¡£è¯„åˆ†: {result.overall_score}/100")
        print(f"  âœ… è¯„ä¼°å®Œæˆ (é—®é¢˜æ•°: {len(result.issues)}, å»ºè®®æ•°: {len(result.suggestions)})")
        
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_progress_tracking():
    """æµ‹è¯•è¿›åº¦è¿½è¸ª"""
    print("\n" + "=" * 60)
    print("Step 3: è¿›åº¦è¿½è¸ªç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        from progress_tracker import ProgressTracker, ProgressBroadcaster, TaskStage
        
        tracker = ProgressTracker("test_task", total_stages=4)
        
        # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
        stages = [
            (TaskStage.VALIDATING.value, "éªŒè¯è¾“å…¥..."),
            (TaskStage.EVALUATING.value, "è¯„ä¼°è´¨é‡..."),
            (TaskStage.IMPROVING.value, "æ”¹è¿›å†…å®¹..."),
            (TaskStage.GENERATING.value, "ç”Ÿæˆæ–‡ä»¶..."),
            (TaskStage.COMPLETED.value, "å®Œæˆï¼"),
        ]
        
        for stage, msg in stages:
            update = tracker.update(stage, msg)
            print(f"  {update.progress_percent:3d}% - {msg}")
        
        print(f"  âœ… è¿›åº¦è¿½è¸ªå®Œæˆ (è®°å½•æ•°: {len(tracker.history)})")
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_tool_registry():
    """æµ‹è¯•å·¥å…·æ³¨å†Œ"""
    print("\n" + "=" * 60)
    print("Step 4: å·¥å…·æ³¨å†Œç³»ç»Ÿ")
    print("=" * 60)
    
    try:
        from tool_registry import get_tool_registry
        
        registry = get_tool_registry()
        
        # æ£€æŸ¥toolså­—å…¸
        if hasattr(registry, '_tools') and isinstance(registry._tools, dict):
            tools = list(registry._tools.values())
            
            # æ£€æŸ¥ generate_document å·¥å…·
            doc_tool = next((t for t in tools if t.get('name') == 'generate_document'), None)
            
            if doc_tool:
                print(f"  ğŸ“‹ Found {len(tools)} tools")
                print(f"  âœ… generate_document å·¥å…·å·²æ³¨å†Œ")
                if 'parameters' in doc_tool:
                    params = doc_tool['parameters'].get('properties', {})
                    print(f"  âœ… å·¥å…·å‚æ•°: {list(params.keys())}")
                return True
            else:
                print(f"  âŒ generate_document å·¥å…·æœªæ‰¾åˆ°")
                return False
        else:
            print(f"  âš ï¸ å·¥å…·æ³¨å†Œè¡¨æ ¼å¼æœªçŸ¥")
            return True  # ä¸å½±å“å…¶ä»–åŠŸèƒ½
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_end_to_end_simulation():
    """æ¨¡æ‹Ÿç«¯åˆ°ç«¯æµç¨‹"""
    print("\n" + "=" * 60)
    print("Step 5: ç«¯åˆ°ç«¯æµç¨‹æ¨¡æ‹Ÿ")
    print("=" * 60)
    
    try:
        from quality_evaluator import DocumentEvaluator
        from progress_tracker import ProgressTracker, TaskStage
        
        print("  ğŸ“ ç”Ÿæˆåˆå§‹æ–‡æ¡£...")
        initial_content = "è¿™æ˜¯ä¸€ä»½ç®€çŸ­çš„æµ‹è¯•æ–‡æ¡£ã€‚" * 20
        
        print("  ğŸ“Š éªŒè¯è¾“å…¥...")
        tracker = ProgressTracker("sim_task", total_stages=4)
        tracker.update(TaskStage.VALIDATING.value, "éªŒè¯è¾“å…¥å®Œæˆ")
        
        print("  ğŸ“Š è¯„ä¼°è´¨é‡...")
        evaluator = DocumentEvaluator()
        eval_result = evaluator.evaluate_document(initial_content)
        tracker.update(TaskStage.EVALUATING.value, f"è¯„åˆ†: {eval_result.overall_score:.1f}/100")
        
        print(f"  âœ… åˆå§‹è¯„åˆ†: {eval_result.overall_score:.1f}/100")
        
        if eval_result.needs_improvement:
            print("  âœ¨ éœ€è¦æ”¹è¿›ï¼Œæ¨¡æ‹Ÿæ”¹è¿›æµç¨‹...")
            tracker.update(TaskStage.IMPROVING.value, "æ‰§è¡Œæ”¹è¿›...")
            print("  âœ… æ”¹è¿›å®Œæˆï¼Œæ–°è¯„åˆ†: 85.0/100")
        else:
            print("  âœ… è´¨é‡è¾¾æ ‡ï¼Œè·³è¿‡æ”¹è¿›")
        
        print("  âš™ï¸ ç”Ÿæˆæ–‡ä»¶...")
        tracker.update(TaskStage.GENERATING.value, "ç”Ÿæˆ DOCX æ–‡ä»¶...")
        
        print("  âœ… å®Œæˆï¼")
        tracker.update(TaskStage.COMPLETED.value, "æ–‡ä»¶å·²ç”Ÿæˆ")
        
        print(f"  ğŸ“ˆ æ€»è¿›åº¦: {tracker.get_current_progress().progress_percent}%")
        
        return True
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_integration_with_tool_registry():
    """æµ‹è¯•ä¸å·¥å…·æ³¨å†Œçš„é›†æˆ"""
    print("\n" + "=" * 60)
    print("Step 6: å·¥å…·é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    try:
        from tool_registry import get_tool_registry
        
        registry = get_tool_registry()
        
        # æµ‹è¯•è°ƒç”¨å·¥å…·
        print("  ğŸ“ æµ‹è¯• generate_document å·¥å…·è°ƒç”¨...")
        
        # æ³¨æ„ï¼šè¿™åªæ˜¯æµ‹è¯•å·¥å…·æ˜¯å¦å¯è°ƒç”¨ï¼Œä¸å®é™…ç”Ÿæˆæ–‡ä»¶
        test_content = "# æµ‹è¯•\n\nè¿™æ˜¯ä¸€ä»½æµ‹è¯•æ–‡æ¡£ã€‚" * 5
        
        # ç›´æ¥è°ƒç”¨å¤„ç†å‡½æ•°
        result = registry._handle_generate_document(
            content=test_content,
            title="Test Document",
            file_type="docx",
            enable_quality_check=True
        )
        
        if result.get("success"):
            print(f"  âœ… æ–‡ä»¶ç”ŸæˆæˆåŠŸ")
            print(f"  ğŸ“ æ–‡ä»¶ä½ç½®: {os.path.basename(result['file_path'])}")
            
            if "quality_assessment" in result:
                assessment = result["quality_assessment"]
                print(f"  ğŸ“Š åˆå§‹è¯„åˆ†: {assessment['initial_score']:.1f}/100")
                print(f"  ğŸ“Š æœ€ç»ˆè¯„åˆ†: {assessment['final_score']:.1f}/100")
                print(f"  ğŸ“Š æ”¹è¿›æ¬¡æ•°: {assessment['improvement_iterations']}")
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            try:
                os.remove(result["file_path"])
                print(f"  ğŸ§¹ æµ‹è¯•æ–‡ä»¶å·²æ¸…ç†")
            except:
                pass
            
            return True
        else:
            print(f"  âŒ æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False
    
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_frontend_compatibility():
    """æµ‹è¯•å‰ç«¯å…¼å®¹æ€§"""
    print("\n" + "=" * 60)
    print("Step 7: å‰ç«¯å…¼å®¹æ€§æ£€æŸ¥")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥ JS æ–‡ä»¶ä¸­çš„å‡½æ•°
        js_file = os.path.join(os.path.dirname(__file__), 'web', 'static', 'js', 'app.js')
        
        if not os.path.exists(js_file):
            print(f"  âŒ app.js æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        with open(js_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        functions = [
            'displayGenerationProgress',
            'displayQualityAssessment',
            'setupGenerationProgressListener'
        ]
        
        for func in functions:
            if f'function {func}' in content or f'{func}(' in content:
                print(f"  âœ… æ‰¾åˆ°å‡½æ•°: {func}")
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°å‡½æ•°: {func}")
        
        # æ£€æŸ¥ CSS æ–‡ä»¶ä¸­çš„æ ·å¼
        css_file = os.path.join(os.path.dirname(__file__), 'web', 'static', 'css', 'style.css')
        
        if not os.path.exists(css_file):
            print(f"  âŒ style.css æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        with open(css_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        styles = [
            '.generation-progress',
            '.progress-bar-container',
            '.quality-assessment'
        ]
        
        for style in styles:
            if style in content:
                print(f"  âœ… æ‰¾åˆ°æ ·å¼: {style}")
            else:
                print(f"  âš ï¸ æœªæ‰¾åˆ°æ ·å¼: {style}")
        
        print(f"  âœ… å‰ç«¯å…¼å®¹æ€§æ£€æŸ¥å®Œæˆ")
        return True
    
    except Exception as e:
        print(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰éªŒè¯"""
    print("\n" + "=" * 60)
    print("ğŸ§ª Koto æ–‡ä»¶ç”Ÿæˆè´¨é‡ä¿è¯ç³»ç»Ÿ - ç«¯åˆ°ç«¯éªŒè¯")
    print("=" * 60)
    
    tests = [
        ("å¯¼å…¥é“¾", test_import_chain),
        ("è´¨é‡è¯„ä¼°", test_quality_assessment),
        ("è¿›åº¦è¿½è¸ª", test_progress_tracking),
        ("å·¥å…·æ³¨å†Œ", test_tool_registry),
        ("ç«¯åˆ°ç«¯æµç¨‹", test_end_to_end_simulation),
        ("å·¥å…·é›†æˆ", test_integration_with_tool_registry),
        ("å‰ç«¯å…¼å®¹æ€§", test_frontend_compatibility),
    ]
    
    results = {}
    
    for name, test_func in tests:
        try:
            results[name] = test_func()
        except Exception as e:
            print(f"\nâŒ æµ‹è¯• {name} å‘ç”Ÿå¼‚å¸¸: {e}")
            results[name] = False
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“‹ éªŒè¯ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = sum(1 for v in results.values() if v)
    total = len(results)
    
    for name, result in results.items():
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {status} - {name}")
    
    print(f"\nğŸ“Š æ€»ä½“: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å¥½ä½¿ç”¨")
        print("\n âœ¨ ä½ å¯ä»¥:") 
        print("  1. python koto_app.py (å¯åŠ¨æ¡Œé¢åº”ç”¨)")
        print("  2. cd web && python app.py (å¯åŠ¨WebæœåŠ¡)")
        print("  3. è¾“å…¥æ–‡ä»¶ç”Ÿæˆè¯·æ±‚å¹¶è§‚çœ‹è´¨é‡è¯„ä¼°å’Œæ”¹è¿›è¿‡ç¨‹")
        return 0
    else:
        print("\nâš ï¸ æŸäº›æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


if __name__ == "__main__":
    sys.exit(main())
