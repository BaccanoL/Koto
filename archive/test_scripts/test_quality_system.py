#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ–‡ä»¶ç”Ÿæˆå’Œè´¨é‡è¯„ä¼°æµç¨‹
"""

import sys
import os

# æ·»åŠ  web ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'web'))

def test_ppt_evaluator():
    """æµ‹è¯• PPT è¯„ä¼°å™¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯• PPT è¯„ä¼°å™¨")
    print("=" * 50)
    
    from quality_evaluator import PPTEvaluator
    import tempfile
    
    # æµ‹è¯•ç”¨ä¾‹ï¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„ PPTï¼ˆå¦‚æžœå¯ç”¨ï¼‰
    try:
        from pptx import Presentation
        from pptx.util import Inches, Pt
        
        # åˆ›å»ºæµ‹è¯• PPT
        prs = Presentation()
        prs.slide_width = Inches(13.333)
        prs.slide_height = Inches(7.5)
        
        # æ·»åŠ å‡ ä¸ªå¹»ç¯ç‰‡
        for i in range(1, 4):
            slide_layout = prs.slide_layouts[1]
            slide = prs.slides.add_slide(slide_layout)
            title = slide.shapes.title
            title.text = f"Slide {i}"
            
            if i == 2:
                # æ·»åŠ å¾ˆå¤šæ–‡æœ¬åˆ°ç¬¬2é¡µ
                body = slide.placeholders[1].text_frame
                body.text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¹»ç¯ç‰‡ã€‚" * 100
        
        # ä¿å­˜æµ‹è¯•æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        with tempfile.NamedTemporaryFile(suffix=".pptx", delete=False) as tmp:
            test_ppt_path = tmp.name
        
        prs.save(test_ppt_path)
        
        # è¯„ä¼° PPT
        evaluator = PPTEvaluator()
        result = evaluator.evaluate_pptx_file(test_ppt_path)
        
        print(f"ðŸ“Š æ€»ä½“è¯„åˆ†: {result.overall_score}/100")
        print(f"ðŸ“‹ ç±»åˆ«è¯„åˆ†: {result.category_scores}")
        print(f"âš ï¸ å‘çŽ°çš„é—®é¢˜æ•°: {len(result.issues)}")
        print(f"ðŸ’¡ æ”¹è¿›å»ºè®®æ•°: {len(result.suggestions)}")
        print(f"ðŸŽ¯ æ”¹è¿›ä¼˜å…ˆçº§: {result.improvement_priority}")
        print(f"âœ“ éœ€è¦æ”¹è¿›: {result.needs_improvement}")
        
        # æ¸…ç†
        os.remove(test_ppt_path)
        print("\nâœ… PPT è¯„ä¼°å™¨æµ‹è¯•é€šè¿‡")
    
    except ImportError:
        print("âš ï¸ python-pptx æœªå®‰è£…ï¼Œè·³è¿‡ PPT è¯„ä¼°æµ‹è¯•")
    except Exception as e:
        print(f"âŒ PPT è¯„ä¼°å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def test_document_evaluator():
    """æµ‹è¯•æ–‡æ¡£è¯„ä¼°å™¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ–‡æ¡£è¯„ä¼°å™¨")
    print("=" * 50)
    
    from quality_evaluator import DocumentEvaluator
    
    # æµ‹è¯•ç”¨ä¾‹1ï¼šä¼˜ç§€æ–‡æ¡£
    excellent_doc = """# é¡¹ç›®æ€»ç»“æŠ¥å‘Š

## é¡¹ç›®æ¦‚è§ˆ
è¿™æ˜¯ä¸€ä»½è¯¦ç»†çš„é¡¹ç›®æ€»ç»“æŠ¥å‘Šï¼ŒåŒ…å«äº†å®Œæ•´çš„é¡¹ç›®ä¿¡æ¯ã€‚

## ä¸»è¦æˆå°±
- å®Œæˆäº† 95% çš„è®¡åˆ’åŠŸèƒ½
- ç”¨æˆ·æ»¡æ„åº¦è¾¾åˆ° 4.8/5 æ˜Ÿ
- æ€§èƒ½æå‡ 40%

## æŠ€æœ¯ç»†èŠ‚
### æž¶æž„è®¾è®¡
é‡‡ç”¨å¾®æœåŠ¡æž¶æž„ï¼Œæé«˜äº†ç³»ç»Ÿçš„å¯æ‰©å±•æ€§ã€‚

### æ ¸å¿ƒç»„ä»¶
1. æ•°æ®å±‚ï¼šPostgreSQL + Redis
2. ä¸šåŠ¡å±‚ï¼šGo + gRPC
3. å±•ç¤ºå±‚ï¼šReact 18

## æµ‹è¯•è¦†ç›–çŽ‡
- å•å…ƒæµ‹è¯•ï¼š87%
- é›†æˆæµ‹è¯•ï¼š92%
- ç«¯åˆ°ç«¯æµ‹è¯•ï¼š78%

## ç»“è®º
é¡¹ç›®æˆåŠŸäº¤ä»˜ï¼Œè¾¾åˆ°æ‰€æœ‰è´¨é‡ç›®æ ‡ã€‚æœªæ¥è®¡åˆ’ç»§ç»­ä¼˜åŒ–æ€§èƒ½ã€‚
"""
    
    evaluator = DocumentEvaluator()
    result = evaluator.evaluate_document(excellent_doc)
    
    print("æµ‹è¯•1ï¼šä¼˜ç§€æ–‡æ¡£")
    print(f"  è¯„åˆ†: {result.overall_score}/100")
    print(f"  éœ€è¦æ”¹è¿›: {result.needs_improvement}")
    
    # æµ‹è¯•ç”¨ä¾‹2ï¼šéœ€è¦æ”¹è¿›çš„æ–‡æ¡£
    poor_doc = "è¿™æ˜¯å¾ˆçŸ­çš„æ–‡æ¡£ã€‚"
    
    result2 = evaluator.evaluate_document(poor_doc)
    print("\næµ‹è¯•2ï¼šç®€çŸ­æ–‡æ¡£")
    print(f"  è¯„åˆ†: {result2.overall_score}/100")
    print(f"  éœ€è¦æ”¹è¿›: {result2.needs_improvement}")
    print(f"  é—®é¢˜: {result2.issues}")
    
    print("\nâœ… æ–‡æ¡£è¯„ä¼°å™¨æµ‹è¯•é€šè¿‡")


def test_progress_tracker():
    """æµ‹è¯•è¿›åº¦è¿½è¸ªå™¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•è¿›åº¦è¿½è¸ªå™¨")
    print("=" * 50)
    
    from progress_tracker import (
        ProgressTracker, ProgressBroadcaster, 
        TaskStage, GenerationProgressManager
    )
    
    # åˆ›å»ºè¿½è¸ªå™¨
    tracker = ProgressTracker("test_task", total_stages=4)
    
    # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
    stages = [
        (TaskStage.VALIDATING.value, "æ­£åœ¨éªŒè¯è¾“å…¥..."),
        (TaskStage.EVALUATING.value, "æ­£åœ¨è¯„ä¼°è´¨é‡..."),
        (TaskStage.IMPROVING.value, "æ­£åœ¨æ”¹è¿›å†…å®¹..."),
        (TaskStage.GENERATING.value, "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶..."),
        (TaskStage.COMPLETED.value, "æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼"),
    ]
    
    for stage, msg in stages:
        update = tracker.update(stage, msg)
        print(f"  {update.progress_percent}% - {update.message}")
    
    print("\nâœ… è¿›åº¦è¿½è¸ªå™¨æµ‹è¯•é€šè¿‡")


def test_feedback_loop():
    """æµ‹è¯•åé¦ˆå¾ªçŽ¯ï¼ˆéœ€è¦ Gemini å®¢æˆ·ç«¯ï¼‰"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•åé¦ˆå¾ªçŽ¯")
    print("=" * 50)
    
    try:
        from feedback_loop import FeedbackLoopManager
        
        # æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„ mock å®¢æˆ·ç«¯
        class MockClient:
            class Models:
                def generate_content(self, **kwargs):
                    class Response:
                        text = "æ”¹è¿›åŽçš„å†…å®¹ï¼šè¿™æ˜¯æ”¹è¿›åŽçš„ç‰ˆæœ¬ï¼ŒåŒ…å«æ›´å¤šç»†èŠ‚ã€‚"
                    return Response()
            models = Models()
        
        # åˆ›å»ºç®¡ç†å™¨
        manager = FeedbackLoopManager(lambda: MockClient())
        
        # æµ‹è¯•è¯„ä¼°ç»“æžœ
        evaluation = {
            "overall_score": 60,
            "needs_improvement": True,
            "issues": ["å†…å®¹è¿‡çŸ­", "ç»“æž„ä¸æ¸…æ™°"],
            "suggestions": ["å¢žåŠ æ›´å¤šç»†èŠ‚", "æ·»åŠ æ ‡é¢˜"],
            "improvement_priority": ["ä¼˜å…ˆæ·»åŠ æ ‡é¢˜", "ç„¶åŽå¢žåŠ å†…å®¹"]
        }
        
        # æµ‹è¯•æ”¹è¿›ï¼ˆä¼šè°ƒç”¨ APIï¼‰
        result = manager.improve_document_content(
            "è¿™æ˜¯åŽŸå§‹å†…å®¹ã€‚",
            evaluation,
            "æµ‹è¯•æ–‡æ¡£"
        )
        
        print(f"  æ”¹è¿›æ¬¡æ•°: {result['iterations']}")
        print(f"  æœ€ç»ˆè¯„åˆ†: {result['final_score']}")
        print(f"  æ”¹è¿›åŽ†å²: {len(result['improvement_history'])} æ¡è®°å½•")
        
        print("\nâœ… åé¦ˆå¾ªçŽ¯æµ‹è¯•é€šè¿‡")
    
    except Exception as e:
        print(f"âš ï¸ åé¦ˆå¾ªçŽ¯æµ‹è¯•éœ€è¦ Gemini å®¢æˆ·ç«¯: {e}")


def test_evaluation_quality():
    """æµ‹è¯•è´¨é‡è¯„ä¼°çš„å‡†ç¡®æ€§"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•è´¨é‡è¯„ä¼°å‡†ç¡®æ€§")
    print("=" * 50)
    
    from quality_evaluator import DocumentEvaluator
    
    test_cases = [
        {
            "name": "ä¼˜ç§€æ–‡æ¡£",
            "content": "# æ ‡é¢˜\n## å°æ ‡é¢˜\nå†…å®¹è¯¦ç»†çš„æ–‡æ¡£ã€‚" * 10,
            "expected_score_min": 80
        },
        {
            "name": "å·®æ–‡æ¡£",
            "content": "çŸ­",
            "expected_score_max": 50
        },
        {
            "name": "ä¸­ç­‰æ–‡æ¡£",
            "content": "# æ ‡é¢˜\nè¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æ–‡æ¡£ï¼Œæœ‰ä¸€äº›ç»“æž„ä½†å†…å®¹ä¸è¶³ã€‚",
            "expected_score_min": 50,
            "expected_score_max": 80
        }
    ]
    
    evaluator = DocumentEvaluator()
    
    for case in test_cases:
        result = evaluator.evaluate_document(case["content"])
        score = result.overall_score
        
        status = "âœ“" if (
            ("expected_score_min" not in case or score >= case["expected_score_min"]) and
            ("expected_score_max" not in case or score <= case["expected_score_max"])
        ) else "âœ—"
        
        print(f"  {status} {case['name']}: {score}/100")
    
    print("\nâœ… è´¨é‡è¯„ä¼°å‡†ç¡®æ€§æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    print("\nðŸ§ª å¼€å§‹æµ‹è¯•è´¨é‡è¯„ä¼°å’Œæ”¹è¿›ç³»ç»Ÿ\n")
    
    test_document_evaluator()
    test_ppt_evaluator()
    test_progress_tracker()
    test_evaluation_quality()
    test_feedback_loop()
    
    print("\n" + "=" * 50)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 50)
